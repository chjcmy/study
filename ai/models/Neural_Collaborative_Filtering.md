# Neural Collaborative Filtering (NCF)

## ğŸ“Œ ê°œë… (Concept)
MFì˜ ì„ í˜•ì ì¸ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ **"ë”¥ëŸ¬ë‹(Deep Learning)"**ì„ ë„ì…í•œ ëª¨ë¸ì…ë‹ˆë‹¤. MFì˜ ì¥ì ê³¼ ì‹ ê²½ë§ì˜ ì¥ì ì„ ê²°í•©í–ˆìŠµë‹ˆë‹¤.

- **í•µì‹¬ ì›ë¦¬**: `GMF` (Generalized Matrix Factorization) + `MLP` (Multi-Layer Perceptron)
- **ë¹„ìœ **: 
    - MFì²˜ëŸ¼ ë‹¨ìˆœí•œ ì·¨í–¥ë„ ë³´ê³  (GMF)
    - ë”¥ëŸ¬ë‹ìœ¼ë¡œ ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ì·¨í–¥ë„ íŒŒì•… (MLP)
    - ë‘ ê²°ê³¼ë¥¼ í•©ì³ì„œ ìµœì¢… íŒë‹¨

## âš™ï¸ êµ¬ì¡° (Architecture)
1.  **GMF Part**: ìœ ì €/ì•„ì´í…œ ì„ë² ë”©ì˜ ìš”ì†Œë³„ ê³± (Element-wise Product)
2.  **MLP Part**: ìœ ì €/ì•„ì´í…œ ì„ë² ë”©ì„ ì´ì–´ ë¶™ì—¬ì„œ(Concat) ì—¬ëŸ¬ ì¸µì˜ ì‹ ê²½ë§ í†µê³¼
3.  **NeuMF Layer**: GMFì™€ MLPì˜ ì¶œë ¥ì„ í•©ì³ì„œ(Concat) ìµœì¢… ì˜ˆì¸¡

## ğŸ‘ ì¥ì  (Pros)
- **ì •í™•ë„**: ë¹„ì„ í˜•ì ì¸ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ MFë³´ë‹¤ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì„.
- **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ êµ¬ì¡°ë¡œ í™•ì¥ì´ ê°€ëŠ¥í•¨.

## ğŸ‘ ë‹¨ì  (Cons)
- **í•™ìŠµ ì†ë„**: MFë³´ë‹¤ ì—°ì‚°ëŸ‰ì´ ë§ì•„ í•™ìŠµì´ ëŠë¦¼.
- **ë³µì¡ì„±**: í•˜ì´í¼íŒŒë¼ë¯¸í„°(ë ˆì´ì–´ ìˆ˜, ë…¸ë“œ ìˆ˜ ë“±) íŠœë‹ì´ í•„ìš”í•¨.

## ğŸ’» ì½”ë“œ ì˜ˆì‹œ (PyTorch)
```python
class NCF(nn.Module):
    def __init__(self, num_users, num_items, embed_dim=32):
        super().__init__()
        # GMF
        self.gmf_user = nn.Embedding(num_users, embed_dim)
        self.gmf_item = nn.Embedding(num_items, embed_dim)
        # MLP
        self.mlp_user = nn.Embedding(num_users, embed_dim)
        self.mlp_item = nn.Embedding(num_items, embed_dim)
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim*2, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        self.predict = nn.Linear(embed_dim + 32, 1)

    def forward(self, users, items):
        # GMF
        gmf_out = self.gmf_user(users) * self.gmf_item(items)
        # MLP
        mlp_in = torch.cat([self.mlp_user(users), self.mlp_item(items)], dim=1)
        mlp_out = self.mlp(mlp_in)
        # Concat
        return self.predict(torch.cat([gmf_out, mlp_out], dim=1))
```
