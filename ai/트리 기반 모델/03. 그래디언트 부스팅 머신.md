# 03. 그래디언트 부스팅 머신 (Gradient Boosting Machine, GBM)

## 1. 핵심 정의

**그래디언트 부스팅(Gradient Boosting)**은 **앙상블(Ensemble)** 학습 방법의 일종으로, 여러 개의 간단한 모델(주로 [[01. 의사 결정 트리|의사 결정 트리]])을 순차적으로 학습시켜, 이전 모델의 오차를 다음 모델이 보완해 나가는 방식으로 점진적으로 예측 성능을 향상시키는 지도 학습 알고리즘입니다.

-   **핵심 아이디어**: "잘못된 부분에 집중하여 점진적으로 개선해 나간다."
-   **앙상블 기법**: 그래디언트 부스팅은 **부스팅(Boosting)**이라는 앙상블 기법에 기반을 둡니다.

## 2. 그래디언트 부스팅의 작동 원리

그래디언트 부스팅은 [[02. 랜덤 포레스트|랜덤 포레스트]]처럼 모델을 병렬로 학습시키는 것이 아니라, **순차적으로** 학습시킵니다.

1.  **초기 모델 생성**: 먼저 매우 간단한 초기 모델을 만듭니다. (예: 회귀 문제의 경우, 타겟 변수의 평균값만 예측하는 모델)
2.  **잔차(Residual) 계산**: 현재 모델의 예측값과 실제값의 차이, 즉 **오차(잔차)**를 계산합니다.
3.  **잔차 학습**: 새로운 간단한 트리 모델(Weak Learner)을 학습시키되, 원래의 타겟 변수가 아닌 **앞 단계에서 계산된 잔차**를 예측하도록 학습시킵니다.
4.  **모델 업데이트**: 기존 모델에, 잔차를 예측하는 새로운 트리의 예측 결과를 더하여 모델을 업데이트합니다. 이때, 새로운 트리의 영향력을 조절하기 위해 **학습률(Learning Rate)**을 곱하여 더해줍니다.
    $$ \text{New_Model} = \text{Old_Model} + (\text{Learning_Rate} \times \text{New_Tree}) $$
5.  **반복**: 2~4번 과정을 미리 정해진 횟수만큼 반복합니다.

### "그래디언트(Gradient)"의 의미

이 과정은 수학적으로 **손실 함수(Loss Function)의 그래디언트(기울기)를 따라 최적의 해를 찾아가는 [[경사 하강법]]**과 동일한 원리입니다. 각 단계에서 계산되는 '잔차'는 손실 함수의 그래디언트와 유사한 역할을 하며, 모델이 손실을 가장 빠르게 줄일 수 있는 방향으로 점진적으로 업데이트되도록 유도합니다.

## 3. 주요 하이퍼파라미터

-   **n_estimators**: 생성할 트리의 개수. 너무 많으면 [[과적합(Overfitting)]]의 위험이 있습니다.
-   **learning_rate (학습률)**: 각 트리가 전체 모델에 기여하는 정도를 조절합니다. 값이 낮을수록 각 트리의 영향력이 줄어들어 더 안정적이지만, 더 많은 트리가 필요합니다.
-   **max_depth**: 각 트리의 최대 깊이. 트리의 복잡도를 제어하며, 과적합을 방지하는 데 중요한 역할을 합니다.

## 4. 장점과 단점

### 장점

-   **높은 예측 성능**: 일반적으로 매우 높은 정확도를 자랑하며, 많은 데이터 과학 경진대회에서 우승을 차지한 알고리즘입니다.
-   **유연성**: 다양한 손실 함수를 사용할 수 있어, 분류, 회귀 등 여러 문제에 유연하게 적용할 수 있습니다.

### 단점

-   **과적합 민감성**: 하이퍼파라미터 튜닝에 매우 민감하며, 튜닝이 제대로 되지 않으면 과적합되기 쉽습니다.
-   **계산 비용**: 순차적으로 트리를 만들어야 하므로, 병렬 처리가 어려운 [[02. 랜덤 포레스트|랜덤 포레스트]]에 비해 학습 속도가 느릴 수 있습니다.
-   **해석의 어려움**: 랜덤 포레스트와 마찬가지로 내부 동작을 해석하기 어렵습니다.

## 5. 대표적인 그래디언트 부스팅 라이브러리

-   **XGBoost (eXtreme Gradient Boosting)**: GBM의 속도와 성능을 크게 개선한 라이브러리. 병렬 처리, 규제(Regularization) 기능 등을 추가하여 매우 인기가 높습니다.
-   **LightGBM (Light Gradient Boosting Machine)**: XGBoost보다 더 빠른 학습 속도와 적은 메모리 사용량을 목표로 개발된 라이브러리. 대용량 데이터셋에서 특히 강점을 보입니다.
-   **CatBoost (Categorical Boosting)**: 범주형 변수를 자동으로 효율적으로 처리하는 기능이 내장되어 있어, 범주형 데이터가 많은 문제에서 뛰어난 성능을 보입니다.

#ai #ensemble #gradient-boosting #boosting #decision-tree #xgboost #lightgbm
