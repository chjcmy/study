# 02. 랜덤 포레스트 (Random Forest)

## 1. 핵심 정의

**랜덤 포레스트(Random Forest)**는 **앙상블(Ensemble)** 학습 방법의 일종으로, 여러 개의 [[01. 의사 결정 트리|의사 결정 트리]]를 개별적으로 학습시킨 후, 그 결과를 종합하여 최종 예측을 결정하는 지도 학습 알고리즘입니다.

-   **핵심 아이디어**: "하나의 뛰어난 전문가(단일 트리)보다, 여러 명의 평범한 전문가(다양한 트리)들의 집단 지성이 더 나은 결정을 내린다."
-   **앙상블 기법**: 랜덤 포레스트는 **배깅(Bagging)**이라는 앙상블 기법에 기반을 둡니다.

## 2. 랜덤 포레스트의 작동 원리

랜덤 포레스트는 '랜덤(Random)'이라는 이름에서 알 수 있듯이, 두 가지 핵심적인 무작위성 요소를 통해 강력한 모델을 만듭니다.

### 1) 배깅 (Bagging: Bootstrap Aggregating)

-   **부트스트랩 (Bootstrap)**: 원본 훈련 데이터셋에서 **중복을 허용하여** 무작위로 샘플을 추출하여, 원본과 동일한 크기의 여러 개의 새로운 훈련 데이터셋을 만드는 방법입니다.
-   **동작**: 랜덤 포레스트의 각 의사 결정 트리는 이렇게 만들어진 서로 다른 부트스트랩 데이터셋을 사용하여 개별적으로 학습됩니다.
-   **효과**: 각 트리가 약간씩 다른 데이터를 학습하게 되므로, 트리들의 예측 결과가 다양해집니다.

### 2) 무작위 특성 선택 (Random Feature Selection)

-   **동작**: [[01. 의사 결정 트리|일반적인 의사 결정 트리]]는 노드를 분할할 때 모든 특성(feature)을 고려하여 최적의 분할을 찾습니다. 하지만 랜덤 포레스트의 각 트리는 노드를 분할할 때마다, **전체 특성 중 일부를 무작위로 선택**하고, 그 선택된 특성 내에서만 최적의 분할을 찾습니다.
-   **효과**: 이 과정은 트리들 간의 **상관관계를 감소**시킵니다. 만약 특정 하나의 특성이 매우 강력하다면, 모든 트리가 그 특성을 우선적으로 사용하여 비슷한 모양의 트리가 만들어질 것입니다. 무작위 특성 선택은 이를 방지하고, 각 트리가 데이터의 다양한 측면을 학습하도록 유도합니다.

### 3) 최종 예측

-   **분류 (Classification)**: 모든 트리가 예측한 클래스들 중에서 가장 많이 나온 클래스(다수결 투표, Hard Voting)를 최종 예측으로 선택합니다.
-   **회귀 (Regression)**: 모든 트리가 예측한 값들의 평균을 최종 예측값으로 사용합니다.

## 3. 왜 랜덤 포레스트가 강력한가?

-   **과적합 방지**: 단일 [[01. 의사 결정 트리]]는 데이터에 [[과적합(Overfitting)]]되기 쉬운 단점이 있습니다. 랜덤 포레스트는 여러 트리의 결과를 평균내거나 투표함으로써, 개별 트리의 예측 오차(분산)를 줄여 모델 전체의 과적합을 효과적으로 방지합니다.
-   **안정성**: 데이터의 작은 변화나 노이즈에 덜 민감하여, 단일 트리보다 훨씬 안정적인 성능을 보입니다.

## 4. 장점과 단점

### 장점

-   **높은 정확도**: 일반적으로 매우 높은 예측 정확도를 보입니다.
-   **강력한 과적합 방지**: 배깅과 무작위 특성 선택 덕분에 과적합에 매우 강합니다.
-   **대용량 데이터 처리**: 대용량 데이터셋과 수많은 특성을 가진 데이터셋에서도 잘 작동합니다.
-   **특성 중요도 제공**: 각 특성이 모델의 예측에 얼마나 중요한지를 측정할 수 있는 기능을 제공합니다.

### 단점

-   **해석의 어려움**: 수백 개의 트리가 결합되어 있어, 단일 트리처럼 모델의 결정 과정을 직관적으로 해석하기 어렵습니다 (블랙박스 모델에 가까움).
-   **계산 비용**: 여러 개의 트리를 학습시켜야 하므로, 단일 트리보다 학습 속도가 느리고 더 많은 메모리를 필요로 합니다.

> 랜덤 포레스트는 [[03. 그래디언트 부스팅 머신|그래디언트 부스팅]]과 함께 가장 널리 사용되는 강력한 앙상블 모델 중 하나입니다.

#ai #ensemble #random-forest #bagging #decision-tree
