# 01.1. 회귀 트리 (Regression Tree)

## 1. 핵심 정의

**회귀 트리(Regression Tree)**는 [[01. 의사 결정 트리|의사 결정 트리]]의 한 종류로, 불연속적인 클래스(범주형)를 예측하는 대신 **연속형 값을 예측**하는 데 사용됩니다. 즉, 분류 트리가 '예/아니오'와 같은 범주형 출력을 반환한다면, 회귀 트리는 '온도', '급여'와 같은 수치형 출력을 반환합니다.

## 2. 분류 트리 vs 회귀 트리

[[01. 의사 결정 트리|의사 결정 트리]]는 목표 변수의 종류에 따라 **분류 트리**와 **회귀 트리**로 나뉩니다. 두 트리는 구조는 유사하지만, 목표와 세부적인 측정 기준에서 차이가 있습니다.

| 구분           | 분류 트리 (Classification Tree) | 회귀 트리 (Regression Tree) |                  |
| :----------- | :-------------------------- | :---------------------- | ---------------- |
| **목표**       | 데이터를 개별 집합(클래스)으로 분류        | 연속형 목표 변수 예측            |                  |
| **목표 변수**    | 범주형 (예: 참/거짓, 개/고양이)        | 연속형 (예: 온도, 가격, 급여)     |                  |
| **리프 노드 예측** | 노드 내 데이터의 과반수 득표 클래스        | 노드 내 데이터 목표값의 평균        |                  |
| **분할 기준**    | 지니 불순도, 엔트로피, 정보 이득         | [[../MSE (평균 제곱 오차)     | MSE]] (평균 제곱 오차) |
| **사용 사례**    | 스팸 탐지, 이미지 분류, 의료 진단        | 수익 예측, 기온 예측, 산불 위험 예측  |                  |

분류 트리는 노드의 **불순도(Impurity)**를 낮추는 방향으로, 회귀 트리는 노드의 **분산(Variance)**을 낮추는 방향으로 학습한다고 요약할 수 있습니다.

## 3. 회귀 트리의 생성 과정

회귀 트리는 데이터셋을 하위 집합으로 **재귀적으로 분할**하여 정보를 최대화하고, 분할 노드의 임의성을 최소화하는 트리 구조를 생성합니다.

### 분할 방법

-   **연속형 특징**: 임계값 $\alpha$로 분할합니다. 데이터가 $\alpha$보다 작으면 왼쪽 노드로, 데이터가 $\alpha$보다 크거나 같으면 오른쪽 노드로 분할됩니다.
-   **이진 특징**: 두 클래스에 따라 분할됩니다.

## 4. 예측값 계산

회귀 트리의 리프 노드에서는 해당 노드에 속한 데이터 포인트들의 목표값으로 예측을 수행합니다.

### 기본 방법: 평균

특정 노드의 예측값 $\hat{y}$는 해당 노드에 있는 데이터 요소의 실제 목표값 **평균**으로 계산됩니다.

$$ \hat{y} = \frac{1}{n}\sum_{i=1}^{n} y_i $$

### 대안: 중위값

-   **사용 시기**: 데이터가 왜곡된 경우 (이상치에 민감할 때) 사용될 수 있습니다.
-   **장점**: 이상값에 덜 민감합니다.
-   **단점**: 계산 비용이 평균보다 더 높을 수 있습니다.
-   **정규 분포**: 데이터가 정규 분포를 따를 경우, 중위값은 평균과 유사합니다.

## 5. 분할 품질 측정: MSE (평균 제곱 오차)

회귀 트리에서 노드를 분할하는 기준은 [[../MSE (평균 제곱 오차)|MSE]]를 기반으로 합니다.

### MSE 정의

각 노드 내 목표 값의 변동을 측정하여 값이 얼마나 분산되어 있는지 확인합니다.

-   **분산이 작음** = 값이 가깝게 일치 = 좋은 분할
-   **목표**: 분할 후 각 하위 노드의 MSE가 최대한 낮아지도록 만드는 것입니다.

### 가중 평균 MSE

분할의 품질을 평가하기 위해 하위 노드들의 MSE를 가중 평균하여 사용합니다.

$$ \text{Weighted MSE} = \frac{n_L}{n_L + n_R} \times \text{MSE}_L + \frac{n_R}{n_L + n_R} \times \text{MSE}_R $$

-   $n_L, n_R$: 왼쪽/오른쪽 노드의 관측치 수
-   $\text{MSE}_L, \text{MSE}_R$: 각 노드의 MSE

**목표**: 가중 MSE를 최소화하는 분할을 선택합니다.

## 6. 훈련 과정 상세

1.  각 잠재적 특징 분할에 대해 왼쪽/오른쪽 하위 집합의 MSE를 계산합니다.
2.  분할의 MSE는 하위 집합 MSE의 가중 평균으로 계산됩니다.
3.  **가장 낮은 가중 MSE**를 가진 분할을 선택합니다.
4.  이 과정을 통해 예측값의 분산을 최소화하고 정확도를 향상시킵니다.

## 7. 특징 유형별 처리

### 이진 특징

-   임계값 없이 두 클래스로 분리됩니다.
-   분할 품질은 클래스 MSE의 가중 평균으로 평가됩니다.
-   가능한 결과가 하나뿐이므로 이미 최적화된 분할로 간주될 수 있습니다.

### 다중 클래스 특징

-   One-vs-One 또는 One-vs-All 전략으로 이진 분할을 생성합니다.
-   각 이진 분할의 가중 MSE를 계산합니다.
-   예측 분산이 가장 낮은 분할을 선택합니다.

### 연속형 특징

**임계값 선택 전략**:

1.  특성 값을 오름차순으로 정렬합니다.
2.  중복 값을 제거합니다.
3.  후보 임계값 $\alpha_i$는 연속 값 쌍 사이의 중간점으로 설정합니다.
    $$ \alpha_i = \frac{X_i + X_{i+1}}{2} $$
4.  가중 MSE를 최소화하는 임계값을 선택합니다.

**한계**:
-   철저한 검색 방법은 빅데이터에 확장하기 어렵습니다.
-   균일 분포를 가정하는 경우가 많습니다.

**개선 방법**:
-   큰 데이터셋에서는 임계값 하위 집합을 선택하여 효율성을 높일 수 있지만, 정확도는 낮아질 수 있습니다.
-   데이터 분포를 고려한 샘플링 방법을 적용할 수 있습니다.

## 8. 핵심 요약

-   회귀 트리는 **연속형 값 예측**을 위한 [[01. 의사 결정 트리|의사 결정 트리]]입니다.
-   [[../MSE (평균 제곱 오차)|MSE]]를 기준으로 최적 분할을 선택합니다.
-   리프 노드의 예측값은 평균 (또는 중위값)입니다.
-   가중 MSE 최소화로 예측 정확도를 향상시킵니다.
-   특징 유형(이진/다중/연속)에 따라 다른 분할 전략을 적용합니다.
-   빅데이터에서는 효율성과 정확성 간의 트레이드오프를 고려해야 합니다.

#ai #regression #decision-tree #regression-tree #mse
