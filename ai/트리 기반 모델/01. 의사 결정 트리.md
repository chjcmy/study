# 01. 의사 결정 트리 (Decision Tree)

## 1. 핵심 정의

**의사 결정 트리(Decision Tree)**는 특정 항목에 대한 관측치를 바탕으로 목표 값에 대한 결론에 도달하는 데 사용되는 지도 학습(Supervised Learning) 알고리즘입니다. 이름 그대로, 데이터를 분류하거나 결과를 예측하는 일련의 결정 규칙들을 **나무 구조**로 시각화한 것입니다.

-   **분류 트리 (Classification Tree)**: 범주형 목표 변수(클래스)를 예측합니다.
-   **[[01.1. 회귀 트리|회귀 트리 (Regression Tree)]]**: 연속형 목표 변수(수치)를 예측합니다.

-   **비유**: '스무고개' 게임과 유사합니다. "이 동물의 다리는 4개인가요?", "날개가 있나요?" 와 같은 질문을 연속적으로 던져 정답을 좁혀나가는 과정과 같습니다.

## 2. 트리 구조의 구성 요소

-   **루트 노드 (Root Node)**: 트리의 가장 최상위에 위치하는 시작 노드. 전체 데이터를 포함합니다.
-   **내부 노드 (Internal Node / Decision Node)**: 데이터를 분할하는 '질문' 또는 '규칙'을 나타내는 노드입니다.
-   **가지 (Branch)**: 내부 노드의 질문에 대한 답변(결과)을 나타내며, 하위 노드로 연결됩니다.
-   **리프 노드 (Leaf Node / Terminal Node)**: 더 이상 분할되지 않는 트리의 끝 노드. 최종적인 **결정(클래스 분류 또는 회귀 값 예측)**을 나타냅니다.

## 3. 의사 결정 트리의 학습 원리: 재귀적 분할

의사 결정 트리는 **재귀적 분할(Recursive Partitioning)**이라는 과정을 통해 학습됩니다.

1.  **최적의 질문 찾기**: 모든 독립 변수와 가능한 모든 분할 지점을 고려하여, 데이터를 가장 '순수하게(pure)' 나눌 수 있는 최적의 질문(규칙)을 찾습니다.
2.  **데이터 분할**: 찾은 규칙에 따라 데이터를 두 개 이상의 하위 그룹으로 나눕니다.
3.  **재귀적 반복**: 각 하위 그룹에 대해 1, 2번 과정을 반복합니다.
4.  **중단**: 더 이상 데이터를 나눌 수 없거나, 미리 정해진 중단 규칙(예: 트리의 최대 깊이, 노드의 최소 샘플 수)에 도달하면 분할을 멈추고 해당 노드를 리프 노드로 만듭니다.

### 어떻게 '최적의 질문'을 찾을까? - 불순도(Impurity)

트리는 각 분할 단계에서 **불순도(Impurity)**를 가장 많이 감소시키는 방향으로 학습합니다. 불순도는 한 노드 안에 얼마나 다양한 클래스의 데이터가 섞여 있는지를 나타내는 지표입니다.

-   **목표**: 분할 후 각 하위 노드의 불순도가 최대한 낮아지도록(즉, 한 가지 클래스의 데이터만 존재하도록) 만드는 것입니다.
-   **정보 이득 (Information Gain)**: 분할 전의 불순도와 분할 후의 불순도의 차이. 의사 결정 트리는 **정보 이득을 최대화**하는 분할을 찾습니다.

#### 대표적인 불순도 측정 지표

1.  **지니 불순도 (Gini Impurity)**:
    $$ Gini = 1 - \sum_{i=1}^{C} (p_i)^2 $$
    -   $C$: 클래스의 수
    -   $p_i$: 해당 노드에서 클래스 $i$에 속하는 샘플의 비율
    -   값이 0이면 노드가 완전히 순수하다는 의미이며, 값이 클수록 불순도가 높습니다. CART(Classification and Regression Trees) 알고리즘에서 주로 사용됩니다.

2.  **엔트로피 (Entropy)**:
    $$ Entropy = - \sum_{i=1}^{C} p_i \log_2(p_i) $$
    -   정보 이론의 개념으로, 데이터의 불확실성을 측정합니다.
    -   값이 0이면 불확실성이 전혀 없다는(완전히 순수하다는) 의미이며, 값이 클수록 불확실성이 높습니다. ID3, C4.5, C5.0 알고리즘에서 사용됩니다.

## 4. 과적합 방지: 가지치기 (Pruning)

아무런 제약 없이 트리를 성장시키면 학습 데이터의 모든 세부 사항(노이즈 포함)까지 학습하여 매우 복잡한 트리가 만들어집니다. 이는 [[과적합(Overfitting)]]을 유발하여 새로운 데이터에 대한 예측 성능을 떨어뜨립니다. 이를 방지하기 위해 **가지치기(Pruning)**를 수행합니다.

-   **사전 가지치기 (Pre-pruning)**: 트리가 완전히 성장하기 전에, 특정 조건을 만족하면 미리 성장을 멈추는 방법입니다. (예: 트리의 최대 깊이 제한, 리프 노드가 되기 위한 최소 샘플 수 지정)
-   **사후 가지치기 (Post-pruning)**: 먼저 트리를 최대한 성장시킨 후, 불필요한 가지를 제거하는 방법입니다.

## 5. 장점과 단점

### 장점

-   **해석의 용이성**: 모델의 결정 과정을 시각적으로 이해하기 쉬워, 비전문가에게 설명하기 좋습니다 (화이트박스 모델).
-   **적은 데이터 전처리**: 수치형, 범주형 데이터를 모두 다룰 수 있으며, 데이터 스케일링의 영향을 받지 않습니다.
-   **비선형 관계 모델링**: 복잡한 비선형 관계도 모델링할 수 있습니다.

### 단점

-   **과적합 경향**: 데이터의 작은 변화에도 민감하게 반응하여 과적합되기 쉽습니다.
-   **불안정성**: 학습 데이터가 약간만 변해도 트리의 구조가 크게 바뀔 수 있습니다.
-   **편향**: 특정 클래스가 데이터의 대부분을 차지할 경우, 편향된 트리가 만들어질 수 있습니다.

> 이러한 단점들을 보완하기 위해 여러 개의 트리를 결합하는 [[02. 랜덤 포레스트|랜덤 포레스트]]나 [[03. 그래디언트 부스팅 머신|그래디언트 부스팅]]과 같은 **앙상블(Ensemble)** 기법이 널리 사용됩니다.

## 6. 관련 모델 (Related Models)

의사 결정 트리의 단점, 특히 과적합에 대한 민감성을 극복하기 위해 여러 개의 트리를 사용하는 앙상블 모델이 개발되었습니다.

-   **[[02. 랜덤 포레스트]]**: 여러 개의 독립적인 의사 결정 트리를 만들어 그 결과를 종합하는 방식입니다.
-   **[[03. 그래디언트 부스팅 머신]]**: 간단한 트리를 순차적으로 만들어 이전 트리의 오차를 보완해나가는 방식입니다.

#ai #classification #decision-tree #gini #entropy #overfitting
