# Multi-Layer Perceptron (MLP)

## ðŸ“Œ ê°œë… (Concept)
ê°€ìž¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ **ì¸ê³µ ì‹ ê²½ë§(Artificial Neural Network)**ìž…ë‹ˆë‹¤. ìž…ë ¥ì¸µ(Input), ì€ë‹‰ì¸µ(Hidden), ì¶œë ¥ì¸µ(Output)ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ì¸µì˜ ë‰´ëŸ°ë“¤ì´ ë³µìž¡í•˜ê²Œ ì—°ê²°ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

- **í•µì‹¬ ì—­í• **: **"ë¹„ì„ í˜•ì (Non-linear) ê´€ê³„ í•™ìŠµ"**
- **ë¹„ìœ **: 
    - ë‹¨ìˆœí•œ ê³±í•˜ê¸°(ì„ í˜•)ë¡œëŠ” "Aë¥¼ ì¢‹ì•„í•˜ë©´ Bë„ ì¢‹ì•„í•œë‹¤" ì •ë„ë§Œ ì•Œ ìˆ˜ ìžˆì§€ë§Œ,
    - MLPëŠ” "Aë¥¼ ì¢‹ì•„í•˜ëŠ”ë° ì£¼ë§ ë°¤ì—ëŠ” Bë¥¼ ì‹«ì–´í•˜ê³  ëŒ€ì‹  Cë¥¼ ì°¾ëŠ”ë‹¤" ê°™ì€ ë³µìž¡í•œ íŒ¨í„´ì„ íŒŒì•…í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

## âš™ï¸ êµ¬ì¡° (Architecture)
1.  **Linear Layer (Dense Layer)**: ìž…ë ¥ê°’ì— ê°€ì¤‘ì¹˜(Weight)ë¥¼ ê³±í•˜ê³  íŽ¸í–¥(Bias)ì„ ë”í•¨. 
    - ìˆ˜ì‹: $y = Wx + b$
    - ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ì „íŒŒ: $h^{(l)} = \sigma(W^{(l)}h^{(l-1)} + b^{(l)})$
2.  **Activation Function (í™œì„±í™” í•¨ìˆ˜)**: ReLU, Sigmoid ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬í•¨. (ì´ê²Œ ì—†ìœ¼ë©´ ì•„ë¬´ë¦¬ ì¸µì„ ìŒ“ì•„ë„ ì„ í˜• ëª¨ë¸ê³¼ ê°™ìŒ)
3.  **Dropout**: ê³¼ì í•©(Overfitting)ì„ ë§‰ê¸° ìœ„í•´ í•™ìŠµ ì¤‘ ì¼ë¶€ ë‰´ëŸ°ì„ ëžœë¤í•˜ê²Œ ë”.

## ðŸ’¡ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œì˜ í™œìš©
- **NCF**: ìœ ì €ì™€ ì•„ì´í…œ ìž„ë² ë”©ì„ ì´ì–´ ë¶™ì—¬ì„œ(Concat) MLPì— í†µê³¼ì‹œì¼œ ë³µìž¡í•œ ìƒí˜¸ìž‘ìš©ì„ í•™ìŠµí•¨.
- **Wide & Deep**: Deep Partê°€ ë°”ë¡œ ì´ MLP êµ¬ì¡°ìž„.

## ðŸ’» ì½”ë“œ ì˜ˆì‹œ (PyTorch)
```python
mlp = nn.Sequential(
    nn.Linear(64, 32),   # ìž…ë ¥ 64 -> ì¶œë ¥ 32
    nn.ReLU(),           # í™œì„±í™” í•¨ìˆ˜
    nn.Dropout(0.2),     # 20% ë‰´ëŸ° ë„ê¸°
    nn.Linear(32, 16),   # ìž…ë ¥ 32 -> ì¶œë ¥ 16
    nn.ReLU()
)
```
