# 01.1. K-평균 클러스터링 (K-Means Clustering)

## 1. 핵심 정의

**K-평균(K-Means)**은 **중심(centroid) 간의 거리**를 기반으로 데이터셋을 유사한 그룹으로 나누는 **반복적인 중심 기반 클러스터링 알고리즘**입니다. [[01. 클러스터링 알고리즘|클러스터링]]의 한 종류로, 데이터를 미리 정해진 $K$개의 비중첩 클러스터로 분할합니다.

## 2. 기본 원리

### 목표

K-Means의 목표는 각 클러스터 내의 데이터 응집도를 최대화하고, 클러스터 간의 분리도를 최대화하는 것입니다. 이는 수학적으로 **클러스터 내 분산(intra-cluster variance)을 최소화**하는 것과 같습니다.

-   **중심(Centroid)**: 한 클러스터 내 모든 데이터 포인트들의 **기하학적 중심(평균 위치)**을 의미합니다.

### K 값의 의미

-   **$K$가 큼**: 클러스터의 크기가 작아지고, 데이터의 세부적인 구조를 더 많이 포착할 수 있습니다.
-   **$K$가 작음**: 클러스터의 크기가 커지고, 데이터의 큰 구조를 파악하게 됩니다.

## 3. 알고리즘 작동 방식

K-Means는 다음의 반복적인 과정을 통해 최적의 클러스터를 찾아갑니다.

1.  **초기화 (Initialization)**
    -   클러스터의 개수 $K$를 사용자가 직접 선택합니다.
    -   $K$개의 초기 중심(centroid) 위치를 훈련 데이터 중에서 무작위로 선택하거나, 특정 전략에 따라 설정합니다.

2.  **반복 과정 (Iteration)**
    -   **Step 1: 할당 (Assignment)**
        -   각 데이터 포인트에서 모든 중심까지의 거리를 계산합니다.
        -   각 데이터 포인트를 가장 가까운 중심이 속한 클러스터에 할당합니다.
    -   **Step 2: 업데이트 (Update)**
        -   각 클러스터에 할당된 모든 데이터 포인트들의 평균 위치를 계산하여, 해당 클러스터의 중심을 새로운 위치로 업데이트합니다.

3.  **수렴 (Convergence)**
    -   2단계의 할당-업데이트 과정을 반복합니다.
    -   더 이상 중심의 위치가 변하지 않거나, 미리 정해진 최대 반복 횟수에 도달하면 알고리즘이 수렴했다고 판단하고 멈춥니다.

### 수학적 목표

K-Means는 모든 클러스터에 대한 **클러스터 내 분산(intra-cluster variance)**의 합을 동시에 최소화하는 것을 목표로 합니다.

$$ \min \sum_{i=1}^{K} \sum_{x \in C_i} ||x - \mu_i||^2 $$

-   $C_i$: $i$번째 클러스터
-   $\mu_i$: $i$번째 클러스터의 중심
-   $x$: 클러스터 내 각 데이터 포인트

## 4. K-Means의 한계

### 1. 불균형 클러스터

-   **문제**: 각 클러스터에 속한 데이터 포인트의 개수가 크게 다를 경우 제대로 작동하지 않을 수 있습니다.
-   **원인**: K-Means는 각 클러스터의 크기가 비슷하다고 가정하는 경향이 있어, 작은 클러스터의 중심이 큰 클러스터 쪽으로 끌려가고, 큰 클러스터가 작은 클러스터의 데이터를 흡수하는 현상이 발생할 수 있습니다.

### 2. 볼록성 가정

-   **가정**: K-Means는 클러스터가 원형 또는 볼록(convex)한 형태일 것이라고 가정합니다.
-   **문제**: 초승달, 고리 모양 등 볼록하지 않은 복잡한 형태의 클러스터는 처리하기 어렵습니다.

### 3. 노이즈와 이상값에 대한 민감성

-   중심을 계산할 때 평균을 사용하므로, 통계적 분산이 이상값(outlier)에 민감합니다. 노이즈나 이상값이 존재할 경우 중심의 위치가 크게 왜곡될 수 있습니다.

## 5. 최적의 K 값 선택 방법

K-Means의 성능은 $K$ 값에 크게 의존하므로, 최적의 $K$를 찾는 것이 매우 중요합니다.

### 휴리스틱 기법

1.  **엘보우 방법 (Elbow Method)**:
    -   $K$ 값을 1부터 점차 늘려가면서 각 $K$에 대한 클러스터 내 분산의 합(WCSS)을 계산하고 그래프로 그립니다.
    -   그래프가 팔꿈치처럼 급격히 꺾이는 지점(Elbow)이 최적의 $K$ 값이 됩니다. 이 지점 이후로는 $K$를 늘려도 성능 개선이 미미합니다.

2.  **실루엣 분석 (Silhouette Analysis)**:
    -   각 데이터 포인트가 자신이 속한 클러스터와 얼마나 유사하고(응집력), 다른 클러스터와는 얼마나 다른지(분리도)를 측정하는 방법입니다.
    -   실루엣 점수는 -1에서 1 사이의 값을 가지며, 1에 가까울수록 클러스터링이 잘 되었다는 의미입니다. 모든 데이터의 평균 실루엣 점수가 가장 높은 $K$를 선택합니다.

3.  **데이비스-볼딘 지수 (Davies-Bouldin Index)**:
    -   클러스터 간의 유사도를 측정하는 지표로, 값이 작을수록 클러스터가 잘 분리되었음을 의미합니다.

## 6. 장점과 단점

### 장점

-   **효율성 및 확장성**: 알고리즘이 비교적 간단하고 빨라 빅데이터에 적합합니다.
-   **단순성**: 구현과 이해가 쉽습니다.

### 단점

-   **K 값 사전 지정**: 최적의 클러스터 개수 $K$를 미리 지정해야 합니다.
-   **초기 중심 민감성**: 초기 중심을 어떻게 선택하는지에 따라 결과가 달라질 수 있습니다.
-   **형태 제약**: 볼록하지 않은 형태나 밀도가 다른 클러스터 처리에 취약합니다.
-   **이상값 민감성**: 이상값에 의해 중심이 왜곡될 수 있습니다.

## 7. 핵심 요약

-   K-Means는 **거리 기반**의 비지도 클러스터링 알고리즘입니다.
-   **반복적으로 중심을 업데이트**하며 클러스터 내 분산을 최소화합니다.
-   **볼록하고 균등한 크기**의 클러스터에 최적화되어 있습니다.
-   **최적의 K 선택**이 성능에 매우 중요하며, 엘보우 방법이나 실루엣 분석 등을 사용합니다.
-   빅데이터에 효율적이지만 노이즈, 이상값, 불균형 데이터에 취약한 단점이 있습니다.

#ai #machine-learning #unsupervised-learning #clustering #kmeans
