# 09 벡터와 행렬: AI의 언어, 데이터를 담는 그릇

## 1. 왜 벡터와 행렬을 배워야 할까요?

미적분이 AI 모델을 '학습'시키는 동적인 도구라면, 벡터와 행렬은 AI가 사용하는 '언어'이자 데이터를 담는 '그릇'입니다. 선형대수학의 핵심인 벡터와 행렬은 컴퓨터가 데이터를 효율적으로 표현하고 처리하는 방식을 정의합니다.

- **데이터 표현**: 컴퓨터는 숫자, 글자, 이미지, 소리 등 모든 것을 숫자의 배열로 처리합니다. 예를 들어, $256 \times 256$ 픽셀의 흑백 이미지는 $256 \times 256$ 크기의 행렬(숫자 격자)로 표현할 수 있습니다. 사용자의 특성(나이, 성별, 구매 기록)은 하나의 벡터(숫자 리스트)로 나타낼 수 있습니다.
- **계산의 효율성**: 벡터와 행렬 연산은 여러 개의 계산을 한 번에 묶어서 처리할 수 있게 해줍니다. 이는 GPU(그래픽 처리 장치)를 사용한 병렬 처리에 최적화되어 있어, 대규모 데이터셋을 사용하는 딥러닝 모델의 고속 계산을 가능하게 합니다.

결론적으로, AI를 깊이 이해하려면 데이터가 벡터와 행렬 형태로 어떻게 표현되고, 이들이 어떻게 상호작용하는지 반드시 알아야 합니다.

## 2. 벡터(Vector): 크기와 방향을 가진 리스트

**벡터는 순서가 있는 숫자의 리스트**입니다. 각 숫자는 벡터의 '요소(element)' 또는 '성분(component)'이라고 부릅니다.

> 예시: 2차원 벡터 $\vec{v} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}$, 3차원 벡터 $\vec{u} = \begin{bmatrix} 3 \\ 1 \\ 4 \end{bmatrix}$

벡터는 두 가지 중요한 정보를 담고 있습니다.

1.  **크기 (Magnitude/Norm)**: 벡터의 길이. 원점 $(0,0)$에서 벡터가 가리키는 점까지의 거리입니다. (피타고라스 정리로 계산)
2.  **방향 (Direction)**: 벡터가 공간에서 가리키는 방향.

AI에서는 사용자의 프로필, 단어의 의미, 이미지의 특징 등 다양한 데이터를 하나의 벡터로 표현합니다.

### 벡터의 기본 연산

- **덧셈과 뺄셈**: 같은 위치의 요소끼리 더하거나 뺍니다. (벡터의 차원은 같아야 합니다.)
  > $$\begin{bmatrix} 2 \\ 1 \end{bmatrix} + \begin{bmatrix} 3 \\ 4 \end{bmatrix} = \begin{bmatrix} 2+3 \\ 1+4 \end{bmatrix} = \begin{bmatrix} 5 \\ 5 \end{bmatrix}$$
- **스칼라 곱 (Scalar Multiplication)**: 벡터의 모든 요소에 하나의 숫자(스칼라)를 곱합니다. 벡터의 방향은 그대로이고 크기만 변합니다.
  > $$3 \cdot \begin{bmatrix} 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \cdot 2 \\ 3 \cdot 1 \end{bmatrix} = \begin{bmatrix} 6 \\ 3 \end{bmatrix}$$

## 3. 행렬(Matrix): 벡터를 담는 격자

**행렬은 숫자를 직사각형 모양의 격자로 배열한 것**입니다. 행렬은 여러 개의 벡터를 행(row) 또는 열(column)으로 묶어놓은 것으로 생각할 수 있습니다.

행렬의 크기는 (행의 수) $\times$ (열의 수) 로 나타냅니다.

> 예시: $2 \times 3$ 행렬 $$ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} $$

AI에서 행렬은 이미지 데이터, 데이터셋(행=샘플, 열=특성), 신경망의 가중치 등 매우 다양한 형태로 활용됩니다.

### 행렬의 기본 연산

- **덧셈과 뺄셈**: 벡터와 마찬가지로, 같은 위치의 요소끼리 더하거나 뺍니다. (행렬의 차원은 같아야 합니다.)
- **행렬 곱 (Matrix Multiplication)**: **가장 중요하고 조금 까다로운 연산입니다.**

  행렬 $A$와 $B$의 곱 $C = AB$는 다음과 같이 계산됩니다.

  1.  **조건**: $A$의 열(column) 개수와 $B$의 행(row) 개수가 같아야 합니다.
  2.  **계산**: 결과 행렬 $C$의 $i$번째 행, $j$번째 열의 요소 $C_{ij}$는 $A$의 $i$번째 행 벡터와 $B$의 $j$번째 열 벡터의 **내적(dot product)** 으로 계산됩니다.

  > **(앞 행렬의 행)과 (뒷 행렬의 열)을 순서대로 곱해서 더한다.**

  **예시: ($2 \times 2$ 행렬) $\times$ ($2 \times 2$ 행렬)**
  $$ \begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} e & f \\ g & h \end{bmatrix} = \begin{bmatrix} ae+bg & af+bh \\ ce+dg & cf+dh \end{bmatrix} $$

  **중요!**: 행렬 곱은 교환 법칙이 성립하지 않습니다. 즉, **$AB \neq BA$** 입니다.

## 4. 벡터와 행렬, AI에서 어떻게 쓰일까?

신경망의 한 층(layer)에서 일어나는 계산은 본질적으로 행렬 연산입니다.

> $$\text{Output} = \text{activation}(\text{Input} \cdot \text{Weight} + \text{Bias})$$

- **Input_Matrix**: 입력 데이터. 예를 들어, 100개의 이미지 데이터(각 이미지는 784개의 픽셀)가 있다면 $100 \times 784$ 행렬이 됩니다.
- **Weight_Matrix**: 해당 층의 학습 가능한 파라미터(가중치)들. 모델이 학습하면서 이 행렬의 값들이 계속 업데이트됩니다.
- **Bias_Vector**: 편향. 각 뉴런에 더해지는 값입니다.

이처럼 복잡해 보이는 신경망의 연산도 결국은 간단한 행렬 곱과 합으로 이루어져 있습니다. 이것이 바로 GPU를 통해 수많은 연산을 병렬적으로 빠르게 처리할 수 있는 이유입니다.

## 확인 문제

1.  두 벡터 $\vec{a} = \begin{bmatrix} 1, 2, 3 \end{bmatrix}$ 와 $\vec{b} = \begin{bmatrix} 4, 5, 6 \end{bmatrix}$ 의 합 $\vec{a} + \vec{b}$ 와, 스칼라 곱 $2\vec{a}$ 를 구해보세요.
2.  두 행렬 $$ A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} $$ 와 $$ B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} $$ 의 곱 $AB$ 를 계산해보세요.
3.  $BA$도 계산해보고 $AB$와 결과가 다른지 확인해보세요.

---

*벡터와 행렬은 선형대수학의 시작이자 AI를 이해하는 첫걸음입니다. 특히 행렬 곱셈은 손으로 직접 몇 번 계산해보면서 그 원리를 익히는 것이 매우 중요합니다.*
