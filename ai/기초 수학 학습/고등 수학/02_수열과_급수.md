# 02 수열과 급수: 순차적 데이터와 알고리즘의 이해

## 1. 수열 (Sequence): 규칙을 가진 숫자의 나열

**수열(Sequence)** 은 간단히 말해, **규칙적으로 나열된 숫자들의 목록**입니다. 각 숫자를 **항(term)** 이라고 부르며, n번째 항을 $a_n$으로 표기합니다.

-   **등차수열 (Arithmetic Sequence)**: 각 항이 그 앞의 항에 일정한 수(공차, $d$)를 더하여 만들어지는 수열입니다.
    -   예시: `2, 5, 8, 11, 14, ...` (공차 $d=3$)
    -   일반항: $a_n = a_1 + (n-1)d$

-   **등비수열 (Geometric Sequence)**: 각 항이 그 앞의 항에 일정한 수(공비, $r$)를 곱하여 만들어지는 수열입니다.
    -   예시: `3, 6, 12, 24, 48, ...` (공비 $r=2$)
    -   일반항: $a_n = a_1 r^{n-1}$

## 2. 급수 (Series): 수열의 합

**급수(Series)** 는 수열의 항들을 더하는 것입니다. 보통 합의 기호인 **시그마($\Sigma$)** 를 사용하여 표현합니다.

-   **유한 급수**: 첫째 항부터 n번째 항까지의 합. $$S_n = \sum_{k=1}^{n} a_k$$
-   **무한 급수**: 수열의 모든 항을 무한히 더하는 것. $$S = \sum_{n=1}^{\infty} a_n$$

### 무한 등비 급수의 수렴

무한히 더하는데 어떻게 값이 하나로 정해질 수 있을까요? 이는 무한 등비 급수에서 공비 $r$의 크기에 따라 결정됩니다.

-   **수렴 (Convergence)**: 공비의 절댓값이 1보다 작을 때 ($|r| < 1$), 급수의 합은 일정한 값에 가까워집니다. 이 값을 **합(Sum)** 이라고 합니다.
    -   **무한 등비 급수의 합**: $$S = \frac{a_1}{1 - r}$$
-   **발산 (Divergence)**: $|r| \geq 1$ 이면, 급수의 합은 특정 값으로 정해지지 않고 계속 커지거나 진동합니다.

## 3. AI와 수열/급수의 연결점

수열과 급수는 순차적인 현상을 수학적으로 모델링하는 언어이며, AI의 여러 중요 개념과 깊이 연결됩니다.

### 1. 순차 데이터 (Sequential Data) 그 자체

-   **시계열 데이터 (Time-Series Data)**: 주식 가격, 기온 변화, 음성 데이터 등 시간의 흐름에 따라 기록된 데이터는 본질적으로 하나의 수열입니다.
-   **자연어 (Natural Language)**: 문장을 구성하는 단어들의 나열은 순서가 매우 중요한 수열입니다.

### 2. 순환 신경망 (Recurrent Neural Networks, RNN)

RNN은 순차 데이터를 처리하기 위해 설계된 특별한 신경망 구조입니다.

-   **RNN의 상태 업데이트**: RNN의 특정 시점 $t$에서의 상태 $h_t$는 이전 시점의 상태 $h_{t-1}$와 현재 시점의 입력 $x_t$에 의해 결정됩니다. $h_t = f(h_{t-1}, x_t)$. 이는 **점화식(recurrence relation)** 으로 정의된 수열과 정확히 같은 구조입니다.
-   **기울기 소실/폭주 (Vanishing/Exploding Gradients)**: RNN을 학습시킬 때, 과거의 정보가 현재까지 전달되는 과정에서 기울기가 너무 작아지거나(소실) 너무 커지는(폭주) 문제가 발생할 수 있습니다. 이 현상은 RNN의 가중치 행렬이 반복적으로 곱해지는 과정에서 발생하며, 이는 **등비수열의 공비 $r$의 크기에 따라 값이 0으로 수렴하거나 무한대로 발산하는 원리와 정확히 일치**합니다. 이 문제를 이해하고 해결하는 데 수열의 수렴/발산 개념이 핵심적인 통찰을 제공합니다.

### 3. 최적화 알고리즘 (Optimization Algorithms)

-   **파라미터 업데이트 과정**: 경사 하강법과 같은 최적화 알고리즘에서 모델의 파라미터는 매 스텝마다 조금씩 업데이트됩니다. 이 파라미터들의 변화 과정은 하나의 수열로 볼 수 있습니다.
-   **수렴 분석**: 학습률(learning rate)과 같은 하이퍼파라미터가 적절하게 설정되었을 때, 이 파라미터 수열이 최적의 값으로 **수렴**하는지를 수학적으로 분석하는 데 수열과 급수 이론이 사용됩니다.

### 4. 테일러 급수 (Taylor Series)

-   이전 장에서 배운 테일러 급수는 복잡한 함수를 다항 함수의 **무한 급수**로 표현하는 것입니다. 이는 AI 최적화 이론의 근간을 이룹니다.

---

*수열과 급수는 단순히 숫자의 나열과 합을 넘어, 순서가 있는 모든 현상을 수학적으로 분석하는 틀을 제공합니다. 특히 RNN의 동작 원리와 학습의 안정성을 이해하는 데 결정적인 역할을 하므로, 그 기본 원리를 잘 파악해두는 것이 중요합니다.*