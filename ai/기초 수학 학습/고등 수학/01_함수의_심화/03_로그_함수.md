# 03 로그 함수 (Logarithmic Functions): 정보와 스케일의 지배자

## 1. 로그 함수의 정의와 특징

**로그 함수(Logarithmic Function)**는 지수 함수의 역함수로, "어떤 수를 만들기 위해 밑을 몇 번 곱해야 하는가?"에 대한 답을 주는 함수입니다.

$$y = \log_a(x) \quad \iff \quad a^y = x \quad (\text{단, } a > 0, a \neq 1, x > 0)$$

-   **밑 (Base, $a$)**: 기준이 되는 숫자입니다.
-   **진수 (Argument, $x$)**: 로그를 취하는 대상 숫자로, 항상 양수여야 합니다.

### 주요 특징

-   **정의역과 치역**: 정의역은 모든 양의 실수($x > 0$)이며, 치역은 모든 실수입니다.
-   **점근선**: 그래프는 $y$축($x=0$)에 한없이 가까워지지만 절대 닿지 않습니다. $y$축이 수직 점근선입니다.
-   **핵심 성질**: 매우 큰 수의 범위를 작은 수의 범위로 압축(compress)합니다. 이는 데이터의 스케일을 조정하거나, 곱셈을 덧셈으로 변환하여 계산을 용이하게 만드는 데 결정적인 역할을 합니다.

## 2. 로그의 강력한 성질들

로그의 진정한 힘은 복잡한 연산을 단순한 연산으로 바꿔주는 성질에서 나옵니다.

-   **곱셈을 덧셈으로**: $\log(xy) = \log(x) + \log(y)$
-   **나눗셈을 뺄셈으로**: $\log(x/y) = \log(x) - \log(y)$
-   **거듭제곱을 곱셈으로**: $\log(x^p) = p \log(x)$

이 성질들, 특히 곱셈을 덧셈으로 바꾸는 능력은 수많은 확률 값들의 곱을 다뤄야 하는 AI 모델에서 계산을 가능하게 만드는 핵심 열쇠입니다.

## 3. 로그 함수와 AI: 계산, 손실, 그리고 정보

### 계산 안정성 (Log-Likelihood)

머신러닝 모델이 특정 데이터가 나타날 확률을 계산할 때, 이는 종종 각 데이터 포인트가 나타날 확률의 곱으로 표현됩니다.
$$ P(\text{전체 데이터}) = P(\text{데이터}_1) \times P(\text{데이터}_2) \times \dots \times P(\text{데이터}_n) $$
0과 1 사이의 작은 확률들을 수없이 곱하면 그 결과는 컴퓨터가 0으로 처리해버리는 **수치 언더플로우(numerical underflow)** 현상을 일으킬 수 있습니다.

여기에 로그를 취하면, 이 문제가 해결됩니다.
$$ \log P(\text{전체 데이터}) = \sum_{i=1}^{n} \log P(\text{데이터}_i) $$
곱셈이 덧셈으로 바뀌면서 계산이 안정적이고 미분하기도 쉬워집니다. 이를 **로그 우도(Log-Likelihood)** 라고 하며, 최대 우도 추정(MLE)의 핵심 기법입니다.

### 손실 함수 (Loss Function)

분류 문제의 표준 손실 함수인 **교차 엔트로피(Cross-Entropy)** 는 로그를 사용하여 정의됩니다.
$$ \text{Loss} = - \sum y_{\text{true}} \log(y_{\text{pred}}) $$
로그 함수의 그래프를 생각해보면($x$가 0에 가까워질수록 $y$는 $-\infty$로 감), 왜 이 함수가 효과적인지 알 수 있습니다.

-   만약 정답($y_{\text{true}}=1$)인데, 모델의 예측 확률이 1에 가깝다면 ($y_{	ext{pred}} \to 1$), $\log(y_{	ext{pred}})$는 0에 가까워져 손실(Loss)이 거의 없습니다.
-   하지만 정답($y_{	ext{true}}=1$)인데, 모델의 예측 확률이 0에 가깝다면 ($y_{	ext{pred}} \to 0$), $\log(y_{	ext{pred}})$는 음의 무한대($-\infty$)로 향하고, 앞에 붙은 마이너스 부호 때문에 손실(Loss)은 **무한대**로 치솟게 됩니다.

이처럼 로그는 모델이 **자신 있게 틀리는 예측에 대해 기하급수적인 페널티**를 부여하여, 모델이 더 정확한 방향으로 빠르게 학습하도록 만듭니다.

### 데이터 스케일링 및 정보량

-   **데이터 변환**: 한쪽에 데이터가 몰려있는 왜곡된(skewed) 분포를 가진 데이터에 로그를 취하면, 분포가 정규 분포에 가까워져 모델이 더 쉽게 패턴을 학습할 수 있도록 돕습니다.
-   **정보량**: 정보 이론에서 정보량은 $I(x) = -\log_2 P(x)$ 로 정의됩니다. 로그는 확률(예측 가능성)과 정보량(놀라움)을 연결하는 다리 역할을 합니다.

---

## 4. 확인 문제 (Practice Problems)

1.  **로그 성질 응용**: $\log_2(12) - \log_2(3)$ 을 로그의 성질을 이용하여 간단히 하면 어떤 값이 나올까요?

2.  **손실 함수 개념**: 어떤 분류 모델이 정답 클래스에 대해 0.00001 이라는 매우 낮은 확률을 예측했습니다. 교차 엔트로피 손실 함수는 이 예측에 대해 높은 페널티를 부여할까요, 낮은 페널티를 부여할까요? 그 이유는 무엇인가요?

3.  **정의역**: 함수 $y = \log_{10}(x-5)$ 가 정의되려면, $x$는 어떤 조건을 만족해야 할까요?

---
*로그 함수는 AI에서 단순히 계산을 편하게 하는 것을 넘어, 모델의 학습을 유도하고, 정보의 가치를 측정하며, 데이터를 다루기 쉽게 만드는 만능 도구입니다.*
