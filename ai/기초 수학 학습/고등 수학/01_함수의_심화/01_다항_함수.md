# 다항 함수 (Polynomial Functions): 데이터의 복잡한 패턴을 그리다

## 1. 다항 함수의 정의

**다항 함수(Polynomial Function)**는 변수 $x$의 거듭제곱과 계수(coefficients)들의 곱으로 이루어진 항들의 합으로 구성된 함수입니다. 가장 일반적인 형태는 다음과 같습니다.

$$f(x) = a_n x^n + a_{n-1}x^{n-1} + \dots + a_1 x + a_0$$

-   **계수 (Coefficients)**: $a_n, a_{n-1}, \dots, a_0$는 함수의 모양을 결정하는 상수입니다.
-   **차수 (Degree)**: 가장 높은 거듭제곱의 지수 $n$을 다항 함수의 차수라고 합니다. 차수는 함수 그래프의 복잡성(얼마나 많이 구부러지는지)을 결정합니다.
-   **최고차항 (Leading Term)**: 가장 높은 차수를 가진 항($a_n x^n$)을 의미합니다. 이 항은 $x$가 매우 크거나 매우 작아질 때 함수의 전체적인 모양을 결정합니다.

## 2. 차수에 따른 다항 함수의 종류와 그래프

-   **0차 함수 (상수 함수)**: $f(x) = a_0$
    -   그래프는 $y$축에 $a_0$에서 만나는 수평선입니다. 변화가 없는 상태를 나타냅니다.
-   **1차 함수 (선형 함수)**: $f(x) = a_1 x + a_0$
    -   그래프는 직선입니다. 데이터의 선형적인 관계를 모델링합니다.
-   **2차 함수 (이차 함수)**: $f(x) = a_2 x^2 + a_1 x + a_0$
    -   그래프는 포물선(parabola) 모양입니다. 위로 볼록하거나 아래로 볼록한, 하나의 굴곡을 가진 패턴을 표현할 수 있습니다.
-   **3차 함수 (삼차 함수)**: $f(x) = a_3 x^3 + \dots$
    -   그래프는 최대 두 개의 굴곡을 가질 수 있으며, S자 형태를 띱니다.

차수가 높아질수록 함수는 더 많은 굴곡을 가질 수 있어, 점점 더 복잡하고 구불구불한 데이터 패턴을 표현할 수 있게 됩니다.

## 3. 다항 함수와 AI: 다항 회귀 (Polynomial Regression)

머신러닝에서 **회귀(Regression)** 는 연속적인 값을 예측하는 문제입니다. (예: 주택 가격 예측)

-   **선형 회귀 (Linear Regression)**: 데이터가 직선적인 관계를 가질 것이라고 가정하고, 데이터를 가장 잘 나타내는 하나의 직선($y = ax + b$)을 찾습니다. 하지만 현실의 데이터는 직선으로만 설명하기 어려운 경우가 많습니다.

-   **다항 회귀 (Polynomial Regression)**: 선형 회귀의 한계를 극복하기 위해 다항 함수를 사용합니다. 즉, 입력 데이터($x$)의 거듭제곱($x^2, x^3, \dots$)을 새로운 특성(feature)으로 추가하여 모델을 훈련시킵니다.

**예시:**
어떤 데이터가 포물선 형태를 보인다고 가정해봅시다.
1.  **선형 회귀**: 이 데이터에 선형 회귀를 적용하면, 데이터의 패턴을 제대로 따라가지 못하고 오차가 큰 직선이 만들어집니다.
2.  **다항 회귀**: 입력 특성 $x$에 더해 $x^2$을 새로운 특성으로 추가합니다. 이제 모델은 $y = w_2 x^2 + w_1 x + b$ 형태의 함수를 학습하게 됩니다. 이 이차 함수는 포물선 형태의 데이터를 훨씬 더 정확하게 예측할 수 있습니다.

**장점**: 다항 회귀를 사용하면 선형 모델의 단순함을 유지하면서도, 데이터의 복잡한 비선형 패턴을 학습할 수 있습니다.

**주의점**: 차수를 너무 높이면(예: 10차, 20차), 모델이 학습 데이터의 아주 사소한 노이즈까지 모두 외워버리는 **과적합(Overfitting)** 이 발생할 수 있습니다. 과적합된 모델은 학습 데이터는 완벽하게 예측하지만, 새로운 데이터에 대해서는 성능이 매우 떨어지게 됩니다. 따라서 적절한 차수를 선택하는 것이 중요합니다.

---
*다항 함수는 AI가 선형적인 관계를 넘어, 현실 세계의 더 복잡하고 구불구불한 패턴을 학습할 수 있게 해주는 유연하고 강력한 도구입니다.*