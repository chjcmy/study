# 로그 (Logarithms): 스케일의 법칙

로그는 매우 큰 숫자 범위를 다루기 쉬운 작은 범위로 압축하고, 복잡한 곱셈을 덧셈으로 변환하는 마법 같은 도구입니다.

### 핵심 로그 성질

-   $\log(xy) = \log(x) + \log(y)$ (곱셈 -> 덧셈)
-   $\log(x/y) = \log(x) - \log(y)$ (나눗셈 -> 뺄셈)
-   $\log(x^p) = p \log(x)$ (지수 -> 곱셈)

### AI와의 연결점

-   **계산의 안정성 및 효율성 (Log-Likelihood)**: 모델이 예측한 여러 사건의 전체 확률은 각 사건의 확률을 모두 곱한 값입니다. 수많은 0과 1 사이의 확률을 곱하면 컴퓨터가 처리하기 어려운 매우 작은 수가 되어 계산 오류(numerical underflow)가 발생할 수 있습니다. 여기에 로그를 취하면 **곱셈이 덧셈으로 바뀌어($\sum \log(P_i)$)** 계산이 매우 안정적이고 간단해집니다. 이는 최대 우도 추정(MLE)의 핵심입니다.

-   **손실 함수 (Loss Functions)**: 모델이 얼마나 틀렸는지를 측정하는 손실 함수에 로그가 핵심적으로 사용됩니다.
    -   **교차 엔트로피 (Cross-Entropy)**: $$Loss = - \sum y_{\text{true}} \log(y_{\text{pred}})$$. 분류 문제의 표준 손실 함수입니다. 모델이 정답($y_{\text{true}}=1$)에 대해 0에 가까운 확률($y_{\text{pred}} \to 0$)을 예측하면, $\log(y_{\text{pred}})$는 음의 무한대로 발산합니다. 이는 모델에게 **자신 있게 틀리는 것에 대해 엄청난 페널티**를 부여하여, 모델이 더 정확한 예측을 하도록 강하게 유도합니다.

-   **정보 이론 (Information Theory)**: 정보의 양을 나타내는 엔트로피(Entropy)는 로그를 사용하여 정의됩니다. 로그는 놀라움의 정도, 즉 정보의 가치를 측정하는 척도입니다.
