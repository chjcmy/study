# 03 로그 (Logarithms): AI의 계산, 손실, 그리고 정보

로그는 매우 큰 숫자 범위를 다루기 쉬운 작은 범위로 압축하고, 복잡한 곱셈을 덧셈으로 변환하는 마법 같은 도구입니다. AI에서는 이 성질을 이용하여 계산을 안정시키고, 모델의 오차를 효과적으로 측정하며, 정보의 양을 정량화합니다.

## 1. 로그의 핵심 성질 (복습)

-   **곱셈을 덧셈으로**: $\log(xy) = \log(x) + \log(y)$
-   **나눗셈을 뺄셈으로**: $\log(x/y) = \log(x) - \log(y)$
-   **거듭제곱을 곱셈으로**: $\log(x^p) = p \log(x)$

이 성질들은 AI 모델의 복잡한 확률 계산과 최적화 과정에서 결정적인 역할을 합니다.

## 2. AI와의 연결점

### 1. 계산 안정성: 로그 우도 (Log-Likelihood)

머신러닝, 특히 생성 모델이나 분류 모델은 주어진 데이터의 **우도(Likelihood)**를 최대화하는 방향으로 학습하는 경우가 많습니다. 우도는 모든 데이터 샘플에 대한 확률의 곱으로 계산됩니다.

$$ L = P(\text{data}_1) \times P(\text{data}_2) \times \dots \times P(\text{data}_n) $$

여기서 각 확률 $P(\text{data}_i)$는 0과 1 사이의 작은 값입니다. 수천, 수만 개의 작은 수를 곱하면 결과는 컴퓨터가 표현할 수 있는 가장 작은 수보다도 작아져 0으로 처리되는 **수치 언더플로우(numerical underflow)**가 발생합니다.

**해결책**: 우도에 로그를 취하는 **로그 우도(Log-Likelihood)**를 사용합니다.
$$ \log(L) = \log(P(\text{data}_1)) + \log(P(\text{data}_2)) + \dots + \log(P(\text{data}_n)) = \sum_{i=1}^{n} \log P(\text{data}_i) $$
로그의 성질 덕분에 거대한 곱셈이 간단한 덧셈으로 바뀌어, 언더플로우 문제 없이 안정적으로 계산할 수 있습니다. 또한, 미분하기도 훨씬 쉬워져 경사 하강법 적용에 유리합니다.

### 2. 손실 함수: 교차 엔트로피 (Cross-Entropy)

분류 문제의 표준 손실 함수인 **교차 엔트로피(Cross-Entropy)**는 로그를 사용하여 모델의 예측이 얼마나 '나쁜지'를 측정합니다. 이진 분류(0 또는 1)의 경우, 하나의 데이터에 대한 손실은 다음과 같습니다.

$$ \text{Loss} = - [ y_{\text{true}}\log(y_{\text{pred}}) + (1-y_{\text{true}})\log(1-y_{\text{pred}}) ] $$

-   **정답이 1일 때 ($y_{\text{true}}=1$):**
    -   Loss = $-\log(y_{\text{pred}})$
    -   모델이 정답에 가깝게 예측하면 ($y_{\text{pred}} \to 1$), $\log(y_{\text{pred}}) \to 0$ 이므로, 손실은 0에 가까워집니다.
    -   모델이 완전히 틀리게 예측하면 ($y_{\text{pred}} \to 0$), $\log(y_{\text{pred}}) \to -\infty$ 이므로, 손실은 무한대로 치솟습니다.

-   **정답이 0일 때 ($y_{\text{true}}=0$):**
    -   Loss = $-\log(1-y_{\text{pred}})$
    -   모델이 정답에 가깝게 예측하면 ($y_{\text{pred}} \to 0$), $1-y_{\text{pred}} \to 1$ 이므로, $\log(1-y_{\text{pred}}) \to 0$ 이 되어 손실이 0에 가까워집니다.
    -   모델이 완전히 틀리게 예측하면 ($y_{\text{pred}} \to 1$), $1-y_{\text{pred}} \to 0$ 이므로, 손실은 무한대로 치솟습니다.

이처럼 로그 함수는 **자신 있게 틀리는 예측에 대해 기하급수적인 페널티**를 부여하여, 모델이 정답을 맞히도록 강력하게 유도하는 이상적인 손실 함수를 만듭니다.

### 3. 정보 이론 (Information Theory)

-   **정보량**: 어떤 사건의 정보량(놀라움의 정도)은 그 사건이 발생할 확률에 반비례합니다. 로그는 이 관계를 수학적으로 표현합니다.
    $$ I(x) = -\log_2 P(x) $$
-   **엔트로피**: 어떤 확률 분포의 불확실성을 측정하는 엔트로피는 각 사건의 정보량의 기댓값(평균)으로, 로그를 사용하여 계산됩니다.

---

## 3. 확인 문제 (Practice Problems)

1.  **로그 성질**: $\log(a) + \log(b) + \log(c)$ 를 하나의 로그 항으로 표현해보세요.

2.  **로그 우도**: 3개의 독립적인 사건이 일어날 확률이 각각 $0.1, 0.2, 0.5$ 라고 할 때, 전체 사건의 로그 우도($\log(P)$)는 어떻게 계산될까요? (힌트: $\log(abc) = \log a + \log b + \log c$)

3.  **교차 엔트로피**: 어떤 이진 분류 모델이 정답이 '1'인 데이터에 대해 '1'일 확률을 0.99라고 예측했습니다. 이 예측에 대한 교차 엔트로피 손실은 클까요, 작을까요? 만약 0.01이라고 예측했다면 손실은 어떻게 변할까요?

---
*로그는 AI에서 단순한 계산 도구를 넘어, 모델의 학습을 가능하게 하고, 모델의 오차를 정의하며, 정보의 본질을 설명하는 핵심적인 언어입니다.*