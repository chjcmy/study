# 01 지수와 로그: 동전의 양면 같은 관계

## 1. 지수와 로그의 정의

지수(Exponent)와 로그(Logarithm)는 하나의 수학적 관계를 다른 관점에서 바라보는 것과 같습니다. 이 둘은 서로 뗄 수 없는 **역함수(Inverse Function)** 관계입니다.

### 지수 (Exponent)

-   **정의**: $y = b^x$
-   **의미**: 밑($b$)을 지수($x$)만큼 거듭 곱하면 결과($y$)가 나온다.
-   **관심사**: 거듭제곱의 **결과**

### 로그 (Logarithm)

-   **정의**: $x = \log_b(y)$
-   **의미**: 밑($b$)을 몇 번 거듭제곱해야 진수($y$)가 될까? 그 횟수가 바로 로그($x$)이다.
-   **관심사**: 거듭제곱의 **횟수(지수)**

> **핵심 관계**: $y = b^x \iff x = \log_b(y)$

**예시:**
-   $2^3 = 8$ 이므로, $\log_2(8) = 3$ 입니다.
-   $10^2 = 100$ 이므로, $\log_{10}(100) = 2$ 입니다.

## 2. 역함수 관계의 시각적 이해

함수와 그 역함수는 $y=x$라는 직선에 대해 대칭인 그래프를 가집니다. 지수 함수 $y=e^x$와 그 역함수인 자연로그 함수 $y=\ln(x)$의 그래프를 보면 이 관계를 명확히 알 수 있습니다.

-   **$y=e^x$**: $x$가 증가함에 따라 $y$가 폭발적으로 증가합니다. $(0, 1)$을 항상 지납니다.
-   **$y=\ln(x)$**: $x$가 증가함에 따라 $y$가 완만하게 증가합니다. $(1, 0)$을 항상 지납니다.

두 그래프는 $y=x$ 직선을 기준으로 정확히 접히며, 이는 한 함수의 입력과 출력을 뒤집으면 다른 함수가 된다는 역함수의 특징을 보여줍니다.

## 3. 왜 이 관계가 AI에서 중요한가?

AI, 특히 딥러닝 모델의 출력은 종종 확률을 나타내기 위해 **지수 함수** 기반의 소프트맥스(Softmax)나 시그모이드(Sigmoid)를 통과합니다. 하지만 이 확률들을 직접 곱하여 계산하면 수치적으로 불안정해지기 쉽습니다.

이때 **로그 함수**를 사용하여 이 문제를 해결합니다.

1.  **지수 함수로 확률 계산**: 모델의 출력을 지수 함수(소프트맥스 등)를 사용하여 $P(\text{데이터})$를 계산합니다.
2.  **로그 함수로 변환**: 계산된 확률 값에 로그를 취하여 **로그 우도(Log-Likelihood)**, $\log P(\text{데이터})$를 만듭니다.
3.  **최적화**: 이 로그 우도를 최대화(또는 음수를 취한 손실 함수를 최소화)하는 방향으로 모델을 학습시킵니다.

이처럼 지수 함수로 표현된 모델의 출력을, 그 역함수인 로그 함수를 통해 안정적이고 효율적인 최적화의 대상으로 변환하는 과정은 딥러닝의 핵심적인 트릭 중 하나입니다.

---

## 4. 확인 문제 (Practice Problems)

1.  **변환 연습**: 다음 지수 표현을 로그 표현으로, 로그 표현을 지수 표현으로 바꿔보세요.
    -   (a) $5^3 = 125$
    -   (b) $\log_4(16) = 2$

2.  **값 찾기**: 다음 식의 값을 구해보세요.
    -   (a) $\log_3(81)$
    -   (b) $2^x = 64$ 일 때, $x$의 값

3.  **역함수 관계**: 함수 $f(x) = 10^x$ 가 있습니다. 이 함수의 역함수 $f^{-1}(x)$는 무엇일까요? $f^{-1}(1000)$의 값은 얼마인가요?

---
*지수와 로그의 역함수 관계를 이해하는 것은, AI 모델이 어떻게 확률적인 출력을 내놓고, 또 어떻게 그 출력을 기반으로 안정적으로 학습하는지를 이해하는 첫걸음입니다.*