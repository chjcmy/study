# 02 급수 (Series): 수열의 합과 그 극한

## 1. 급수의 정의: 수열의 항들을 더하다

**급수(Series)**는 수열의 항들을 더하는 것입니다. 수열이 나열된 숫자들의 목록이라면, 급수는 그 숫자들을 모두 합한 결과입니다.

-   **표기**: 합의 기호인 **시그마($\Sigma$)** 를 사용하여 표현합니다.
    -   $n$번째 항까지의 합: $S_n = \sum_{k=1}^{n} a_k = a_1 + a_2 + \dots + a_n$
    -   무한히 많은 항의 합: $S = \sum_{n=1}^{\infty} a_n = a_1 + a_2 + a_3 + \dots$

## 2. 유한 급수: 등차급수와 등비급수의 합

### 등차급수 (Arithmetic Series)의 합

등차수열의 첫째항부터 $n$번째 항까지의 합 $S_n$은 다음과 같습니다.
$$ S_n = \frac{n(a_1 + a_n)}{2} = \frac{n\{2a_1 + (n-1)d\}}{2} $$
-   $a_1$: 첫째항
-   $a_n$: $n$번째 항
-   $d$: 공차

### 등비급수 (Geometric Series)의 합

등비수열의 첫째항부터 $n$번째 항까지의 합 $S_n$은 다음과 같습니다.
$$ S_n = \frac{a_1(r^n - 1)}{r - 1} \quad (\text{단, } r \neq 1) $$
-   $a_1$: 첫째항
-   $r$: 공비

## 3. 무한 급수: 수렴과 발산

무한히 많은 항을 더하는 **무한 급수**는 항상 합이 존재할까요? 그렇지 않습니다. 무한 급수는 특정 값에 가까워질 수도 있고(수렴), 그렇지 않을 수도 있습니다(발산).

-   **수렴 (Convergence)**: 무한 급수의 합이 어떤 유한한 값 $S$에 한없이 가까워질 때, 이 급수는 $S$에 수렴한다고 합니다.
-   **발산 (Divergence)**: 무한 급수의 합이 특정 값에 가까워지지 않고 계속 커지거나(양의 무한대), 계속 작아지거나(음의 무한대), 또는 진동하는 경우, 이 급수는 발산한다고 합니다.

### 무한 등비 급수의 수렴 조건과 합

무한 등비 급수 $\sum_{n=1}^{\infty} a_1 r^{n-1}$는 공비 $r$의 값에 따라 수렴 여부가 결정됩니다.

-   **수렴 조건**: 공비의 절댓값이 1보다 작을 때 ($|r| < 1$)
-   **수렴하는 합**: 공비의 절댓값이 1보다 작을 때, 무한 등비 급수의 합 $S$는 다음과 같습니다.
    $$ S = \frac{a_1}{1 - r} $$
-   **발산 조건**: 공비의 절댓값이 1보다 크거나 같을 때 ($|r| \geq 1$)

## 4. AI와의 연결점: 안정적인 학습과 모델 분석

### 1. 최적화 알고리즘의 수렴 분석

머신러닝 모델을 학습시키는 최적화 알고리즘(예: 경사 하강법)은 모델 파라미터를 반복적으로 업데이트합니다. 이 업데이트 과정은 일종의 수열을 형성하며, 이 수열이 최적의 값으로 수렴하는지 여부는 급수 이론으로 분석할 수 있습니다.

-   **학습률 (Learning Rate)**: 파라미터 업데이트의 보폭을 결정하는 학습률은 급수의 공비 $r$과 유사한 역할을 합니다. 학습률이 너무 크면 파라미터가 최적값을 지나쳐 발산할 수 있고, 너무 작으면 수렴 속도가 느려집니다.
-   **모멘텀 (Momentum)**: 이전 업데이트 방향을 일정 비율로 반영하는 모멘텀 기법은 등비수열의 개념과 유사하게 작동하여 수렴 속도를 가속화하고 지역 최솟값(local minima)에 갇히는 것을 방지합니다.

### 2. 순환 신경망 (RNN)의 기울기 소실/폭주

이전 장에서 언급했듯이, RNN에서 기울기 소실(Vanishing Gradients) 또는 폭주(Exploding Gradients) 문제는 역전파 과정에서 가중치 행렬이 반복적으로 곱해지면서 발생합니다. 이는 무한 등비 급수의 수렴/발산 조건과 직접적으로 연결됩니다.

-   가중치 행렬의 특성값(eigenvalue)의 절댓값이 1보다 작으면 기울기가 소실되어 과거 정보가 제대로 전달되지 않고,
-   1보다 크면 기울기가 폭주하여 학습이 불안정해집니다.

이러한 문제를 해결하기 위해 LSTM, GRU와 같은 복잡한 RNN 구조가 개발되었으며, 이는 기울기 흐름을 제어하여 학습을 안정화하는 데 기여합니다.

--- 

## 5. 확인 문제 (Practice Problems)

1.  **유한 등차급수**: 첫째항이 5이고 공차가 2인 등차수열의 첫째항부터 10번째 항까지의 합($S_{10}$)을 구해보세요.

2.  **유한 등비급수**: 첫째항이 1이고 공비가 2인 등비수열의 첫째항부터 5번째 항까지의 합($S_5$)을 구해보세요.

3.  **무한 등비급수**: 첫째항이 1이고 공비가 $1/2$인 무한 등비급수의 합은 얼마인가요? 공비가 2인 무한 등비급수의 합은 얼마인가요?

4.  **AI 연결**: RNN에서 기울기 소실 또는 폭주 문제가 발생하는 수학적 원리를 무한 등비 급수의 수렴/발산 조건과 연결하여 설명해보세요.

---
*급수는 수열의 동적인 측면을 다루며, AI 모델의 학습 과정이 안정적으로 최적값에 도달하는지, 그리고 순차 데이터가 어떻게 처리되는지를 이해하는 데 중요한 수학적 통찰을 제공합니다.*
