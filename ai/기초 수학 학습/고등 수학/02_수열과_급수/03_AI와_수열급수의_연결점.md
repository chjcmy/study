# 03 AI와 수열/급수의 연결점: 순차와 반복의 수학

수열과 급수는 단순히 수학 문제를 넘어, 순서와 반복이 있는 모든 현상을 분석하는 강력한 프레임워크입니다. AI, 특히 순차적인 데이터를 다루는 딥러닝 모델에서 이 개념은 핵심적인 동작 원리를 설명합니다.

## 1. 순차 데이터 (Sequential Data) 모델링

AI가 다루는 많은 데이터는 본질적으로 **수열**입니다.

-   **시계열 데이터 (Time-Series Data)**: 매일 기록되는 주식 종가, 매시간 측정되는 웹사이트 트래픽, 오디오 신호의 파형 등은 모두 시간 순서에 따라 나열된 숫자들의 수열입니다.
-   **자연어 (Natural Language)**: "고양이가 강아지를 쫓는다"라는 문장은 `[고양이, 가, 강아지, 를, 쫓는다]` 라는 단어 토큰의 수열입니다. 이 순서가 바뀌면 의미가 완전히 달라지므로, 순서 정보는 매우 중요합니다.

AI 모델은 이러한 수열 데이터의 규칙성을 학습하여 다음 값을 예측하거나, 전체 시퀀스의 의미를 파악합니다.

## 2. 순환 신경망 (Recurrent Neural Networks, RNN)

RNN은 순차 데이터를 처리하기 위해 명시적으로 설계된 신경망입니다. 그 구조 자체가 수열의 **점화식(Recurrence Relation)** 과 동일합니다.

-   **RNN의 상태 업데이트**: RNN의 특정 시점 $t$에서의 은닉 상태(hidden state) $h_t$는 바로 이전 시점의 은닉 상태 $h_{t-1}$와 현재 시점의 입력 $x_t$를 받아 계산됩니다.
    $$ h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h) $$
    여기서 $W_{hh}, W_{xh}, b_h$는 학습 가능한 파라미터(가중치와 편향)입니다. 이 식은 $h_t$가 $h_{t-1}$에 의해 결정되는 점화식 구조를 명확히 보여줍니다.

-   **기울기 소실/폭주 (Vanishing/Exploding Gradients)**: RNN의 학습(역전파) 과정에서, 손실에 대한 과거 시점의 기울기를 계산하려면 연쇄 법칙에 따라 가중치 행렬 $W_{hh}$가 반복적으로 곱해집니다.
    $$ \frac{\partial h_t}{\partial h_k} = \frac{\partial h_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \dots \frac{\partial h_{k+1}}{\partial h_k} \propto (W_{hh})^{t-k} $$
    이는 **등비수열**과 정확히 같은 원리로 작동합니다.
    -   만약 $W_{hh}$가 1보다 작은 역할을 하면, 곱셈이 반복될수록 기울기는 0으로 사라집니다 (**기울기 소실**). 이는 모델이 먼 과거의 정보를 학습하지 못하게 만듭니다.
    -   만약 $W_{hh}$가 1보다 큰 역할을 하면, 곱셈이 반복될수록 기울기는 무한대로 폭주합니다 (**기울기 폭주**). 이는 학습을 매우 불안정하게 만듭니다.
    이처럼 수열의 수렴/발산 개념은 RNN의 근본적인 한계를 설명하고, 이를 해결하기 위한 LSTM, GRU 같은 새로운 모델 구조의 필요성을 제시합니다.

## 3. 최적화 알고리즘 (Optimization Algorithms)

경사 하강법과 같은 최적화 알고리즘은 파라미터 $\theta$를 반복적으로 업데이트하여 손실 함수를 최소화합니다.

$$ \theta_{n+1} = \theta_n - \eta \nabla L(\theta_n) $$

-   **파라미터의 수열**: 각 스텝의 파라미터 $\theta_0, \theta_1, \theta_2, \dots$ 는 하나의 수열을 형성합니다.
-   **수렴**: 최적화의 목표는 이 수열이 손실 함수의 최솟값에 해당하는 파라미터 값으로 **수렴**하도록 만드는 것입니다.
-   **학습률 스케줄링**: 학습 과정에서 학습률($\eta$)을 점차 줄여나가는 기법(Learning Rate Scheduling)은, 수열의 항이 진행될수록 변화의 폭이 줄어드는 것과 같습니다. 이는 최적값 근처에서 더 세밀한 탐색을 가능하게 하여 안정적인 수렴을 돕습니다.

## 4. 테일러 급수 (Taylor Series)

테일러 급수는 복잡한 함수를 다항 함수의 **무한 급수**로 근사하는 방법입니다. 이는 뉴턴 방법(Newton's Method)과 같은 고급 최적화 알고리즘의 이론적 기반을 제공하며, 손실 함수의 국소적인(local) 모양을 분석하는 데 사용됩니다.

## 5. 확인 문제 (Practice Problems)

1.  **순차 데이터**: 우리 주변에서 볼 수 있는 시계열 데이터의 예시를 2개 더 들어보세요.

2.  **RNN 개념**: RNN의 '기울기 소실' 문제를 등비수열의 공비($r$)와 연결하여 설명해보세요. 기울기 소실이 발생할 때, 공비 $r$은 어떤 조건을 만족할까요?

3.  **최적화**: 경사 하강법에서 학습률을 너무 크게 설정하면, 파라미터 수열($\theta_n$)이 수렴하지 않고 발산할 수 있습니다. 그 이유를 간단히 설명해보세요.

---
*수열과 급수는 AI, 특히 순차적 데이터를 다루는 모델과 학습 알고리즘의 동적인 과정을 이해하고 분석하는 데 필수적인 수학적 언어입니다.*
