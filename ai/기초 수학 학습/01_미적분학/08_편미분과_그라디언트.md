# 08 편미분과 그라디언트: 다변수 함수의 변화를 다루는 기술

## 1. 왜 편미분이 필요한가요? (From One to Many)

지금까지 우리는 변수가 하나인 함수 $y = f(x)$의 변화율을 다루는 [[05_미분과_도함수|미분]]을 배웠습니다. 하지만 AI 모델을 포함한 현실 세계의 문제들은 대부분 여러 개의 변수에 의해 결과가 결정됩니다. 예를 들어, 집값은 방의 개수, 집의 크기, 동네의 위치 등 수많은 요인(변수)에 따라 달라집니다.

이처럼 **여러 개의 독립 변수를 가지는 함수를 '[[02_함수와_다변수_함수|다변수 함수]]'** 라고 합니다. 예를 들어 $f(x, y) = x^2 + 3y$ 와 같은 함수입니다.

그렇다면 이런 다변수 함수에서는 '변화율'을 어떻게 측정할 수 있을까요? $x$가 변할 때의 변화율과 $y$가 변할 때의 변화율은 다를 것입니다. 이때 사용하는 것이 바로 **'편미분(Partial Differentiation)'** 입니다.

## 2. 편미분: 하나에만 집중하기

편미분의 핵심 아이디어는 아주 간단합니다.

> **"여러 변수 중, 내가 관심 있는 단 하나의 변수만 변수로 보고, 나머지 변수들은 모두 상수(숫자)처럼 취급하여 미분한다."**

- **$x$에 대한 편미분**: $y$를 숫자로 생각하고, $x$에 대해서만 미분합니다.
- **$y$에 대한 편미분**: $x$를 숫자로 생각하고, $y$에 대해서만 미분합니다.

편미분은 일반 미분($d$)과 구분하기 위해 $\partial$ (델, del 또는 라운드 디) 기호를 사용합니다.

- $f(x, y)$를 $x$에 대해 편미분한 결과: $\frac{\partial f}{\partial x}$
- $f(x, y)$를 $y$에 대해 편미분한 결과: $\frac{\partial f}{\partial y}$

## 3. 예제로 익히는 편미분

**예제: $f(x, y) = x^2 y^3 + 2x + 5y$**

1.  **$x$에 대한 편미분 ($\frac{\partial f}{\partial x}$)**: $y$를 상수 취급합니다.
    - $x^2 y^3$ 에서 $y^3$는 상수입니다. $(\text{상수}) \cdot x^2$ 미분은 $(\text{상수}) \cdot 2x$ 이므로 $2xy^3$
    - $2x$는 $x$에 대해 미분하면 $2$
    - $5y$는 $x$ 입장에서 보면 그냥 상수이므로 미분하면 $0$
    - **결과: $\frac{\partial f}{\partial x} = 2xy^3 + 2$**

2.  **$y$에 대한 편미분 ($\frac{\partial f}{\partial y}$)**: $x$를 상수 취급합니다.
    - $x^2 y^3$ 에서 $x^2$는 상수입니다. $(\text{상수}) \cdot y^3$ 미분은 $(\text{상수}) \cdot 3y^2$ 이므로 $3x^2y^2$
    - $2x$는 $y$ 입장에서 보면 상수이므로 미분하면 $0$
    - $5y$는 $y$에 대해 미분하면 $5$
    - **결과: $\frac{\partial f}{\partial y} = 3x^2y^2 + 5$**

## 4. 그라디언트: 모든 방향의 변화를 담은 벡터

편미분을 통해 각 변수 방향으로의 변화율을 따로따로 구할 수 있었습니다. **'그라디언트(Gradient)'** 는 이 모든 편미분 결과를 하나의 [[09_벡터와_행렬|벡터(vector)]]로 묶은 것입니다. 그라디언트는 $\nabla f$ (나블라 f, nabla f 라고 읽습니다) 로 표기합니다.

함수 $f(x, y)$의 그라디언트는 다음과 같습니다.

> $$\nabla f = \left[ \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right]$$

만약 함수가 $f(x, y, z)$ 라면 그라디언트는 $$\nabla f = \left[ \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z} \right]$$ 가 됩니다.

## 5. 그라디언트의 마법: 가장 가파른 길을 알려주다

그라디언트는 단순히 편미분을 모아놓은 것 이상의 중요한 의미를 가집니다.

> **"어떤 지점에서 그라디언트 벡터가 가리키는 방향은 함수 값이 가장 가파르게 증가하는 방향이며, 그 벡터의 크기는 그 방향으로의 변화율(기울기)을 나타낸다."**

산을 등산한다고 상상해봅시다. 현재 위치에서 어느 방향으로 발을 내디뎌야 가장 빠르게 고도를 높일 수 있을까요? 그 방향을 알려주는 것이 바로 그라디언트입니다.

## 6. 그라디언트와 AI: 경사 하강법의 핵심

AI 모델 학습의 목표는 [[16_정보_이론_기초#^교차 엔트로피|손실 함수(Loss Function)]]의 값을 최소로 만드는 것입니다. 즉, 손실 함수라는 거대한 산의 가장 낮은 지점을 찾아가는 것과 같습니다.

이때 사용하는 방법이 **'경사 하강법(Gradient Descent)'** 입니다.

1.  현재 위치(현재 모델의 파라미터 값)에서 손실 함수의 그라디언트를 계산합니다.
2.  그라디언트는 함수 값이 가장 가파르게 '증가'하는 방향을 가리킵니다.
3.  우리는 함수 값을 '감소'시켜야 하므로, **그라디언트의 반대 방향**으로 조금씩 이동합니다.
4.  이 과정을 계속 반복하면 결국 손실 함수의 최솟값(또는 근처)에 도달하게 됩니다.

이처럼 그라디언트는 AI 모델이 어느 방향으로 학습해야 할지 알려주는 핵심적인 나침반 역할을 합니다.

## 확인 문제

1.  함수 $f(x, y) = 3x^2 + 2xy - y^4$ 에 대해 $\frac{\partial f}{\partial x}$ 와 $\frac{\partial f}{\partial y}$ 를 각각 구해보세요.
2.  위 함수 $f(x, y)$의 그라디언트 $\nabla f$ 를 구해보세요.
3.  점 $(1, 1)$ 에서의 그라디언트 $\nabla f(1, 1)$ 는 무엇일까요?

---

*편미분과 그라디언트는 다변수 함수를 다루는 기본이자, 딥러닝의 핵심 원리인 경사 하강법을 이해하기 위한 필수 관문입니다. 각 변수 방향의 변화율을 모아놓은 벡터가 왜 가장 가파른 방향을 나타내는지 직관적으로 이해하는 것이 중요합니다.*