# 16 정보 이론 기초: 정보의 양과 분포의 차이를 측정하다

## 1. 정보 이론이란 무엇일까요?

우리는 일상생활에서 '정보'라는 말을 자주 사용합니다. 하지만 정보의 양을 어떻게 측정할 수 있을까요? 어떤 정보가 더 가치 있고, 어떤 정보가 더 놀라운 것일까요? **정보 이론(Information Theory)** 은 이러한 질문에 답하기 위해 정보, 불확실성, 무작위성을 수학적으로 정량화하는 학문입니다.

AI, 특히 머신러닝에서는 정보 이론의 개념들이 매우 중요하게 사용됩니다.

-   **불확실성 측정**: 모델의 예측이 얼마나 불확실한지, 데이터셋이 얼마나 혼란스러운지 등을 측정합니다.
-   **분포 비교**: 모델의 예측 분포가 실제 데이터의 분포와 얼마나 다른지 측정하여 모델의 성능을 평가합니다.
-   **변수 간 관계**: 두 변수가 서로 얼마나 많은 정보를 공유하는지 측정하여 특성 선택(Feature Selection) 등에 활용합니다.

## 2. 정보량 (Information Content)

어떤 사건 $x$가 발생할 확률이 $P(x)$일 때, 이 사건이 가지고 있는 정보량 $I(x)$는 다음과 같이 정의됩니다.

> $$I(x) = -log2(P(x))$$ 

-   **단위**: $\log_2$를 사용하면 정보량의 단위는 **비트(bits)** 가 됩니다.
-   **의미**: **발생 확률이 낮은 사건일수록 더 많은 정보량(더 큰 놀라움)을 가집니다.** 예를 들어, "내일 해가 뜬다"는 정보는 확률이 높으므로 정보량이 적습니다. 하지만 "내일 서울에 눈이 온다"는 정보는 확률이 낮으므로 정보량이 많습니다.

## 3. 엔트로피 (Entropy): 평균 정보량 또는 불확실성

**엔트로피(Entropy)** 는 어떤 확률 변수 $X$가 가질 수 있는 모든 사건들의 **평균 정보량**을 의미합니다. 즉, 확률 변수 $X$의 결과가 얼마나 불확실한지를 나타내는 척도입니다.

> $$H(X) = - Σ P(x) * log2(P(x))$$ (모든 가능한 `x`에 대해 합산)

-   **의미**: 엔트로피 값이 높을수록 확률 변수의 결과가 더 예측하기 어렵고(불확실성이 높고), 더 많은 정보량을 가집니다.
-   **예시**: 공정한 동전(앞면/뒷면 확률이 각각 0.5)은 엔트로피가 높습니다. (어떤 결과가 나올지 예측하기 어렵기 때문) 반면, 양면이 모두 앞면인 동전은 엔트로피가 0입니다. (결과가 항상 앞면으로 예측 가능하기 때문)
-   **AI에서의 활용**: 의사결정 트리(Decision Tree) 알고리즘에서 데이터를 분할할 때, 엔트로피를 사용하여 불확실성이 가장 많이 감소하는 방향으로 분할합니다.

## 4. 교차 엔트로피 (Cross-Entropy): 두 분포의 차이 측정

**교차 엔트로피(Cross-Entropy)** 는 두 확률 분포 $P$와 $Q$가 얼마나 다른지를 측정하는 척도입니다. 여기서 $P$는 실제(정답) 분포이고, $Q$는 우리가 예측한(모델링한) 분포입니다.

> $$H(P, Q) = - Σ P(x) * log2(Q(x))$$ 

-   **의미**: 실제 분포 $P$를 따르는 사건들을, 예측 분포 $Q$에 기반한 인코딩 방식으로 표현할 때 필요한 **평균 비트 수**를 의미합니다.
-   **AI에서의 활용**: **머신러닝, 특히 분류(Classification) 문제에서 가장 널리 사용되는 손실 함수(Loss Function)입니다.** 모델의 예측 분포 $Q$가 실제 정답 분포 $P$와 얼마나 다른지를 측정하며, 이 값을 최소화하는 방향으로 모델을 학습시킵니다. (예: $P$는 원-핫 인코딩된 정답 레이블, $Q$는 모델이 출력한 각 클래스에 대한 확률)

## 5. KL 발산 (Kullback-Leibler Divergence): 정보 손실량

**KL 발산(Kullback-Leibler Divergence, KLD)** 은 두 확률 분포 $P$와 $Q$ 사이의 **상대적인 엔트로피** 또는 **정보 이득(Information Gain)** 을 측정합니다. $P$를 실제 분포로 보고, $Q$를 $P$에 대한 근사 분포로 볼 때, $Q$를 사용하여 $P$를 표현할 때 발생하는 **정보 손실량**을 의미합니다.

> $$D_{KL}(P || Q) = Σ P(x) * log2(P(x) / Q(x))$$ 

-   **관계**: $D_{KL}(P || Q) = H(P, Q) - H(P)$ 입니다. 즉, 교차 엔트로피에서 실제 분포의 엔트로피를 뺀 값입니다.
-   **특징**: 항상 $D_{KL} ≥ 0$ 이며, $P = Q$ 일 때만 $0$이 됩니다. 하지만 **대칭적이지 않습니다** ($D_{KL}(P || Q) ≠ D_{KL}(Q || P)$).
-   **AI에서의 활용**: 생성 모델(Generative Models)인 VAE(Variational Autoencoder)의 손실 함수에 사용되어, 모델이 생성한 분포가 실제 데이터 분포와 얼마나 유사한지 측정합니다. 강화 학습(Reinforcement Learning)에서도 정책(Policy) 간의 차이를 측정하는 데 사용됩니다.

## 6. 상호 정보량 (Mutual Information): 두 변수의 관계

**상호 정보량(Mutual Information, MI)** 은 두 확률 변수 $X$와 $Y$가 서로 얼마나 많은 정보를 공유하는지 측정합니다. 즉, 한 변수를 알면 다른 변수에 대해 얼마나 더 알게 되는지를 나타냅니다.

> $I(X; Y) = H(X) - H(X|Y)$ ($X$의 불확실성 중 $Y$를 알게 됨으로써 감소하는 양)

-   **의미**: $I(X; Y)$ 값이 높을수록 두 변수 간의 관계가 강합니다. 만약 두 변수가 완전히 독립이라면 상호 정보량은 $0$이 됩니다.
-   **AI에서의 활용**: 특성 선택(Feature Selection)에서 어떤 특성(Feature)이 목표 변수(Target Variable)에 대해 얼마나 많은 정보를 가지고 있는지 측정하여 중요한 특성을 선별하는 데 사용됩니다.

## 확인 문제

1.  엔트로피와 정보량의 관계를 설명해보세요.
2.  분류 문제에서 교차 엔트로피가 손실 함수로 사용되는 이유를 설명해보세요.
3.  KL 발산이 교차 엔트로피와 엔트로피의 차이로 표현되는 이유를 설명해보세요.

---

*정보 이론은 AI 모델이 데이터를 이해하고, 학습하며, 평가하는 데 필요한 깊이 있는 통찰을 제공합니다. 특히 엔트로피와 교차 엔트로피는 딥러닝에서 매일 사용되는 개념이므로 잘 이해해두는 것이 좋습니다.*