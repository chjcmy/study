# 11 벡터와 행렬 미분: 딥러닝의 엔진, 역전파의 언어

## 1. 왜 벡터와 행렬을 미분해야 할까요?

딥러닝 모델은 수백만, 수억 개의 파라미터(가중치와 편향)를 가진 매우 복잡한 함수입니다. 이 파라미터들은 보통 [[09_벡터와_행렬|벡터]]와 [[09_벡터와_행렬|행렬]] 형태로 구성되어 있습니다. 모델을 학습시킨다는 것은, 손실 함수([[16_정보_이론_기초#^교차 엔트로피|Loss Function]])라는 최종 결과값(스칼라)을 최소화하는 파라미터(벡터, 행렬)를 찾는 과정입니다.

손실 값을 줄이려면, 각 파라미터가 손실 값에 얼마나 영향을 미치는지 알아야 합니다. 즉, **"스칼라(손실 값)를 벡터(가중치) 또는 행렬(가중치)로 미분"** 하여 기울기([[08_편미분과_그라디언트|Gradient]])를 구해야 합니다. 이 기울기의 반대 방향으로 파라미터를 조금씩 업데이트하는 과정이 바로 [[08_편미분과_그라디언트#^경사 하강법|경사 하강법]]이며, 이 전체 과정을 **[[11_벡터와_행렬_미분#^벡터 연쇄 법칙과 역전파|역전파(Backpropagation)]]** 라고 부릅니다.

따라서 벡터와 행렬 미분은 딥러닝 모델을 학습시키는 엔진의 핵심 원리라고 할 수 있습니다.

## 2. 표기법: 분모 기준 레이아웃 (Denominator Layout)

행렬 미분은 표기법이 다양하여 혼란을 줄 수 있습니다. 우리는 가장 직관적인 **분모 기준 레이아웃(Denominator Layout)** 을 사용합니다. 이 표기법의 핵심 규칙은 간단합니다.

> **"미분 결과의 형태(shape)는 미분하는 대상(분모)의 형태와 같다."**

-   스칼라를 **벡터**로 미분하면 -> **벡터**가 나온다.
-   스칼라를 **행렬**로 미분하면 -> **행렬**이 나온다.

## 3. 핵심 유형별 미분

### 유형 1: 스칼라를 벡터로 미분 (Scalar by Vector)

이는 우리가 이미 배운 **그라디언트(Gradient)** 와 같습니다.

-   함수: $y = f(\vec{x})$ ($y$는 스칼라, $\vec{x}$는 $n$차원 벡터 $[x_1, x_2, \dots, x_n]$)
-   미분: $\frac{dy}{d\vec{x}}$는 $y$를 각 $x_i$로 편미분한 결과를 모아놓은 벡터입니다.

> $\frac{dy}{d\vec{x}} = \nabla y = \left[ \frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \dots, \frac{\partial y}{\partial x_n} \right]$

결과의 형태가 분모인 벡터 $\vec{x}$와 동일한 $n$차원 벡터입니다.

### 유형 2: 스칼라를 행렬로 미분 (Scalar by Matrix)

-   함수: $y = f(X)$ ($y$는 스칼라, $X$는 $m \times n$ 행렬)
-   미분: $\frac{dy}{dX}$는 $y$를 각 $X_{ij}$로 편미분한 결과를 모아놓은 행렬입니다.

> $\frac{dy}{dX}$의 $(i, j)$ 위치의 원소 = $\frac{\partial y}{\partial X_{ij}}$

결과의 형태가 분모인 행렬 $X$와 동일한 $m \times n$ 행렬입니다.

### 유형 3: 벡터를 벡터로 미분 (Vector by Vector)

-   함수: $\vec{y} = f(\vec{x})$ ($\vec{y}$는 $m$차원 벡터, $\vec{x}$는 $n$차원 벡터)
-   미분: $\frac{d\vec{y}}{d\vec{x}}$는 **야코비안 행렬(Jacobian Matrix)** 이라고 불리는 $m \times n$ 행렬이 됩니다.

> $J = \frac{d\vec{y}}{d\vec{x}}$ 의 $(i, j)$ 위치의 원소 = $\frac{\partial y_i}{\partial x_j}$

야코비안 행렬의 각 행은 출력 벡터의 각 원소($y_i$)를 입력 벡터 $\vec{x}$로 미분한 그라디언트입니다. 야코비안은 다차원 함수의 연쇄 법칙에서 핵심적인 역할을 합니다.

## 4. 벡터 연쇄 법칙과 역전파

딥러닝의 역전파는 결국 **벡터 형태의 [[06_연쇄_법칙_(Chain_Rule)|연쇄 법칙(Chain Rule)]]** 입니다.

$\vec{z} = f(\vec{y})$ 이고 $\vec{y} = g(\vec{x})$ 일 때, 스칼라 연쇄 법칙은 $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$ 였습니다. 벡터 미분에서는 이 곱셈이 **야코비안 행렬의 곱**으로 바뀝니다.

> $\\frac{\\partial \\vec{z}}{\\partial \\vec{x}} = \\frac{\\partial \\vec{z}}{\\partial \\vec{y}} \\frac{\\partial \\vec{y}}{\\partial \\vec{x}}$

여기서 $\\frac{\\partial \\vec{z}}{\\partial \\vec{y}}$와 $\\frac{\\partial \\vec{y}}{\\partial \\vec{x}}$는 각각 야코비안 행렬입니다. 이 행렬 곱을 통해 최종 출력의 변화가 입력의 변화에 어떻게 연관되는지를 계산할 수 있습니다.

**신경망에서의 예시:**

입력($\vec{x}$) -> [선형 변환: $\\vec{y} = W\\vec{x} + \\vec{b}$] -> [활성화 함수: $\\vec{z} = \\sigma(\\vec{y})$] -> 손실($L$)

우리의 목표는 $\\frac{\\partial L}{\\partial W}$ (손실을 가중치 행렬 $W$로 미분)를 구하는 것입니다. 연쇄 법칙에 따라 다음과 같이 계산됩니다.

> $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial \\vec{z}} \\frac{\\partial \\vec{z}}{\\partial \\vec{y}} \\frac{\\partial \\vec{y}}{\\partial W}$

-   $\\frac{\\partial L}{\\partial \\vec{z}}$: 손실 함수를 활성화 함수 출력으로 미분
-   $\\frac{\\partial \\vec{z}}{\\partial \\vec{y}}$: 활성화 함수를 선형 변환 출력으로 미분
-   $\\frac{\\partial \\vec{y}}{\\partial W}$: 선형 변환을 가중치 행렬로 미분

역전파 알고리즘은 출력층에서부터 이 연쇄 법칙을 재귀적으로 적용하여 각 층의 가중치에 대한 기울기를 계산하고, 경사 하강법을 통해 가중치를 업데이트하는 과정입니다.

## 확인 문제

1.  분모 기준 레이아웃(Denominator Layout)의 핵심 규칙은 무엇인가요?
2.  스칼라를 벡터로 미분한 결과는 무엇이며, 이를 무엇이라고 부르나요?
3.  벡터를 벡터로 미분한 결과는 무엇이며, 이를 무엇이라고 부르나요?
4.  딥러닝의 역전파와 벡터 연쇄 법칙은 어떤 관계인가요?

---

*벡터와 행렬 미분은 딥러닝의 학습 원리를 이해하기 위한 마지막 관문입니다. 모든 공식을 암기할 필요는 없습니다. 핵심은 '손실이라는 최종 스칼라 값을 각 파라미터 행렬/벡터로 미분하여 기울기를 구하고, 이 기울기를 통해 파라미터를 업데이트한다'는 큰 그림을 이해하는 것입니다.*