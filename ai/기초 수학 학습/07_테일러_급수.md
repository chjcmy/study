# 07 테일러 급수: 복잡한 함수를 다항식으로 근사하는 마법

## 1. 테일러 급수의 핵심 아이디어

$\sin(x)$, $\cos(x)$, $e^x$ 와 같은 함수들은 계산하기가 까다롭습니다. 컴퓨터는 $\sin(0.5)$ 같은 값을 어떻게 계산할까요? 직접 계산하는 것이 아니라, 계산하기 매우 쉬운 **다항 함수(polynomial)** 형태로 근사하여 계산합니다.

**테일러 급수(Taylor Series)** 의 핵심 아이디어는 바로 이것입니다.

> **"아무리 복잡하고 어려운 함수라도, 특정 지점 근처에서는 그 함수를 흉내 내는 다항 함수로 근사할 수 있다."

즉, 어떤 함수를 무한한 차수의 다항 함수의 합으로 표현하는 것입니다. 이 다항 함수는 특정 지점에서 원래 함수와 동일한 함수 값, 동일한 1차 미분계수(기울기), 동일한 2차 미분계수(볼록성) 등을 갖도록 만들어져, 그 지점 근처에서 원래 함수와 거의 똑같이 행동합니다.

## 2. 근사의 단계적 발전

함수 $f(x)$를 $x=a$ 라는 지점 근처에서 근사하는 과정을 단계적으로 살펴봅시다.

-   **0차 근사 (상수 근사)**: 가장 간단한 방법은 $x=a$에서의 함수 값 $f(a)$로 근사하는 것입니다. $f(x) \approx f(a)$. 이것은 그냥 수평선이므로 오차가 매우 큽니다.

-   **1차 근사 (선형 근사)**: 함수 값뿐만 아니라, $x=a$에서의 **기울기(1차 미분계수)** 까지 똑같이 만들어주면 어떨까요? 이것이 바로 접선의 방정식입니다. $f(x) \approx f(a) + f'(a)(x-a)$. 0차 근사보다 훨씬 정확한 근사가 됩니다.

-   **2차 근사 (이차 함수 근사)**: 이제 **곡률(2차 미분계수)** 까지 똑같이 만들어줍시다. $x=a$에서 원래 함수와 같은 곡률을 갖는 포물선을 만드는 것입니다. $f(x) \approx f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2$. 이제 $a$ 근처에서 거의 완벽하게 함수를 따라합니다.

-   **$n$차 근사**: 이 과정을 계속 반복하여 3차, 4차, ... $n$차 미분계수까지 모두 같게 만들어주면, 근사 다항 함수는 원래 함수와 점점 더 가까워집니다.

## 3. 테일러 급수 공식

위 과정을 무한히 반복하면, $x=a$에서 함수 $f(x)$의 테일러 급수를 얻을 수 있습니다.

> $$f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \dots$$

이를 시그마 기호로 표현하면 다음과 같습니다.

> $$f(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^n$$

여기서 $f^{(n)}(a)$는 $f(x)$를 $n$번 미분한 후 $x=a$를 대입한 값이고, $n!$은 n 팩토리얼($n \cdot (n-1) \cdot \dots \cdot 1$)입니다.

### 매클로린 급수 (Maclaurin Series)

매클로린 급수는 테일러 급수의 특별한 경우로, 근사하는 기준점 $a$를 $0$으로 설정한 것입니다. ($a=0$)

> $$f(x) = f(0) + f'(0)x + \frac{f''(0)}{2!}x^2 + \frac{f'''(0)}{3!}x^3 + \dots$$

## 4. 중요한 매클로린 급수 예시

-   $$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots$$
-   $$\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \dots$$
-   $$\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \dots$$

이 공식들을 보면, 왜 $e^x$를 미분해도 자기 자신이 되는지, $\sin(x)$를 미분하면 $\cos(x)$가 되는지를 다항 함수의 미분으로 쉽게 이해할 수 있습니다.

## 5. 왜 AI에서 테일러 급수가 중요할까요?

테일러 급수는 AI, 특히 최적화(Optimization) 이론에서 핵심적인 역할을 합니다.

1.  **최적화 알고리즘의 이해**: 머신러닝 모델을 학습시키는 것은 결국 손실 함수(Loss Function)의 최솟값을 찾는 과정입니다. 테일러 급수를 사용하면 이 복잡한 손실 함수를 특정 지점 근처에서 간단한 1차 또는 2차 함수로 근사할 수 있습니다.
    -   **경사 하강법 (Gradient Descent)**: 손실 함수를 1차 테일러 급수로 근사한 것과 같습니다. 기울기($f'$) 정보만을 사용하여 최솟값 방향으로 이동합니다.
    -   **뉴턴 방법 (Newton's Method)**: 손실 함수를 2차 테일러 급수로 근사합니다. 기울기($f'$)와 곡률($f''$) 정보를 모두 사용하여 더 빠르고 효율적으로 최솟값을 찾습니다.

2.  **이론적 분석**: 특정 알고리즘이 왜 잘 작동하는지, 얼마나 빨리 수렴하는지 등을 수학적으로 증명하고 분석하는 데 테일러 급수가 사용됩니다.

## 확인 문제

1.  테일러 급수의 핵심 아이디어는 무엇인가요?
2.  매클로린 급수는 테일러 급수와 어떤 관계인가요?
3.  $f(x) = e^x$ 의 매클로린 급수에서 처음 3개 항을 이용하여 $e^{0.1}$ 의 근사값을 구해보세요.

---

*테일러 급수는 복잡한 함수를 우리가 다루기 쉬운 다항 함수로 변환하는 강력한 다리 역할을 합니다. 특히 AI의 최적화 원리를 깊이 있게 이해하기 위한 필수적인 개념입니다.*