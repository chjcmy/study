# 14 최대 우도 추정 (MLE): 데이터가 말해주는 최선의 파라미터 찾기

## 1. MLE의 핵심 질문: 이 데이터는 어떤 동전에서 나왔을까?

어떤 동전을 10번 던졌더니 앞면이 7번, 뒷면이 3번 나왔다고 합시다. 이 동전은 앞면이 나올 확률 $p$가 얼마인 동전이라고 생각하는 것이 가장 합리적일까요? 아마 대부분의 사람들은 직관적으로 $p = 0.7$ 이라고 답할 것입니다.

**최대 우도 추정(Maximum Likelihood Estimation, MLE)** 은 바로 이 직관을 수학적으로 정립한 방법입니다. 즉, **"내가 가지고 있는 이 데이터가 어떤 확률 분포 모델로부터 나왔을 가능성(우도)이 가장 높은가?"** 라는 질문에 답하는 과정입니다. 위 예시에서 "앞면이 나올 확률 $p$가 얼마일 때, '10번 중 7번 앞면'이라는 결과가 나올 가능성이 가장 높은가?"를 찾는 것과 같습니다.

MLE는 주어진 데이터를 가장 잘 설명하는 모델의 파라미터(모수)를 찾는 강력한 통계적 도구입니다.

## 2. 확률(Probability) vs 우도(Likelihood)

이 둘은 비슷해 보이지만, 질문의 방향이 정반대입니다.

- **[[12_확률과_통계_기초#^확률의 기본 개념|확률(Probability)]]**: **파라미터(모델)가 주어졌을 때**, 특정 데이터가 관찰될 가능성.
  - 질문: "앞면 나올 확률이 0.5인 공정한 동전이 주어졌을 때, 10번 던져서 앞면이 7번 나올 확률은 얼마인가?"
  - $P(\text{데이터} | \theta)$

- **우도**: **데이터가 주어졌을 때**, 특정 파라미터(모델)가 이 데이터를 만들어냈을 가능성.
  - 질문: "10번 던져 앞면이 7번 나오는 데이터가 주어졌을 때, 이 동전의 앞면 나올 확률이 0.5일 우도는 얼마인가? (또는 0.7일 우도는 얼마인가?)"
  - $L(\theta | \text{데이터})$

MLE의 목표는 이 **우도(Likelihood)를 최대로 만드는 파라미터를 찾는 것**입니다.

## 3. 우도 함수 (Likelihood Function)

우도 함수는 파라미터 $\theta$(세타)를 변수로 가지는 함수 $L(\theta | \text{데이터})$ 입니다. 각 데이터 샘플이 독립적으로 발생했다고 가정하면, 전체 데이터에 대한 우도는 각 데이터 샘플의 확률을 모두 곱한 것과 같습니다.

> $$L(\theta | \text{데이터}) = \prod_{i=1}^{n} P(\text{데이터}_i | \theta)$$

동전 던지기 예시로 돌아가 봅시다. 파라미터는 $p$이고, 데이터는 '앞면 7번, 뒷면 3번' 입니다.

> $$L(p | \text{데이터}) = p^7 (1-p)^3$$

이제 우리의 목표는 이 $L(p)$를 최대로 만드는 $p$를 찾는 것입니다.

## 4. 로그 우도 (Log-Likelihood): 계산을 위한 마법

$p^7 (1-p)^3$ 같은 곱셈 함수는 미분하기가 까다롭습니다. 또한, 수많은 작은 확률들을 곱하다 보면 컴퓨터가 계산하기 어려운 아주 작은 숫자(numerical underflow)가 될 수 있습니다.

이때 사용하는 트릭이 바로 **[[03_로그_함수|로그(log)]]를 취하는 것**입니다. 로그 함수는 단조 증가 함수이므로, 원래 함수를 최대로 만드는 지점과 로그를 씌운 함수를 최대로 만드는 지점이 동일합니다.

- $\log(AB) = \log(A) + \log(B)$ (곱셈이 덧셈으로 바뀜)
- $\log(A^k) = k \log(A)$ (지수가 곱셈으로 내려옴)

위의 우도 함수에 로그를 취하면 **로그 우도 함수(Log-Likelihood Function)** 가 됩니다.

> $$\\log L(p) = \\log(p^7 (1-p)^3) = 7\\log(p) + 3\\log(1-p)$$

이제 이 덧셈 함수를 미분하여 최댓값을 찾는 것은 훨씬 쉬워집니다.

## 5. 최댓값 찾기 (Calculus to the Rescue!)

어떤 함수의 최댓값/최솟값은 그 함수를 미분해서 [[08_편미분과_그라디언트|기울기]]가 0이 되는 지점에서 나타납니다. 로그 우도 함수를 파라미터 $p$에 대해 미분하고, 그 결과가 0이 되는 $p$를 찾으면 됩니다.

> $$\\frac{d}{dp} [7\\log(p) + 3\\log(1-p)] = \\frac{7}{p} - \\frac{3}{1-p} = 0$$

이 방정식을 풀면 $7(1-p) = 3p$ -> $7 - 7p = 3p$ -> $10p = 7$ -> **$p = 0.7$** 이라는 결과를 얻을 수 있습니다. 우리의 직관과 정확히 일치합니다!

## 6. MLE와 AI

많은 머신러닝 모델의 학습 과정은 본질적으로 MLE입니다. 예를 들어, 로지스틱 회귀(Logistic Regression)는 주어진 데이터에 대해 우도를 최대로 만드는 파라미터(가중치)를 찾는 과정입니다. 딥러닝에서 자주 사용되는 손실 함수인 **[[16_정보_이론_기초#^교차 엔트로피|크로스 엔트로피(Cross-Entropy)]]를 최소화하는 것은 로그 우도를 최대화하는 것과 수학적으로 동일**합니다. 즉, 모델을 학습시킨다는 것은 데이터의 우도를 최대화하는 파라미터를 찾는 여정이라고 할 수 있습니다.

## 확인 문제

1.  확률과 우도의 차이점을 자신의 언어로 설명해보세요.
2.  우도 함수에 로그를 씌우는 이유는 무엇인가요? (2가지)
3.  공정한 동전($p=0.5$)을 4번 던졌을 때 모두 앞면이 나왔습니다. 이 데이터에 대한 우도 $L(p=0.5)$는 얼마일까요?

---

*MLE는 데이터로부터 모델을 학습하는 가장 기본적이고 중요한 원리 중 하나입니다. "데이터를 가장 잘 설명하는 파라미터를 찾는다"는 핵심 아이디어를 꼭 기억해주세요.*