# 01. K-최근접 이웃 (K-Nearest Neighbors, KNN)

## 1. 핵심 정의

**K-최근접 이웃(K-Nearest Neighbors, KNN)**은 **레이블이 지정된 훈련 데이터**를 사용하여 새로운 데이터 포인트의 레이블을 예측하는 **지도 학습(Supervised Learning)** 알고리즘입니다. 분류(Classification)와 회귀(Regression) 문제 모두에 사용될 수 있습니다.

-   **핵심 가정**: "서로 가까운 데이터 포인트는 비슷한 특징을 가질 것이다."
-   **이웃(Neighbors)**: 특정 데이터 포인트에 가장 가까운 K개의 훈련 데이터 포인트를 의미합니다.

## 2. 작동 원리

### 2.1. 거리 측정 (Distance Metrics)

KNN의 핵심은 '가깝다'는 것을 어떻게 정의하느냐, 즉 **거리(Distance)**를 어떻게 계산하느냐에 있습니다. 대표적인 거리 측정 방법은 다음과 같습니다.

-   **유클리드 거리 (Euclidean Distance)**: 두 점 사이의 가장 일반적인 직선 거리입니다.
    $$ d = \sqrt{\sum_{i=1}^{n}(q_i - p_i)^2} $$
-   **맨해튼 거리 (Manhattan Distance)**: 각 좌표축 방향으로의 이동 거리를 합산한 거리입니다. (도시의 블록을 따라 이동하는 것과 유사)
    $$ d = \sum_{i=1}^{n}|q_i - p_i| $$

### 2.2. 예측 과정

#### 분류 (Classification)

1.  **K 값 선택**: 고려할 이웃의 수($K$)를 결정합니다.
2.  **거리 계산**: 새로운 데이터(쿼리 포인트)에서 훈련 데이터의 모든 포인트까지의 거리를 계산합니다.
3.  **K개 이웃 선택**: 계산된 거리 중 가장 가까운 $K$개의 훈련 데이터 포인트를 선택합니다.
4.  **과반수 투표 (Majority Voting)**: 선택된 $K$개의 이웃 중에서 가장 빈도가 높은 클래스를 새로운 데이터의 예측 클래스로 결정합니다.

#### 회귀 (Regression)

분류와 과정은 유사하지만, 마지막 단계에서 차이가 있습니다.

-   선택된 $K$개 이웃의 목표 변수(수치)의 **평균(mean) 또는 중위수(median)**를 새로운 데이터의 예측값으로 사용합니다.

## 3. K 값의 영향과 최적 K 찾기

$K$ 값은 모델의 복잡도를 결정하는 매우 중요한 하이퍼파라미터입니다.

-   **K가 작을 때**: 모델이 매우 유연해져 훈련 데이터의 세부적인 패턴까지 학습하지만, 노이즈에도 민감해져 **[[과적합(Overfitting)]]**이 발생하기 쉽습니다.
-   **K가 클 때**: 모델이 단순해져 노이즈의 영향을 덜 받지만, 데이터의 미세한 패턴을 무시하여 **[[과소적합(Underfitting)]]**이 발생할 수 있습니다.

**최적의 K 찾기**:
일반적으로 테스트 데이터셋을 사용하여 다양한 $K$ 값에 대한 모델의 정확도를 측정하고, 가장 높은 정확도를 보이는 $K$ 값을 선택합니다.

## 4. KNN의 특징: 게으른 학습자 (Lazy Learner)

-   **훈련 단계 없음**: KNN은 별도의 훈련 과정이 없습니다. 단순히 훈련 데이터를 메모리에 저장하기만 합니다.
-   **예측 시 계산**: 새로운 데이터에 대한 예측 요청이 들어오면, 그때 비로소 저장된 모든 훈련 데이터와의 거리를 계산합니다. 이 때문에 '게으른 학습자'라고 불립니다.
-   **단점**: 이 방식은 예측 시점에 계산 비용이 매우 높고, 데이터가 많을수록 예측 속도가 느려지는 원인이 됩니다.

## 5. 주요 문제점과 해결책

### 1. 클래스 불균형 문제

-   **문제**: 데이터가 불균형할 경우, 다수 클래스가 이웃으로 더 많이 선택되어 예측을 지배하게 됩니다.
-   **해결책 (거리 가중치)**: 단순 투표가 아닌, 거리를 고려하여 가까운 이웃에 더 큰 가중치를 부여하는 **가중 투표(weighted voting)**를 사용합니다.

### 2. 특징 스케일 문제

-   **문제**: 특정 특징의 값 범위가 다른 특징보다 훨씬 크면, 그 특징이 거리 계산을 지배하여 예측이 편향될 수 있습니다. (예: '나이'는 1-100, '소득'은 1,000만-1억)
-   **해결책 ([[표준화|특징 스케일링]])**: 모든 특징을 비슷한 스케일로 조정합니다. 대표적으로 **표준화(Standardization)**나 **정규화(Normalization)** 기법을 사용합니다.

### 3. 차원의 저주 (Curse of Dimensionality)

-   **문제**: 데이터의 차원(특성의 수)이 증가할수록, 데이터 포인트 간의 거리가 멀어지고 데이터가 희소(sparse)해집니다. 고차원 공간에서는 '가까운 이웃'이라는 개념 자체가 무의미해져 모델의 성능이 급격히 저하됩니다.
-   **해결책**:
    -   **특징 선택 (Feature Selection)**: 예측에 중요한 관련 특징만 선택하여 차원을 줄입니다.
    -   **차원 축소 (Dimensionality Reduction)**: PCA와 같은 기법을 사용하여 고차원 데이터를 저차원으로 압축합니다.

## 6. 장점과 단점

### 장점

-   **직관적이고 간단함**: 모델의 작동 원리를 이해하기 매우 쉽습니다.
-   **비모수 모델**: 데이터의 분포에 대한 가정이 필요 없습니다.
-   **유연성**: 분류와 회귀 문제에 모두 적용 가능합니다.

### 단점

-   **계산 비용**: 예측 속도가 느리고, 데이터가 클수록 계산량이 기하급수적으로 증가합니다.
-   **메모리 사용량**: 전체 훈련 데이터를 저장해야 합니다.
-   **스케일에 민감**: 특징 스케일링이 필수적입니다.
-   **차원의 저주**: 고차원 데이터에 취약합니다.

## 7. 핵심 요약

-   KNN은 **거리 기반**의 간단하고 직관적인 지도 학습 알고리즘입니다.
-   분류는 **과반수 투표**, 회귀는 **평균/중위수**로 예측합니다.
-   **최적의 K 값**을 찾는 것이 모델 성능에 매우 중요합니다.
-   **특징 스케일링**과 **차원 축소**는 성능 향상을 위해 필수적으로 고려되어야 합니다.
-   **게으른 학습자** 특성으로 인해 예측 속도가 느린 단점이 있습니다.

#ai #classification #regression #knn #lazy-learning
