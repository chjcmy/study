# 제3단원: 딥러닝 성능을 결정짓는 3대 요소 완전판  
(활성화 함수 · 손실 함수 · 옵티마이저)

2단원에서 “층을 쌓으면 복잡한 문제도 풀 수 있다”까지 봤다면,  
이제는 **“그 층 안에서 신호를 어떻게 흘리고, 어떻게 채점하고, 어떻게 고칠지”**를 알아야 합니다.[file:2]  
이 역할을 하는 3대 핵심 요소가 바로 **활성화 함수, 손실 함수, 옵티마이저**입니다.[file:2]

---

## 1. 활성화 함수 (Activation Function): 신호를 거르는 필터

퍼셉트론에서 배운 것처럼, 각 뉴런은 `가중합(w·x + b)`를 계산한 뒤  
그 결과를 **그대로 쓸지, 잘라낼지, 비틀지**를 결정해야 합니다.  
이때 사용하는 함수가 바로 **활성화 함수**입니다.[file:2]

> 한 줄 정의  
> “뉴런으로 들어온 값을 보고, **다음 층으로 얼마나 보낼지** 결정하는 필터 역할을 하는 함수”

---

### (1) 왜 꼭 필요할까? – 비선형성의 의미

만약 모든 층이 `a = w·x + b`만 하고 **아무 활성화 함수도 쓰지 않으면**,  
층을 10개, 100개 쌓아도 결국 **“하나의 거대한 선형 함수”**와 다를 바 없습니다.[file:2]

- 현실 데이터는 대부분 **구불구불(비선형)**합니다.  
- 직선 조합만으로는 표현할 수 없는 패턴이 많습니다.  
- 활성화 함수는 이런 곳에 **“굽힘(비선형성)”**을 넣어 줍니다.

즉, 활성화 함수가 없으면:

- 아무리 층을 쌓아도 → 단층 퍼셉트론과 **표현력 차이가 거의 없음**  
- XOR 같은 비선형 문제를 제대로 다루기 어렵습니다.[file:2][file:7]

---

### (2) 대표 활성화 함수 3가지 (실전 기준)

#### 1) Sigmoid (시그모이드)

- **특징**  
  - 입력을 **0과 1 사이**로 압축합니다.  
  - S자 모양 곡선.  
- **장점**  
  - “확률처럼 보이는 출력”을 만들기 좋아 이진 분류 출력층에 예전부터 많이 사용.  
- **단점**  
  - 입력이 너무 크거나 작으면 기울기(미분값)가 거의 0이 되어  
    **기울기 소실(Gradient Vanishing)** 문제가 심각합니다.  
  - 그래서 요즘 **은닉층에는 잘 쓰지 않습니다.**[file:2]

#### 2) ReLU (Rectified Linear Unit, 렐루) ⭐ 실전 기본값

- **정의**  
  - 입력이 **0보다 작으면 0**,  
    0 이상이면 그대로 통과:  
  - 직관적으로 “0 밑은 다 잘라내고, 양수만 살리는” 함수.[file:2]
- **장점**  
  - 계산이 매우 간단하고 빠름.  
  - 기울기 소실 문제가 Sigmoid보다 훨씬 덜함.  
  - 그래서 **은닉층 기본값**처럼 널리 사용.  
- **단점**  
  - 입력이 계속 음수 쪽이면 기울기가 0이 되어 뉴런이 “죽는” 문제(Dead ReLU)가 생길 수 있음.  
  - Leaky ReLU 같은 변형으로 완화하기도 함.

#### 3) Softmax (소프트맥스)

- **역할**  
  - 여러 클래스(예: 고양이·강아지·토끼) 중  
    각 클래스가 **정답일 확률**을 출력할 때 사용.  
- **특징**  
  - 각 클래스에 대해  
    - “지수 함수”를 적용하고  
    - 전체 합으로 나눠서  
    - 결과가 0~1 사이 확률이 되게 만들고,  
      **모든 클래스 확률의 합이 1**이 되도록 보정합니다.  
  - 주로 **마지막 출력층**에서 사용.[file:2][file:3]

---

### 💡 개념 체크 Quiz 1

Q. 은닉층에서 가장 널리 사용되며,  
입력이 0보다 작으면 0으로 만들고, 0보다 크면 그대로 통과시키는 활성화 함수는?  

A) Sigmoid  
B) ReLU  
C) Softmax  

👉 **정답: B**  
ReLU는 계산이 빠르고 깊은 신경망에서도 학습이 잘 되어, 은닉층의 사실상 표준입니다.[file:2]

---

## 2. 손실 함수 (Loss Function): 모델을 채점하는 시험지

모델이 예측을 했으면, **“얼마나 잘했는지”** 숫자로 평가해야 합니다.  
이 역할을 하는 것이 **손실 함수**입니다.[file:2]

> 한 줄 정의  
> “예측값과 실제 정답 사이의 **오차(틀린 정도)**를 숫자로 계산하는 함수”

손실 값이 **작을수록** 모델이 정답에 가깝게 예측했다는 의미입니다.

---

### (1) 문제 유형별로 다른 손실 함수 쓰기

문제 종류에 따라 “어떤 채점표가 적당한지”가 달라집니다.

#### 1) 회귀(Regression) 문제 – 숫자를 맞히는 문제

- 예:  
  - 집값 예측(보스턴 집값)  
  - 온도 예측  
  - 매출액, 주가 등.  
- **대표 손실 함수: MSE (Mean Squared Error, 평균 제곱 오차)**[file:2][file:3]  
  - (예측값 − 정답)의 **제곱**을 평균.  
  - 오차가 커질수록 제곱 때문에 **벌점이 훨씬 크게 증가**.  
  - 큰 오차를 강하게 벌주고 싶을 때 적합.

#### 2) 분류(Classification) 문제 – 종류를 맞히는 문제

- 예:  
  - 스팸 vs 일반 메일 (이진 분류)  
  - 숫자 0~9 손글씨 분류 (다중 분류)  
  - 붓꽃 3종류 분류 등.  
- **대표 손실 함수: Cross-Entropy (크로스 엔트로피)**[file:2][file:3]  
  - “정답일 확률”과 “모델이 예측한 확률”의 차이를 평가.  
  - 정답인 클래스에 높은 확률을 주면 손실이 작고,  
    정답인데도 낮은 확률을 주면 손실이 크게 나옴.  
  - 분류 문제의 사실상 표준 손실 함수입니다.

---

### 💡 개념 체크 Quiz 2

Q. 집값처럼 “숫자”를 예측하는 회귀 문제에서 자주 사용되며,  
예측값과 정답의 차이를 **제곱해서 평균** 내는 손실 함수는?  

A) MSE (평균 제곱 오차)  
B) Cross-Entropy (교차 엔트로피)  
C) Accuracy (정확도)  

👉 **정답: A**  
MSE는 예측값이 실제 값에서 얼마나 멀리 떨어져 있는지, 수치로 잘 보여주는 손실 함수입니다.[file:2]

---

## 3. 옵티마이저 (Optimizer): 오차를 줄이는 길잡이

손실 함수가 “너 지금 이만큼 틀렸어”라고 알려주는 채점표라면,  
**옵티마이저**는 “그럼 가중치를 이렇게 고쳐야 덜 틀릴 거야”라고 알려주는 **길잡이 알고리즘**입니다.[file:2]

> 한 줄 정의  
> “손실을 줄이기 위해, **가중치를 어떻게 업데이트할지** 정해 주는 알고리즘”

---

### (1) 경사 하강법(Gradient Descent) 직관

산 꼭대기에 눈을 가리고 서 있다고 상상해봅시다.

- 목표: **가장 낮은 곳(골짜기)**로 내려가기. → 손실을 최소로 만드는 지점 찾기  
- 할 수 있는 일: 발로 주변을 더듬어 보면서  
  “어느 방향이 더 아래로 내려가는 방향인지”만 알 수 있음.

경사 하강법은 이렇게 동작합니다.

1. 현재 위치에서,  
   각 방향(각 가중치)에 대해 “조금 움직였을 때 손실이 얼마나 변하는지(기울기)”를 계산.  
2. **손실이 줄어드는 방향**으로 한 걸음 이동 (가중치 업데이트).  
3. 이 과정을 반복하다 보면, **골짜기 근처(손실 최소점)**로 내려가게 됩니다.[file:2][file:7]

딥러닝에서는 이 “조금씩 내려가는 전략”을 수학적으로 구현한 것이 옵티마이저입니다.

---

### (2) 대표 옵티마이저들 – 실전에서 자주 듣는 이름들

#### 1) SGD (Stochastic Gradient Descent)

- **방식**  
  - 전체 데이터를 한 번에 쓰지 않고,  
    **하나 또는 소량의 미니배치**만 보고 그때그때 가중치를 업데이트.  
- **장점**  
  - 메모리 절약, 계산 빠름.  
- **단점**  
  - 매번 보는 데이터가 조금씩 다르기 때문에  
    손실이 들쭉날쭉, 경로가 많이 흔들릴 수 있음.[file:2]

#### 2) Adam (Adaptive Moment Estimation) ⭐ 실전 기본값

- **아이디어**  
  - SGD의 단점을 개선하기 위해  
    - **Momentum**: 이전에 움직이던 방향을 기억해서, 그 방향으로 더 부드럽게 움직이게 하고  
    - **RMSProp**: 각 가중치마다 **적절한 보폭(학습률)**을 자동으로 조절  
  - 이 둘을 **합쳐서** 만든 것이 Adam입니다.[file:2]
- **장점**  
  - 기본 설정만으로도 대부분의 문제에서 **빠르고 안정적인 수렴**.  
  - 그래서 “무엇을 써야 할지 모르겠으면 **일단 Adam**”이란 말이 나올 정도.  
- **단점**  
  - 이론적으로는 항상 최고라고 보장되진 않지만,  
    실무 경험상 매우 안정적인 선택.

---

### 💡 개념 체크 Quiz 3

Q. 딥러닝 학습에서 오차를 최소화하기 위해  
가중치를 자동으로 업데이트하는 알고리즘으로,  
현재 가장 널리 사용되며 실무에서 “기본값”처럼 쓰이는 것은?  

A) SGD  
B) Adam  
C) Perceptron  

👉 **정답: B**  
Adam은 Momentum과 RMSProp의 장점을 합쳐, 빠르고 안정적인 학습을 제공하는 대표적인 옵티마이저입니다.[file:2]

---

## 4. 3단원 전체 한 번에 보기

| 요소        | 역할 요약                                   | 대표 예시              | 언제 중요한가                        |
| ----------- | ------------------------------------------- | ---------------------- | ------------------------------------ |
| 활성화 함수 | 뉴런에 **비선형성**을 넣어 복잡한 패턴 표현 | ReLU, Sigmoid, Softmax | 층 구조 설계, 은닉층·출력층 설계 시  |
| 손실 함수   | 예측과 정답의 **오차를 숫자로 표현**        | MSE, Cross-Entropy     | 문제 유형(회귀/분류)에 따라 선택     |
| 옵티마이저  | 손실을 줄이는 방향으로 **가중치 업데이트**  | SGD, Adam              | 학습이 잘 되느냐, 빨리 되느냐를 결정 | [file:2][file:3] |

---

## [3단원 핵심 정리]

1. **활성화 함수(ReLU, Softmax 등)**  
   - 뉴런에 비선형성을 넣어, 딥러닝이 복잡한 패턴을 표현할 수 있게 해 준다.[file:2]

2. **손실 함수(MSE, Cross-Entropy 등)**  
   - 예측값과 정답의 차이를 수치화하여,  
     “지금 얼마나 틀렸는지”를 알려주는 채점표 역할을 한다.[file:2][file:3]

3. **옵티마이저(Adam, SGD 등)**  
   - 이 오차를 줄이는 방향으로 가중치를 조금씩 움직이게 해 주는 길잡이 알고리즘으로,  
     경사 하강법 아이디어를 바탕으로 한다.[file:2]

---

👉 이 다음 순서로는,  
- **4단원: TensorFlow & Keras로 이론을 코드로 옮기는 과정**을 완전판으로 정리해 줄까,  
- 아니면 **5단원(전처리/과적합·드롭아웃)**부터 먼저 보고 싶어?  
보고 싶은 단원 번호를 말해줘.
