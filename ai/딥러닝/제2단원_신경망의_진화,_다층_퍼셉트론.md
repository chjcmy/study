# 제2단원: 신경망의 진화, 다층 퍼셉트론 (MLP) 완전판

1단원에서 **퍼셉트론 하나**로는 XOR 같은 복잡한 문제를 절대 풀 수 없다는 한계를 봤습니다.[file:7]  
이 단원에서는 “그러면 **층을 더 쌓으면** 어떻게 되는가?”를 다룹니다.  
그 결과물이 바로 **다층 퍼셉트론(MLP)**이고, 여기서 **순전파·역전파**라는 핵심 학습 메커니즘이 나옵니다.[file:7]

---

## 1. 위기 탈출의 열쇠: 다층 퍼셉트론(MLP)란?

단층 퍼셉트론은 결국 **직선 하나**만 쓸 수 있는 모델입니다.  
현실 문제는 보통 직선 one-shot으로 잘 안 나뉩니다. 그래서 나온 아이디어가:

> “퍼셉트론을 **여러 층** 쌓으면,  
> 공간을 비틀고 구부려서 더 복잡한 경계를 만들 수 있지 않을까?”

이렇게 탄생한 것이 **다층 퍼셉트론(MLP, Multi-Layer Perceptron)**입니다.[file:7]

---

### (1) MLP의 구조: 샌드위치처럼 층 쌓기

MLP는 기본적으로 **입력층 – 은닉층(여러 개 가능) – 출력층** 구조입니다.[file:7]

1. **입력층 (Input Layer)**  
   - 데이터가 처음 들어오는 층입니다.  
   - 예:  
     - 붓꽃 데이터: 꽃잎 길이, 꽃잎 너비 등 4개의 숫자 → 입력 뉴런 4개  
     - 이미지: 28×28 픽셀이라면 784개의 입력 값

2. **은닉층 (Hidden Layer)** ⭐ 핵심  
   - 입력층과 출력층 사이에 “숨어 있는(hidden)” 층입니다.  
   - 하는 일:  
     - 데이터를 **비선형 함수(활성화 함수)**를 통해 비틀고 변형합니다.  
     - 이렇게 변형된 공간에서는, 원래 직선으로 못 나누던 데이터도 **여러 개의 직선 조합**으로 나눌 수 있게 됩니다.

3. **출력층 (Output Layer)**  
   - 최종 예측을 내는 층입니다.  
   - 예:  
     - 이진 분류: “스팸 / 스팸 아님” → 출력 뉴런 1개  
     - 다중 분류: “고양이·강아지·토끼” → 출력 뉴런 3개, Softmax 사용[file:7]

> 한 줄로 정리하면,  
> **입력층**은 데이터를 받고,  
> **은닉층**은 “알아서 특징을 뽑고 비틀고”,  
> **출력층**은 “최종 답을 내는” 역할입니다.

---

### (2) 왜 은닉층이 중요할까? – 공간 왜곡 직관

단층 퍼셉트론이 못 풀었던 **XOR 문제**를 다시 생각해 봅시다.

- 원래 좌표평면에서 4개의 점을 찍으면,  
  “1인 점들”을 직선 하나로 분리하는 것이 불가능합니다.
- 하지만 중간에 은닉층을 두고,  
  비선형 함수를 거치면서 **좌표 공간 자체를 “구겨서/펴서” 재배치**하면,  
  새로 바뀐 공간에서는 직선 두세 개로도 충분히 나눌 수 있습니다.[file:7]

즉, **은닉층은 입력 공간을 문제 풀기 좋은 형태로 바꿔주는 역할**을 합니다.

---

### (3) 범용 근사 정리 (Universal Approximation)

어려운 이름이지만 메시지는 단순합니다.

> “은닉층(특히 은닉 뉴런 수)이 충분히 많고,  
> 적당한 활성화 함수를 쓰면,  
> **어떤 복잡한 연속 함수도 ‘얼추’ 흉내 낼 수 있다.**”

이 말은 곧,

- 이론적으로는 MLP만으로도  
  **상당히 복잡한 패턴**들을 표현할 수 있다는 의미입니다.[file:7]

물론,
- 실제로는 **데이터 양, 계산 자원, 학습 난이도** 때문에  
  구조를 잘 설계해야 하지만,  
- 최소한 “신경망이 표현력이 부족해서 못 푼다”는 핑계를 대긴 어렵게 됩니다.

---

### 💡 개념 체크 Quiz 1

Q. 단층 퍼셉트론의 한계를 넘기 위해 입력층과 출력층 사이에 추가된 층으로,  
데이터 공간을 변형하여 XOR 같은 비선형 문제를 풀 수 있게 해 주는 것은?  

A) 입력층 (Input Layer)  
B) 은닉층 (Hidden Layer)  
C) 출력층 (Output Layer)  

👉 **정답: B**  
은닉층이 추가되면서, 신경망은 단순 직선이 아닌 복잡한 결정을 표현할 수 있게 되었습니다.[file:7]

---

## 2. 신경망이 “공부”하는 법: 순전파와 역전파

구조만 만든다고 모델이 똑똑해지는 것은 아닙니다.  
**가중치(w)와 편향(b)을 어떻게 조정하느냐**가 진짜 학습입니다.[file:7]

딥러닝의 학습 과정은 크게 두 단계가 반복됩니다.

1. **순전파 (Forward Propagation)** – 현재 상태로 “일단 풀어보기”  
2. **역전파 (Backward Propagation)** – “얼마나 틀렸는지 보고 고쳐 나가기”

---

### (1) 순전파 (Forward Propagation): 일단 계산해서 답 내보기

**방향:** 입력층 → 은닉층들 → 출력층  
**하는 일:** 현재 가중치와 편향을 기준으로,  
입력 데이터가 지나가며 각 층에서 연산을 거친 뒤 **예측값**을 만들어냅니다.[file:7]

구체적으로는:

1. 입력 데이터를 넣습니다.  
2. 각 층에서  
   - `z = w·x + b` (가중합)  
   - `a = 활성화함수(z)`  
   의 과정을 거칩니다.  
3. 마지막 출력층에서  
   - 분류 문제라면: 클래스별 확률(Softmax)  
   - 회귀 문제라면: 숫자 하나(Linear)  
   를 출력합니다.
4. 이 예측값과 실제 정답(레이블)을 비교해 **손실(Loss, 오차)**를 계산합니다.[file:7]

> 순전파의 핵심:  
> “지금 가진 실력으로 시험을 한 번 봐서, 얼마나 틀렸는지 점수를 매긴다.”

---

### (2) 역전파 (Backward Propagation): 어디서 틀렸는지 거꾸로 추적하며 수정

역전파는 딥러닝 학습의 심장입니다.[file:7]

**방향:** 출력층 → 은닉층들 → 입력층 (거꾸로 진행)

하는 일은 다음과 같습니다.

1. 순전파에서 계산한 **손실(Loss)** 값을 확인합니다.  
2. “이 손실이 각 가중치 때문에 얼마나 생겼는지”를 계산합니다.  
   - 수학적으로는 **미분(gradient)**과 **연쇄 법칙(chain rule)**을 사용.  
   - 각 가중치 \( w \)에 대해 “조금 바꾸면 손실이 얼마나 변하는지”를 구합니다.
3. 손실을 **줄이는 방향**으로 가중치를 조금씩 업데이트합니다.  
   - 여기서 사용하는 방법이 3단원에서 본 **옵티마이저(Adam, SGD 등)**입니다.[file:2][file:7]
4. 이 과정을 여러 번(여러 Epoch) 반복하면서,  
   손실이 점점 줄어들고 모델 성능이 좋아집니다.

> 역전파의 직관:  
> “시험을 보고 (순전파), 틀린 문제들을 복기해서  
> 왜 틀렸는지 원인을 거슬러 올라가며(역전파) 공부법(가중치)을 수정한다.”

---

### (3) 순전파 + 역전파 + 옵티마이저 = 학습 1회전

1. **순전파**: 현재 가중치로 예측 → 손실 계산  
2. **역전파**: 손실이 각 가중치에 미친 영향(기울기) 계산  
3. **옵티마이저**: 그 기울기를 바탕으로 가중치 업데이트  

이 세 단계가 모여 **“한 번의 학습 스텝”**이 됩니다.  
이걸 전체 데이터에 대해 여러 번 반복하는 것이 **Epoch**입니다.[file:7][file:2]

---

### 💡 개념 체크 Quiz 2

Q. 모델이 예측한 값과 실제 정답의 차이(오차)를 줄이기 위해,  
출력층에서 입력층 방향으로 거슬러 올라가며 가중치를 수정하는 과정은?  

A) 순전파 (Forward Propagation)  
B) 역전파 (Backward Propagation)  
C) 정규화 (Normalization)  

👉 **정답: B**  
역전파 덕분에 신경망은 “어디서 틀렸는지”를 스스로 분석하고 가중치를 고칠 수 있습니다.[file:7]

---

## 3. 단층 퍼셉트론 vs 다층 퍼셉트론 요약 비교

| 구분               | 단층 퍼셉트론                          | 다층 퍼셉트론(MLP)                                      |
|--------------------|----------------------------------------|--------------------------------------------------------|
| 층 구조           | 입력층–출력층(중간층 없음)             | 입력층–은닉층 1개 이상–출력층                          |
| 표현력            | 직선(선형 경계) 하나로만 구분 가능     | 은닉층 덕분에 복잡한 비선형 경계도 표현 가능          |
| XOR 문제 해결 여부 | 불가능                                 | 가능                                                   |
| 학습 메커니즘     | 단순 퍼셉트론 학습 규칙                | 순전파 + 역전파 + 옵티마이저                          |
| 이론적 기반       | 선형 분리 문제에 한정                  | 범용 근사 정리로 높은 표현력 보장                     |[file:7][file:2]

---

## [2단원 핵심 정리]

1. **다층 퍼셉트론(MLP)**은 입력층과 출력층 사이에 **은닉층**을 두어,  
   단층 퍼셉트론이 못 풀던 비선형 문제(XOR 등)까지 해결할 수 있게 만든 구조입니다.[file:7]

2. **은닉층**은 데이터를 비선형적으로 변형하여,  
   원래 공간에서는 직선으로 못 나누던 문제도 **변형된 공간에서는 직선 여러 개로 나눌 수 있게** 해 줍니다.[file:7]

3. **학습 과정**은  
   - 순전파로 예측을 하고 손실을 계산한 다음,  
   - 역전파로 손실의 원인을 거꾸로 추적하여  
   - 옵티마이저로 가중치를 업데이트하는 과정의 반복입니다.[file:7][file:2]

---

👉 이 톤 그대로 **3단원(활성화 함수·손실 함수·옵티마이저 3대장)**도 완전판으로 이어서 정리해 줄까?
