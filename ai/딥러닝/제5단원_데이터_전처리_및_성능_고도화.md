# 제5단원: 데이터 전처리 및 성능 고도화 완전판  
(정규화 · 원-핫 인코딩 · 과적합 · 드롭아웃)

4단원까지는 “모델을 어떻게 만들고 학습시키는가”를 봤습니다.[file:3]  
하지만 **좋은 데이터**와 **과적합 방지** 없이는, 어떤 모델도 실전에서 버티지 못합니다.  
이 단원에서는 **데이터 전처리**와 **과적합·드롭아웃**을 비전공자도 이해할 수 있게 정리합니다.[file:8]

---

## 1. 데이터 전처리 (Preprocessing): 모델이 먹기 좋은 재료 만들기

요리를 잘하려면 재료부터 깨끗하게 손질해야 하듯,  
딥러닝에서도 **날것의 데이터(raw data)**를 그대로 넣으면 문제가 생깁니다.[file:8]

> 한 줄 정의  
> “전처리 = 데이터의 모양과 범위를 **모델이 학습하기 좋은 형태로 다듬는 과정**”

문서에서는 특히 **정규화(스케일링)**과 **원-핫 인코딩**을 강조합니다.[file:8]

---

### (1) 정규화 (Normalization/Scaling): 숫자 범위 맞추기

**문제 상황**

- 어떤 데이터셋에는  
  - 키: 150~190 (대략 150~190)  
  - 시력: 0.1~2.0  
  - 나이: 10~80  
- 이렇게 서로 **단위·범위가 제각각**인 숫자들이 섞여 있습니다.  
- 이걸 그대로 모델에 넣으면,  
  - 값이 큰 특징(예: 170 같은 키)이  
  - 값이 작은 특징(예: 1.0 같은 시력)보다  
    “훨씬 더 중요하다”고 모델이 **오해**할 수 있습니다.[file:8]

**해결 아이디어**

> 모든 숫자를 비슷한 범위(예: 0~1 사이)로 **맞춰주자.**

- 예: 키 150~190을 0.0~1.0 사이로 선형 변환  
- 이렇게 하면  
  - 각 특징이 **비슷한 스케일**에서 기여하게 되어  
  - 학습이 더 안정되고 빠르게 진행됩니다.[file:8]

실제로는 Min-Max 스케일링, 표준화 등 다양한 방법이 있지만,  
문서에서 강조하는 포인트는 “범위를 맞춰줘야 한다”는 개념입니다.

---

### (2) 원-핫 인코딩 (One-Hot Encoding): 문자를 숫자로 바꾸는 똑똑한 방법

**문제 상황**

- 카테고리형 데이터 예: 과일 = {사과, 바나나, 오렌지}  
- 컴퓨터는 “사과”, “바나나” 같은 **문자**를 직접 이해하지 못합니다.  
- 단순히  
  - 사과=1, 바나나=2, 오렌지=3  
  이렇게 번호만 붙이면,  
  - 모델이 “3은 1보다 3배 중요하다” 같은 **이상한 의미**를 학습할 수 있습니다.[file:8]

**원-핫 인코딩 아이디어**

> “각 카테고리마다 **자기 자리**를 하나 만들고,  
> 해당되는 자리만 1, 나머지는 0으로 두자.”

- 예:  
  - 사과   → [1, 0, 0]  
  - 바나나 → [0, 1, 0]  
  - 오렌지 → [0, 0, 1]  
- 특징  
  - 1은 “해당됨(Hot)”  
  - 0은 “해당 안 됨(Cold)”  
- 이렇게 하면 **순서·대소관계가 사라지기 때문에**,  
  “오렌지가 사과보다 3배 크다” 같은 오해가 사라집니다.[file:8]

---

### 💡 개념 체크 Quiz 1

Q. '사과, 바나나' 같은 **범주형(문자)** 데이터를,  
컴퓨터가 이해할 수 있는 **0과 1 배열**로 바꾸는 대표적인 기법은?  

A) 정규화 (Normalization)  
B) 원-핫 인코딩 (One-Hot Encoding)  
C) 드롭아웃 (Dropout)  

👉 **정답: B**  
각 카테고리에 자기 위치를 주고, 해당되는 위치만 1로 켜 주는 방식이라서 “One-Hot”이라고 부릅니다.[file:8]

---

## 2. 과적합 (Overfitting): 공부를 너무 열심히 해서 망하는 상태

딥러닝의 **최대 적** 중 하나가 과적합입니다.[file:8]

> 한 줄 정의  
> “연습 문제(학습 데이터)는 기가 막히게 맞추는데,  
> 실제 시험(새 데이터)에서는 엉망이 되는 상태”

---

### (1) 과적합의 증상

다음과 같은 현상이 보이면 과적합을 의심해야 합니다.[file:8]

- 학습 데이터에 대한 정확도(Train Accuracy)는 99%  
- 검증/테스트 데이터에 대한 정확도(Validation Accuracy)는 60%  
- 그래프를 그려 보면  
  - 학습 손실은 계속 감소  
  - 검증 손실은 어느 순간부터 **다시 증가**하는 패턴

즉, 모델이 “연습 문제 답만 외운 상태”라  
**응용력이 없는 학생**이 된 것입니다.

---

### (2) 과적합이 생기는 원인

대표적인 원인은 다음과 같습니다.[file:8]

1. **모델이 너무 복잡할 때**  
   - 층이 너무 깊거나, 뉴런 수가 지나치게 많아서  
     데이터에 포함된 **노이즈(우연한 특징)**까지 다 외워버리는 경우.

2. **학습을 너무 오래 시킬 때 (Epoch 과다)**  
   - 처음에는 잘 일반화하다가,  
   - 어느 시점부터 점점 “훈련 데이터 특이점”만 외우기 시작.

3. **데이터가 너무 적거나 편향돼 있을 때**  
   - 특정 패턴만 반복적으로 등장하면,  
     그 패턴만 과하게 학습해 버립니다.

---

### 💡 개념 체크 Quiz 2

Q. 모델이 학습 데이터는 거의 완벽하게 맞히지만,  
새로운 데이터에 대해서는 성능이 급격히 떨어지는 현상을 무엇이라 하는가?  

A) 과소적합 (Underfitting)  
B) 과적합 (Overfitting)  
C) 최적화 (Optimization)  

👉 **정답: B**  
연습에만 “Over(과하게)” 딱 맞춰져 있어서, 실전에서는 약해진 상태입니다.[file:8]

---

## 3. 드롭아웃 (Dropout): 일부러 멍청하게 만들어 더 강하게 만드는 기법

과적합을 막는 대표 기법 중 하나가 **드롭아웃(Dropout)**입니다.[file:8]

> 한 줄 정의  
> “학습할 때 일부 뉴런을 **랜덤하게 꺼버려서**,  
> 특정 뉴런/특징에 너무 의존하지 않도록 만드는 방법”

---

### (1) 드롭아웃의 동작 원리

- 학습 중에, 예를 들어 드롭아웃 비율이 0.5라면:  
  - 은닉층 뉴런의 **절반 정도를 랜덤하게 꺼 버립니다.**  
- 이 상태로 순전파·역전파를 수행합니다.  
- 다음 미니배치에서는 **다른 뉴런들이 꺼질 수도** 있습니다.

이렇게 하면,

- 어떤 한 뉴런이나 특정 경로에 의존하는 대신,  
- **여러 경로를 골고루 사용하는 “탄탄한” 표현**을 배우게 됩니다.[file:8]

> 비유  
> - 항상 한쪽 눈만 쓰면 그쪽에만 의존하게 되지만,  
> - 연습할 때 일부러 한쪽 눈을 가리고 연습하면  
>   양쪽 다 써도 잘 보이게 되는 것과 비슷합니다.

---

### (2) 왜 과적합 방지에 도움이 될까?

- 특정 특징들만 집착해서 외우는 것이 어렵게 됩니다.  
- 매번 다른 일부 뉴런이 꺼지기 때문에,  
  모델은 **여러 조합에도 잘 작동하는 일반적인 패턴**을 학습하게 됩니다.[file:8]
- 결과적으로  
  - 학습 데이터에만 딱 맞는 대신,  
  - 새로운 데이터에도 잘 작동하는 **일반화 능력**이 향상됩니다.

**중요 포인트**

- 드롭아웃은 **학습할 때만** 적용합니다.  
- 평가(Evaluate)나 추론(Predict) 단계에서는 **모든 뉴런을 켜고** 전체 능력을 100% 사용합니다.[file:8]

---

### 💡 개념 체크 Quiz 3

Q. 과적합을 방지하기 위해,  
학습 과정에서 일부 뉴런을 임의로 비활성화하여  
특정 특징에 대한 의존도를 낮추는 기법은?  

A) 드롭아웃 (Dropout)  
B) 풀링 (Pooling)  
C) 컴파일 (Compile)  

👉 **정답: A**  
Dropout은 “연결을 일부러 끊어(Drop)” 모델을 더 튼튼하게 만드는 대표적인 정규화 기법입니다.[file:8]

---

## 4. 이 단원 내용 한눈에 보기

| 주제         | 한 줄 정의                                         | 예시/키워드                     |
| ------------ | -------------------------------------------------- | ------------------------------- |
| 정규화       | 특징마다 숫자 범위를 맞춰 **공정하게 학습**하게 함 | 0~1 스케일링, 키·시력 같이 사용 |
| 원-핫 인코딩 | 문자/카테고리를 0/1 벡터로 표현                    | [1,0,0], [0,1,0], [0,0,1]       |
| 과적합       | 연습 문제는 잘 맞추지만 새 문제는 못 푸는 상태     | Train 99%, Val 60%              |
| 드롭아웃     | 뉴런 일부를 랜덤하게 꺼서 과적합을 줄이는 기법     | p=0.5, 학습 때만 적용           | [file:8] |

---

## [5단원 핵심 정리]

1. **전처리(정규화, 원-핫 인코딩)**는  
   날것의 데이터를 모델이 이해하기 쉬운 형태로 다듬는 필수 과정이다.[file:8]

2. **과적합(Overfitting)**은  
   학습 데이터에만 너무 맞춰져서,  
   실제 새로운 데이터 성능이 떨어지는 위험한 상태다.[file:8]

3. **드롭아웃(Dropout)**은  
   학습 중에 일부 뉴런을 랜덤하게 끔으로써,  
   특정 패턴에 과도하게 의존하지 않게 만들고 **일반화 성능**을 높여 준다.[file:8]

---

👉 다음은 **제6단원: 컴퓨터 비전의 시작**으로 넘어가서,  
“이미지를 숫자로 보는 법”부터 CNN으로 연결되는 이야기까지 완전판으로 정리해 줄까?
