# 학습 (Learning)

인공 신경망의 **학습(Learning)** 또는 **훈련(Training)** 과정은 모델이 주어진 데이터를 이용하여 **오차(Error)**를 스스로 줄여나가며 최적의 **가중치(Weight)**와 **편향(Bias)**을 찾아가는 전체적인 절차를 의미합니다.

이 과정은 **순전파(Forward Propagation)**, **손실 계산(Loss Calculation)**, **역전파(Backpropagation)**, **파라미터 업데이트(Parameter Update)**라는 네 가지 핵심 단계가 반복적으로 수행되는 사이클입니다.

## 신경망 학습의 전체 과정

학습은 **에포크(Epoch)**라는 단위로 이루어집니다. 1 에포크는 전체 학습 데이터셋을 모두 사용하여 한 번의 학습 사이클을 완료한 것을 의미합니다.

하나의 학습 사이클은 다음과 같이 구성됩니다.

### 1. 순전파 (Forward Propagation)

*   **목표:** 입력 데이터로부터 모델의 예측값을 계산합니다.
*   **과정:**
    1.  학습 데이터의 **입력값(Input)**을 신경망의 입력층에 주입합니다.
    2.  입력값은 신경망의 각 층을 순서대로 거치며, 각 뉴런에서 **가중합(Weighted Sum)**과 **활성화 함수(Activation Function)** 계산을 통해 다음 층으로 전달됩니다.
    3.  이 과정이 출력층까지 이어져 최종 **예측값(Prediction)**이 산출됩니다.

### 2. 손실 계산 (Loss Calculation)

*   **목표:** 모델의 예측이 얼마나 틀렸는지를 정량적으로 측정합니다.
*   **과정:**
    1.  순전파를 통해 얻은 **예측값**과 데이터의 **실제 정답 값(Ground Truth)**을 비교합니다.
    2.  **손실 함수(Loss Function)** (예: MSE, Cross-Entropy)를 사용하여 둘 사이의 **오차(Error) 또는 손실(Loss)**을 계산합니다.
    3.  손실 값이 크다는 것은 모델의 예측이 많이 틀렸다는 의미이며, 작을수록 예측이 정확하다는 의미입니다.

### 3. 역전파 (Backpropagation)

*   **목표:** 계산된 손실 값에 각 가중치와 편향이どれだけ 영향을 미쳤는지(**기울기(Gradient)**)를 계산합니다.
*   **과정:**
    1.  **연쇄 법칙(Chain Rule)**을 사용하여 출력층에서부터 입력층 방향으로 역으로 진행합니다.
    2.  손실 함수를 각 층의 가중치와 편향으로 편미분하여, 손실에 대한 각각의 **기울기**를 계산합니다.
    3.  이 기울기는 "손실을 가장 효과적으로 줄이기 위해 각 가중치를 어느 방향으로, 얼마나 조절해야 하는지"에 대한 정보를 담고 있습니다.

### 4. 파라미터 업데이트 (Parameter Update)

*   **목표:** 역전파를 통해 얻은 기울기 정보를 이용하여 모델의 가중치와 편향을 갱신합니다.
*   **과정:**
    1.  **최적화 알고리즘(Optimizer)** (예: SGD, Adam)을 사용하여 계산된 기울기를 바탕으로 가중치와 편향을 업데이트합니다.
    2.  업데이트 규칙: `새로운 가중치 = 기존 가중치 - (학습률 * 기울기)`
    3.  **학습률(Learning Rate)**은 업데이트의 보폭을 결정하며, 이 값을 통해 모델이 손실 함수의 최저점을 향해 점진적으로 이동하도록 합니다.

---

## 반복 학습

신경망은 위의 4가지 단계를 수많은 **에포크(Epoch)**와 **이터레이션(Iteration)** 동안 반복합니다. 이 과정을 통해 손실 함수의 값은 점차 줄어들고, 모델은 입력 데이터의 패턴을 학습하여 새로운 데이터에 대해서도 정확한 예측을 할 수 있는 능력을 갖추게 됩니다.

결론적으로, 학습 과정은 모델이 "실수를 통해 배우는" 과정이라고 비유할 수 있습니다. 오차를 계산하고, 그 원인을 역추적하여(역전파), 실수를 줄이는 방향으로 스스로를 수정(업데이트)해나가는 것입니다.
