# 오차 (Error)와 손실 함수 (Loss Function)

인공지능 모델 학습에서 **오차(Error)**는 모델이 예측한 값과 실제 정답 값 사이의 차이를 의미합니다. 모델의 성능이 얼마나 나쁜지를 나타내는 지표이며, 학습의 목표는 이 오차를 최소화하는 것입니다.

**손실 함수(Loss Function)** 또는 **비용 함수(Cost Function)**는 이러한 오차를 정량적으로 계산하는 함수입니다. 신경망은 손실 함수의 값을 최소화하는 방향으로 가중치(Weight)와 편향(Bias)을 업데이트하며 학습을 진행합니다.

## 손실 함수의 종류

손실 함수는 해결하고자 하는 문제의 유형(회귀, 분류 등)에 따라 다르게 선택됩니다.

### 1. 회귀 (Regression) 문제의 손실 함수

연속적인 값을 예측하는 회귀 문제에서는 주로 예측값과 실제값의 거리나 차이를 기반으로 손실을 계산합니다.

*   **평균 제곱 오차 (Mean Squared Error, MSE):**
    *   가장 널리 사용되는 회귀 손실 함수입니다.
    *   `MSE = (1/n) * Σ(실제값 - 예측값)²`
    *   오차의 제곱을 사용하기 때문에, 예측값이 실제값과 멀리 떨어질수록 (즉, 오차가 클수록) 손실이 기하급수적으로 커지는 특징이 있습니다.
    *   큰 오차에 대해 민감하게 반응하여 모델이 큰 실수를 하지 않도록 학습시킵니다.

*   **평균 절대 오차 (Mean Absolute Error, MAE):**
    *   `MAE = (1/n) * Σ|실제값 - 예측값|`
    *   오차의 절대값을 사용합니다.
    *   MSE와 달리 이상치(outlier)에 덜 민감합니다. 예측값이 크게 벗어나더라도 손실이 선형적으로 증가하기 때문입니다.

### 2. 분류 (Classification) 문제의 손실 함수

주어진 데이터를 특정 클래스로 분류하는 문제에서는 클래스별 확률 분포를 기반으로 손실을 계산합니다.

*   **크로스 엔트로피 오차 (Cross-Entropy Error):**
    *   분류 문제에서 가장 일반적으로 사용되는 손실 함수입니다.
    *   모델이 예측한 확률 분포와 실제 정답의 확률 분포 사이의 차이를 측정합니다.
    *   예측이 정답에 가까울수록 손실은 0에 가까워지고, 멀어질수록 손실은 급격히 커집니다.
    *   **이진 크로스 엔트로피 (Binary Cross-Entropy):**
        *   두 개의 클래스(예: 0 또는 1, 합격/불합격)로 분류하는 이진 분류 문제에 사용됩니다.
        *   모델이 출력하는 단일 확률 값(주로 시그모이드 함수 출력)을 이용해 손실을 계산합니다.
    *   **범주형 크로스 엔트로피 (Categorical Cross-Entropy):**
        *   세 개 이상의 클래스로 분류하는 다중 클래스 분류 문제에 사용됩니다.
        *   모델이 각 클래스에 대해 출력하는 확률 배열(주로 소프트맥스 함수 출력)을 이용해 손실을 계산합니다.

## 오차의 역할

*   **학습 방향 제시:** 손실 함수의 값(오차)은 모델이 얼마나 잘못 예측했는지를 알려주며, **역전파(Backpropagation)** 과정에서 이 값을 줄이는 방향으로 가중치를 업데이트하도록 하는 기준이 됩니다.
*   **성능 평가 지표:** 학습 과정뿐만 아니라, 학습된 모델의 성능을 평가하는 데도 사용됩니다. 테스트 데이터셋에 대한 손실 값을 통해 모델의 일반화 성능을 가늠할 수 있습니다.

결론적으로, 오차와 손실 함수는 인공지능 모델이 "정답"에 더 가까워지도록 학습 경로를 안내하는 핵심적인 역할을 수행합니다.

---
## 연관 문서
- 이전 단계: [[1. 순전파]]
- 다음 단계: [[3. 역전파]]

