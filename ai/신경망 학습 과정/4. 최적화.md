# 최적화 (Optimization)

최적화는 인공 신경망의 학습 과정에서 **손실 함수(Loss Function)의 값을 최소화**하는 최적의 **가중치(Weight)**와 **편향(Bias)**을 찾는 과정을 의미합니다. 즉, 모델이 가장 낮은 오차를 갖도록 파라미터를 조정하는 작업입니다.

최적화를 위해 사용되는 알고리즘을 **최적화기(Optimizer)**라고 부르며, 가장 기본이 되는 알고리즘은 **경사 하강법(Gradient Descent)**입니다.

## 경사 하강법 (Gradient Descent)

경사 하강법은 손실 함수의 기울기(Gradient)를 이용하여 손실이 가장 가파르게 감소하는 방향으로 가중치를 업데이트하는 방식입니다. 마치 산에서 가장 가파른 경사를 따라 내려가면 계곡의 가장 낮은 지점에 도달할 수 있다는 아이디어와 같습니다.

*   **기울기(Gradient):** 특정 지점에서 손실 함수를 각 가중치로 편미분한 값입니다. 기울기는 해당 지점에서 함수 값이 가장 크게 증가하는 방향을 나타냅니다.
*   **업데이트 규칙:** `새로운 가중치 = 기존 가중치 - (학습률 * 기울기)`
    *   기울기의 반대 방향으로 가중치를 이동시켜 손실을 줄입니다.
    *   **학습률(Learning Rate)**은 한 번에 얼마나 이동할지(업데이트할지)를 결정하는 보폭(step size) 역할을 합니다.

### 경사 하강법의 종류

1.  **배치 경사 하강법 (Batch Gradient Descent):**
    *   전체 학습 데이터를 **모두** 사용하여 한 번의 기울기를 계산하고 가중치를 업데이트합니다.
    *   **장점:** 전체 데이터의 정보를 반영하므로 안정적으로 최적점에 수렴합니다.
    *   **단점:** 데이터셋이 클 경우 메모리 소모가 매우 크고, 한 번 업데이트하는 데 시간이 오래 걸립니다.

2.  **확률적 경사 하강法 (Stochastic Gradient Descent, SGD):**
    *   전체 데이터에서 **하나의 샘플**을 무작위로 선택하여 기울기를 계산하고 즉시 가중치를 업데이트합니다.
    *   **장점:** 업데이트 속도가 매우 빠르고, 메모리 소모가 적습니다. 또한, 노이즈가 있는 업데이트 덕분에 지역 최적점(Local Minimum)을 탈출하고 전역 최적점(Global Minimum)을 찾을 가능성이 있습니다.
    *   **단점:** 업데이트 방향의 변동성이 커서 수렴 과정이 불안정할 수 있습니다.

3.  **미니배치 경사 하강법 (Mini-batch Gradient Descent):**
    *   배치와 SGD의 절충안으로, 전체 데이터를 **작은 그룹(미니배치)**으로 나누어 각 미니배치마다 기울기를 계산하고 가중치를 업데이트합니다.
    *   **장점:** SGD의 빠른 업데이트 속도와 배치 경사 하강법의 안정성을 모두 가집니다. GPU를 활용한 병렬 처리에 효율적이라 현재 가장 널리 사용되는 방식입니다.
    *   **단점:** 미니배치의 크기(batch size)라는 하이퍼파라미터를 추가로 설정해야 합니다.

## 고급 최적화 알고리즘

기본적인 경사 하강법의 단점(느린 수렴 속도, 지역 최적점 문제 등)을 개선하기 위해 다양한 고급 최적화 알고리즘이 개발되었습니다.

*   **Momentum (모멘텀):**
    *   이전의 기울기 방향을 현재 업데이트에 반영하는 방식입니다. 마치 언덕을 내려오는 공이 관성에 의해 더 빠르게 내려가는 것과 같습니다.
    *   SGD보다 빠르게 수렴하고, 진동을 줄이는 효과가 있습니다.

*   **AdaGrad (Adaptive Gradient):**
    *   각 가중치마다 **서로 다른 학습률**을 적용합니다.
    *   자주 업데이트된 가중치는 학습률을 작게 만들고, 적게 업데이트된 가중치는 학습률을 크게 만들어 효율적인 학습을 돕습니다.
    *   **단점:** 학습이 길어지면 학습률이 너무 작아져 거의 업데이트되지 않는 문제가 발생할 수 있습니다.

*   **RMSProp (Root Mean Square Propagation):**
    *   AdaGrad의 학습률 감소 문제를 해결하기 위해, 기울기 제곱의 지수 이동 평균을 사용하여 학습률을 조절합니다.
    *   학습률이 0에 수렴하는 것을 방지합니다.

*   **Adam (Adaptive Moment Estimation):**
    *   **Momentum**과 **RMSProp**의 장점을 결합한 알고리즘입니다.
    *   각 가중치마다 적응적인 학습률을 유지하면서, 동시에 이전 기울기의 방향(관성)을 함께 고려합니다.
    *   현재 가장広く 사용되는 안정적이고 효율적인 최적화기 중 하나입니다.

최적화 알고리즘의 선택은 모델의 성능과 학습 속도에 큰 영향을 미치는 중요한 요소입니다.

---
## 연관 문서
- 이전 단계: [[3. 역전파]]
- 다음 단계: [[5. 학습]]

