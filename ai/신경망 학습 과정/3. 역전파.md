# 역전파 (Backpropagation)

역전파는 인공 신경망을 학습시키기 위한 가장 일반적인 알고리즘입니다. 이 과정은 **순전파(Forward Propagation)** 단계에서 발생한 **오차(Error)**를 기반으로, 신경망의 각 **가중치(Weight)**와 **편향(Bias)**을 얼마나 조정해야 할지 계산하는 역할을 합니다.

역전파의 핵심 원리는 **경사 하강법(Gradient Descent)**과 **연쇄 법칙(Chain Rule)**에 기반합니다.

## 역전파의 과정

1.  **순전파 단계:**
    *   입력 데이터가 신경망에 주입되어 각 층을 거치며 최종 출력값을 계산합니다.
    *   이때 계산된 예측값과 실제값(정답)의 차이를 **오차(손실 함수 값)**로 계산합니다.

2.  **역전파 단계:**
    *   계산된 오차를 기반으로 출력층에서부터 입력층 방향으로 역으로 진행하며 각 가중치와 편향이 오차에 얼마나 영향을 미쳤는지(**기울기(Gradient)**)를 계산합니다.
    *   이 기울기 계산에는 **연쇄 법칙(Chain Rule)**이 사용됩니다. 즉, 각 층의 미분 값을 연쇄적으로 곱하여 최종 기울기를 구합니다.
    *   **출력층의 기울기 계산**: 손실 함수를 출력층의 가중치로 미분하여 기울기를 계산합니다.
    *   **은닉층의 기울기 계산**: 출력층에서 계산된 기울기를 이용하여, 이전 층(은닉층)의 가중치에 대한 기울기를 연쇄 법칙을 통해 계산합니다. 이 과정을 입력층에 도달할 때까지 반복합니다.

3.  **가중치 및 편향 업데이트:**
    *   계산된 기울기 값을 사용하여 각 가중치와 편향을 업데이트합니다.
    *   업데이트는 **경사 하강법**에 따라 기울기가 감소하는 방향으로 진행됩니다. 즉, 오차를 최소화하는 방향으로 가중치를 조정합니다.
    *   `새로운 가중치 = 기존 가중치 - (학습률 * 기울기)`
    *   **학습률(Learning Rate)**은 가중치를 얼마나 크게 업데이트할지 결정하는 하이퍼파라미터입니다.

## 역전파의 중요성

*   **효율적인 학습:** 역전파는 복잡하고 깊은 신경망에서도 각 가중치에 대한 기울기를 효율적으로 계산할 수 있게 해줍니다.
*   **자동화된 학습:** 모델이 예측값과 실제값의 차이를 스스로 줄여나가도록 자동화된 학습 과정을 가능하게 합니다.
*   **딥러닝의 핵심:** 역전파 알고리즘의 발견 덕분에 깊은 신경망, 즉 딥러닝 모델의 학습이 가능해졌습니다.

---
## 연관 문서
- 이전 단계: [[1. 순전파]], [[2. 오차]]
- 다음 단계: [[4. 최적화]]
