# 05. 로지스틱 회귀 (Logistic Regression)

## 1. 핵심 정의

**로지스틱 회귀(Logistic Regression)**는 이름에 '회귀'가 들어가지만, 실제로는 **이진 분류(Binary Classification)** 문제에 사용되는 통계 모델입니다. 즉, 어떤 입력이 주어졌을 때 결과가 두 가지 범주(예: 합격/불합격, 스팸/정상, 질병 유/무) 중 하나일 확률을 예측하는 데 사용됩니다.

-   **목표**: 입력 특성(feature)을 기반으로 특정 클래스에 속할 **확률**을 0과 1 사이의 값으로 예측하고, 이 확률을 특정 임계값(threshold)과 비교하여 최종적으로 클래스를 분류합니다.

## 2. 로지스틱 회귀 방정식

로지스틱 회귀는 [[02. 다중 선형 회귀|선형 회귀]]의 결과를 직접 사용하는 대신, 이 선형 결과(선형 결합)를 **시그모이드 함수(Sigmoid Function)**에 통과시켜 확률 값으로 변환합니다.

1.  **선형 결합 (Linear Combination)**: 먼저 독립 변수들의 선형 결합을 계산합니다.
    $$ z = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n $$
    여기서 $z$는 선형 회귀의 예측값과 유사하지만, 로지스틱 회귀에서는 이 값을 직접 예측으로 사용하지 않습니다.

2.  **시그모이드 함수 (Sigmoid Function)**: $z$ 값을 0과 1 사이의 확률 값으로 변환합니다.
    $$ P(y=1|x) = \sigma(z) = \frac{1}{1 + e^{-z}} $$
    -   $P(y=1|x)$: 주어진 $x$에 대해 $y$가 1일 확률
    -   $\sigma(z)$: [[시그모이드 함수]]
    -   $e$: 자연상수

이 시그모이드 함수는 $z$가 매우 큰 양수일 때 1에 가까워지고, 매우 큰 음수일 때 0에 가까워지는 S자 형태의 곡선입니다.

## 3. 결정 경계 (Decision Boundary)

로지스틱 회귀는 예측된 확률 $P(y=1|x)$를 기반으로 클래스를 분류합니다. 일반적으로 임계값(threshold)을 0.5로 설정합니다.

-   $P(y=1|x) \geq 0.5$ 이면, 클래스 1로 분류 (예: 합격)
-   $P(y=1|x) < 0.5$ 이면, 클래스 0으로 분류 (예: 불합격)

이 임계값을 기준으로 클래스를 나누는 경계를 **결정 경계(Decision Boundary)**라고 합니다.

## 4. 모델 훈련 (파라미터 학습)

[[05.1. 로지스틱 회귀 훈련 상세|로지스틱 회귀 모델을 훈련시킨다는 것]]은 최적의 파라미터($\theta_0, \theta_1, \dots, \theta_n$)를 찾는 것을 의미합니다.

### 4.1. 비용 함수: 교차 엔트로피 손실 (Cross-Entropy Loss)

선형 회귀에서 MSE를 사용했던 것과 달리, 로지스틱 회귀에서는 **교차 엔트로피 손실(Cross-Entropy Loss)** 또는 **로그 손실(Log Loss)**을 비용 함수로 사용합니다.

-   **이유**: 시그모이드 함수를 사용한 예측값에 MSE를 적용하면 비용 함수가 비볼록(non-convex)해져 [[경사 하강법]]이 전역 최솟값(global minimum)을 찾기 어렵게 됩니다. 교차 엔트로피는 볼록(convex) 함수이므로 최적화가 용이합니다.
-   **교차 엔트로피 손실**:
    $$ J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))] $$
    -   $m$: 훈련 데이터의 개수
    -   $y^{(i)}$: $i$번째 실제 정답 (0 또는 1)
    -   $h_\theta(x^{(i)})$: $i$번째 예측 확률 ($P(y=1|x^{(i)}))$)

### 4.2. 최적화 알고리즘: 경사 하강법 (Gradient Descent)

비용 함수 $J(\theta)$를 최소화하기 위해 [[경사 하강법]]을 사용합니다.

-   각 파라미터 $\theta_j$에 대해 비용 함수의 기울기를 계산하고, **학습률(Learning Rate, $\alpha$)**에 따라 파라미터를 업데이트합니다.
    $$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) $$
-   로지스틱 회귀의 기울기 계산식은 선형 회귀와 형태는 유사하지만, 시그모이드 함수가 포함되어 있습니다.

## 5. 장점과 단점

### 장점

-   구현이 비교적 간단하고 해석하기 쉽습니다.
-   출력값이 확률로 제공되어 예측의 불확실성을 이해하는 데 도움이 됩니다.
-   선형적으로 분리 가능한 데이터셋에서 좋은 성능을 보입니다.

### 단점

-   선형적으로 분리 불가능한 복잡한 데이터셋에서는 성능이 떨어질 수 있습니다.
-   다중 클래스 분류에는 직접적으로 사용하기 어렵고, 확장된 형태(Softmax Regression)를 사용해야 합니다.

#ai #classification #logistic-regression #machine-learning
