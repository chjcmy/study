# 02. 다중 선형 회귀 (Multiple Linear Regression)

## 1. 핵심 정의

**다중 선형 회귀(Multiple Linear Regression)**는 [[01. 단순 선형 회귀]]의 확장된 형태로, **두 개 이상의 독립 변수**를 사용하여 하나의 연속적인 종속 변수를 예측하는 기법입니다.

-   **목표**: 여러 요인(독립 변수)들이 결과(종속 변수)에 미치는 영향을 종합적으로 분석하고 예측의 정확도를 높이는 것입니다.

## 2. 회귀 방정식

다중 선형 회귀 모델은 다음과 같은 방정식으로 표현됩니다.

$$ \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n $$

-   $\hat{y}$: 모델이 예측한 종속 변수의 값
-   $x_1, x_2, \dots, x_n$: 각각의 독립 변수들 (예: 엔진 크기, 실린더 수, ...)
-   $\theta_0$: y절편(편향)
-   $\theta_1, \theta_2, \dots, \theta_n$: 각 독립 변수에 대한 **가중치(weight)**. 각 변수가 종속 변수에 미치는 영향의 크기를 나타냅니다.

기하학적으로 단순 선형 회귀가 2차원 공간의 '선(Line)'이라면, 독립 변수가 2개인 다중 선형 회귀는 3차원 공간의 '평면(Plane)'으로, 그 이상의 변수를 가질 경우 '초평면(Hyperplane)'으로 표현됩니다.

## 3. 파라미터 추정 방법

다중 선형 회귀 역시 [[01. 단순 선형 회귀#3. 최적의 회귀선 찾기|단순 선형 회귀]]와 마찬가지로 비용 함수인 **[[MSE (평균 제곱 오차)|평균 제곱 오차]]**를 최소화하는 파라미터 $\theta_0, \theta_1, \dots, \theta_n$를 찾는 것을 목표로 합니다.

### 1) 정규 방정식 (Normal Equation) / OLS

-   **개념**: [[최소제곱법(Ordinary Least Squares, OLS)]]을 행렬 연산으로 확장하여 모든 파라미터 $\theta$를 한 번의 계산으로 구해냅니다.
    $$ \hat{\theta} = (X^T X)^{-1} X^T y $$
-   **장점**: 데이터와 변수의 수가 적을 때 빠르고 정확합니다.
-   **단점**: 변수의 수가 매우 많아지면 $(X^T X)^{-1}$를 계산하는 비용이 기하급수적으로 커져 비효율적이 됩니다.

### 2) 경사 하강법 (Gradient Descent)

-   **개념**: [[경사 하강법]]은 다중 선형 회귀에서도 동일하게 적용됩니다. 모든 파라미터($\theta_0, \dots, \theta_n$)에 대해 비용 함수([[MSE (평균 제곱 오차)|MSE]])를 각각 편미분하여 기울기를 구하고, 학습률 $\alpha$에 따라 파라미터를 반복적으로 업데이트합니다.
-   **업데이트 규칙**: 모든 $j$ (from 0 to n)에 대해 동시에 업데이트합니다.
    $$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \text{MSE}(\theta) $$
    -   $\alpha$ (**알파**): **학습률(Learning Rate)**은 모든 파라미터 업데이트에 동일하게 적용되며, 최적의 모델을 찾는 데 매우 중요한 하이퍼파라미터입니다.
-   **장점**: 변수나 데이터의 양이 아무리 많아도 적용할 수 있어, 대규모 머신러닝 문제에 필수적입니다.

## 4. 주요 고려사항 및 함정

### 1. 과적합 (Overfitting)

-   **정의**: [[과적합]]은 모델이 학습 데이터에만 너무 잘 맞춰져서, 새로운 데이터에 대한 예측 성능이 떨어지는 현상입니다.
-   **원인**: 독립 변수를 너무 많이 추가하면 모델의 복잡도가 불필요하게 높아져, 데이터의 실제 패턴이 아닌 노이즈까지 학습하게 됩니다.
-   **해결책**:
    -   **변수 선택**: 중요한 변수만 선택하여 모델을 단순화합니다.
    -   **정규화 (Regularization)**: 모델의 파라미터(가중치 $\theta$)가 너무 커지지 않도록 페널티를 부과합니다. (예: Ridge, Lasso 회귀)

### 2. 다중공선성 (Multicollinearity)

-   **정의**: [[다중공선성]]은 모델에 사용된 독립 변수들끼리 높은 상관관계를 가지는 현상입니다.
-   **문제점**:
    -   특정 변수의 영향력을 개별적으로 파악하기 어렵게 만듭니다. (예: '키'와 '몸무게'는 상관관계가 높아, 둘 중 어떤 변수가 '달리기 속도'에 영향을 미치는지 불분명해짐)
    -   파라미터 추정치가 불안정해져 모델의 신뢰도를 떨어뜨립니다.
-   **해결책**:
    -   **상관관계 분석**: 변수 간 상관관계를 확인하고, 상관관계가 매우 높은 변수 중 하나를 제거합니다.
    -   **VIF (분산 팽창 요인)**: VIF 값을 측정하여 10 이상이면 다중공선성이 높다고 판단하고 해당 변수를 제거하는 것을 고려합니다.

## 5. 범주형 변수 처리

'성별', '도시명'과 같은 범주형 데이터는 수학적 연산을 위해 숫자로 변환해야 합니다.

-   **원-핫 인코딩 (One-Hot Encoding)**: 각 범주를 새로운 이진(0 또는 1) 변수로 만듭니다. 예를 들어, '도시' 변수에 '서울', '부산', '인천'이 있다면, 'is_서울', 'is_부산', 'is_인천'이라는 3개의 새로운 변수를 만들고 해당하는 도시에만 1을 할당합니다.

#ai #regression #linear-regression #multicollinearity #overfitting
