# 03. 다항 회귀 (Polynomial Regression)

## 1. 핵심 정의

**다항 회귀(Polynomial Regression)**는 독립 변수와 종속 변수 간의 관계가 직선이 아닌 **곡선 형태**일 때, 독립 변수를 거듭제곱하여 만든 새로운 항들을 추가하여 비선형 관계를 모델링하는 기법입니다.

-   **목표**: [[01. 단순 선형 회귀|선형 회귀]]가 데이터의 직선적인 추세만 포착하는 한계를 넘어, 더 복잡하고 구불구불한 데이터 패턴을 학습하는 것입니다.

## 2. 회귀 방정식

다항 회귀 모델은 독립 변수 $x$의 거듭제곱 항($x^2, x^3, \dots$)을 새로운 독립 변수로 취급하여 [[02. 다중 선형 회귀|다중 선형 회귀]] 모델을 만드는 것과 같습니다.

예를 들어, 2차 다항 회귀의 방정식은 다음과 같습니다.

$$ \hat{y} = \theta_0 + \theta_1 x + \theta_2 x^2 $$

-   $\\hat{y}$: 예측값
-   $x$: 원래의 독립 변수
-   $x^2$: $x$를 제곱하여 만든 새로운 **파생 변수(derived feature)**
-   $\\theta_0, \\theta_1, \\theta_2$: 각 항에 대한 가중치(계수)

## 3. 선형 회귀인가, 비선형 회귀인가?

**매우 중요한 점**: 다항 회귀는 데이터의 **비선형 관계**를 모델링하지만, 알고리즘 자체는 **선형 회귀**로 분류됩니다.

-   **이유**: "선형"의 기준은 변수 $x$가 아니라, 모델이 학습해야 할 **파라미터($\\theta$)**에 대한 것입니다.
-   위의 2차 다항 회귀식에서, $\\hat{y}$는 파라미터 $\\theta_0, \\theta_1, \\theta_2$에 대한 선형 결합으로 표현됩니다. ($x$와 $x^2$을 각각 $x_1, x_2$라는 별개의 변수로 본다면, 이는 완벽한 다중 선형 회귀식입니다.)
-   따라서, [[최소제곱법(OLS)|최소제곱법]]이나 [[경사 하강법]]과 같은 선형 회귀의 해법을 그대로 사용하여 파라미터를 찾을 수 있습니다.

## 4. 차수(Degree) 선택의 중요성과 과적합

다항 회귀에서 가장 중요한 결정은 **다항식의 차수(degree)**를 몇으로 할지 정하는 것입니다.

-   **차수가 너무 낮으면 (Underfitting)**: 모델이 데이터의 복잡한 패턴을 충분히 따라가지 못해 예측 성능이 떨어집니다. (예: 포물선 형태의 데이터에 1차 직선을 적합시키는 경우)
-   **차수가 너무 높으면 (Overfitting)**: 모델이 학습 데이터의 모든 점을 통과하려고 너무 구불구불하게 만들어져, 데이터의 실제 패턴이 아닌 **노이즈(noise)까지 학습**하게 됩니다. 이런 모델은 학습 데이터에 대한 성능은 매우 높지만, 새로운 데이터에 대한 예측 성능은 매우 떨어지는 [[과적합]] 상태에 빠집니다.

따라서, 모델의 복잡도와 일반화 성능 사이의 균형을 맞추는 최적의 차수를 찾는 것이 매우 중요합니다.

## 5. 장점과 단점

### 장점

-   선형 회귀의 틀을 유지하면서도 데이터의 비선형 패턴을 학습할 수 있습니다.
-   구현이 비교적 간단합니다.

### 단점

-   적절한 차수를 선택하기 어렵습니다.
-   차수가 높아질수록 모델이 매우 복잡해지고 과적합의 위험이 커집니다.
-   데이터의 범위를 조금만 벗어나도 예측이 매우 불안정해질 수 있습니다.

#ai #regression #polynomial-regression #overfitting
