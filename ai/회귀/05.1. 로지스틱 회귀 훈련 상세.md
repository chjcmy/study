# 05.1. 로지스틱 회귀 훈련 상세 (Logistic Regression Training Details)

## 1. 훈련의 목적

[[05. 로지스틱 회귀|로지스틱 회귀]] 모델 훈련의 목적은 입력 특징(feature)을 목표 결과에 매핑하는 **최적의 파라미터($\theta$)를 찾아** 오류를 최소화하면서 클래스를 예측하는 것입니다. 이는 모델이 데이터의 패턴을 가장 잘 학습하도록 만드는 과정입니다.

## 2. 훈련 과정 (5단계)

로지스틱 회귀 모델의 일반적인 훈련 과정은 다음과 같습니다.

1.  **초기 파라미터 설정**: 모델의 파라미터($\theta$) 값을 무작위로 또는 특정 전략에 따라 초기화합니다.
2.  **확률 예측**: 현재 파라미터($\theta$)를 사용하여 각 관측치에 대해 클래스가 1일 확률($P(y=1|x)$)을 계산합니다.
3.  **오차 측정**: 예측된 확률과 실제 클래스($y$) 간의 오차를 [[교차 엔트로피 손실|비용 함수(Cost Function)]]인 **로그 손실(Log Loss)**로 측정합니다.
4.  **파라미터 업데이트**: 측정된 오차를 줄이는 방향으로 파라미터($\theta$)를 업데이트합니다. 이때 [[경사 하강법]]과 같은 최적화 알고리즘을 사용합니다.
5.  **반복**: 로그 손실이 충분히 작아지거나, 파라미터가 더 이상 크게 변하지 않거나, 미리 설정된 최대 반복 횟수에 도달할 때까지 2~4단계를 반복합니다.

## 3. 비용 함수: 로그 손실 (Log Loss)

### 정의

**로그 손실(Log Loss)**은 [[05. 로지스틱 회귀|로지스틱 회귀]]에서 사용되는 주요 비용 함수로, 예측 확률 $\hat{p}_i$가 실제 클래스 $y_i$와 얼마나 잘 일치하는지 측정합니다. 이는 [[교차 엔트로피 손실]]과 동일합니다.

### 수식

$$ \text{Log Loss} = -\frac{1}{n}\sum_{i=1}^{n} \left[ y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right] $$ 

-   $n$: 훈련 데이터의 총 개수
-   $y_i$: $i$번째 관측치의 실제 클래스 (0 또는 1)
-   $\hat{p}_i$: $i$번째 관측치에 대한 클래스 1의 예측 확률

### 특징

-   **정확한 예측 보상**: 클래스 1을 정확하게 예측하고($\hat{p}_i \approx 1, y_i = 1$) 또는 클래스 0을 정확하게 예측하면($\hat{p}_i \approx 0, y_i = 0$) 로그 손실 값이 작아집니다.
-   **잘못된 예측 불이익**: 실제 클래스와 반대되는 클래스를 높은 확률로 예측하면 로그 손실이 매우 커집니다. 이는 모델이 잘못된 예측에 대해 큰 페널티를 받아 빠르게 수정하도록 유도합니다.

## 4. 최적화 알고리즘: 경사 하강법 (Gradient Descent)

[[경사 하강법]]은 로그 손실 함수를 최소화하여 최적의 파라미터($\theta$)를 찾는 반복적 최적화 기법입니다.

### 작동 원리

비용 함수의 **도함수(기울기)**를 사용하여 비용 함수가 가장 가파르게 감소하는 방향으로 파라미터 값을 조금씩 조정합니다.

-   **기울기(Gradient)**: 현재 파라미터 값에서 비용 함수가 가장 빠르게 증가하는 방향을 가리킵니다.
-   **경사 하강**: 우리는 비용 함수를 최소화해야 하므로, 기울기의 **음수 방향**으로 파라미터를 업데이트합니다.
-   **학습률(Learning Rate, $\alpha$)**: 각 단계에서 파라미터를 얼마나 크게 업데이트할지 제어하는 스케일링 요소입니다. [[01. 단순 선형 회귀#^경사 하강법 (Gradient Descent)|학습률]]이 너무 크면 최솟값을 지나쳐 발산할 수 있고, 너무 작으면 수렴 속도가 느려집니다.

### 시각화

비용 함수를 포물선 형태의 표면으로 상상할 때, 경사 하강법은 이 표면 위에서 가장 낮은 지점(최솟값)을 찾아 내려가는 과정과 같습니다. 기울기가 가파른 곳에서는 큰 단계로 이동하고, 최저점에 가까워질수록 기울기가 0에 수렴하여 이동 폭이 줄어듭니다.

### 한계점

-   전체 데이터셋으로 기울기를 계산하므로, **데이터셋이 매우 클 경우 계산 비용이 높고 느립니다.**
-   적절한 학습률 설정이 중요하며, 너무 높으면 최솟값을 놓칠 수 있습니다.

## 5. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

[[확률적 경사 하강법(Stochastic Gradient Descent, SGD)]]은 경사 하강법의 한계점을 개선한 변형입니다.

### 개선점

-   **속도**: 전체 데이터셋 대신 **무작위로 선택된 하나의 샘플(또는 작은 배치)**만 사용하여 기울기를 계산하고 파라미터를 업데이트합니다. 이로 인해 계산 속도가 훨씬 빠릅니다.
-   **확장성**: 대규모 데이터셋에 매우 적합합니다.
-   **글로벌 최솟값**: 비용 함수에 여러 개의 지역 최솟값(local minimum)이 있을 경우, SGD의 무작위성 덕분에 국소 최솟값을 건너뛰고 글로벌 최솟값을 찾을 가능성이 높아집니다.

### 단점

-   파라미터 업데이트 과정이 경사 하강법보다 불안정하고 진동이 심하여, 수렴 경로가 부드럽지 않을 수 있습니다. 이로 인해 정확도가 다소 떨어질 수 있습니다.

### 수렴 개선 방법

1.  **학습률 감소 (Learning Rate Decay)**: 훈련이 진행됨에 따라 학습률을 점차 줄여, 최솟값에 가까워질수록 더 세밀하게 탐색하도록 합니다.
2.  **샘플 크기 증가**: 기울기 계산에 사용되는 랜덤 데이터 샘플 크기(미니 배치 크기)를 점차 늘려, 업데이트의 안정성을 높입니다.

## 6. 경사 하강법 vs SGD 비교

| 구분         | 경사 하강법 (GD)       | 확률적 경사 하강법 (SGD)     | 
| :----------- | :--------------------- | :--------------------------- | 
| **데이터 사용** | 전체 데이터셋          | 무작위 하위 집합 (샘플/미니 배치) | 
| **속도**     | 느림                   | 빠름                         | 
| **정확도**   | 높음                   | 상대적으로 낮음              | 
| **확장성**   | 낮음                   | 높음                         | 
| **수렴 패턴** | 부드러운 수렴          | 진동하며 수렴                | 

## 7. 핵심 요약

-   [[05. 로지스틱 회귀|로지스틱 회귀]] 훈련은 **비용 함수(로그 손실)를 최소화**하는 파라미터 $\theta$를 찾는 과정입니다.
-   **로그 손실**은 정확한 예측에 보상을 주고 잘못된 예측에 큰 불이익을 줍니다.
-   **경사 하강법**은 반복적으로 기울기를 따라 비용 함수의 최솟값을 탐색합니다.
-   **SGD**는 무작위 샘플로 빠르지만 정확도가 다소 떨어지며 대규모 데이터에 적합합니다.
-   학습률과 샘플 크기 조정으로 수렴 성능을 개선할 수 있습니다.

#ai #classification #logistic-regression #training #gradient-descent #sgd
