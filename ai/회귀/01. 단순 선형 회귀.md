# 01. 단순 선형 회귀 (Simple Linear Regression)

## 1. 핵심 정의

**단순 선형 회귀(Simple Linear Regression)**는 **단일 독립 변수**가 **연속적인 종속 변수**에 미치는 영향을 모델링하는 기법입니다. 두 변수 간의 관계가 **직선**이라고 가정합니다.

-   **독립 변수 (Independent Variable, $x$)**: 예측에 사용되는 원인 변수 (예: 공부 시간)
-   **종속 변수 (Dependent Variable, $y$)**: 예측하고자 하는 결과 변수 (예: 시험 성적)

## 2. 회귀선 방정식

단순 선형 회귀 모델은 다음과 같은 직선 방정식으로 표현됩니다.

$$ \hat{y} = \theta_0 + \theta_1 x $$

-   $\hat{y}$: 모델이 예측한 종속 변수의 값
-   $x$: 독립 변수의 값
-   $\theta_0$ (세타 제로): **y절편(y-intercept)** 또는 **편향(bias)**. 독립 변수 $x$가 0일 때의 예측값입니다.
-   $\theta_1$ (세타 원): **기울기(slope)** 또는 **가중치(weight)**. 독립 변수 $x$가 한 단위 증가할 때 $\hat{y}$가 얼마나 변하는지를 나타냅니다.

## 3. 최적의 회귀선 찾기

모델의 목표는 주어진 데이터를 가장 잘 설명하는 '최적의' 직선, 즉 최적의 파라미터($\theta_0, \theta_1$)를 찾는 것입니다.

### 3.1. 비용 함수: 평균 제곱 오차 (MSE)

최적의 선을 찾기 위해, 우리는 모델의 예측이 얼마나 틀렸는지를 측정하는 **비용 함수(Cost Function)**를 정의해야 합니다. 선형 회귀에서는 주로 **평균 제곱 오차(Mean Squared Error, MSE)**를 사용합니다.

-   **잔차 (Residual)**: 실제 값($y_i$)과 모델의 예측 값($\hat{y}_i$)의 차이. ($y_i - \hat{y}_i$)
-   **평균 제곱 오차 (MSE)**: 모든 데이터 포인트에 대해 잔차의 제곱을 구한 뒤, 그 값들의 평균을 낸 것입니다.
    $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - (\theta_0 + \theta_1 x_i))^2 $$
    제곱을 하는 이유는 오차의 방향(+, -)에 상관없이 크기만을 고려하고, 큰 오차에 더 큰 페널티를 부여하기 위함입니다.

**선형 회귀의 목표는 이 MSE를 최소화하는 $\theta_0$와 $\theta_1$을 찾는 것입니다.**

### 3.2. 파라미터 추정 방법

#### 1) 정규 방정식 (Normal Equation) / OLS

-   **개념**: [[최소제곱법(Ordinary Least Squares, OLS)]]은 MSE를 최소화하는 $\theta$ 값을 미분을 통해 수학적으로 직접 계산하는 방법입니다.
-   **계산 공식**:
    1.  독립 변수와 종속 변수의 평균($\bar{x}, \bar{y}$)을 계산합니다.
    2.  기울기 $\theta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}$ 을 계산합니다.
    3.  절편 $\theta_0 = \bar{y} - \theta_1 \bar{x}$ 을 계산합니다.
-   **장점**: 데이터가 많지 않을 때 매우 빠르고 정확합니다.
-   **단점**: 변수의 수가 매우 많아지면 계산이 복잡해지고 느려집니다.

#### 2) 경사 하강법 (Gradient Descent)

-   **개념**: [[경사 하강법]]은 비용 함수(MSE)의 기울기(gradient)를 계산하여, 기울기가 가장 가파르게 감소하는 방향으로 파라미터($\theta_0, \theta_1$)를 반복적으로 업데이트하며 최적값을 찾아가는 방법입니다.
-   **업데이트 규칙**:
    $$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} \text{MSE}(\theta_0, \theta_1) \quad (\text{for } j=0, 1) $$
    -   $\theta_j$: 업데이트할 파라미터 ($\\theta_0$ 또는 $\\theta_1$)
    -   $\alpha$ (**알파**): **학습률(Learning Rate)**. 파라미터를 한 번에 얼마나 업데이트할지 결정하는 보폭(step size)입니다.
        -   **학습률이 너무 크면**: 최적값을 지나쳐 발산(diverge)할 수 있습니다.
        -   **학습률이 너무 작으면**: 학습 속도가 매우 느려집니다.
    -   $\frac{\partial}{\partial \theta_j} \text{MSE}$: MSE를 각 파라미터로 [[편미분]]한 값, 즉 기울기입니다. 이 기울기는 현재 위치에서 비용이 가장 빠르게 증가하는 방향을 알려줍니다.
-   **장점**: 데이터셋이 매우 크거나 변수가 많을 때 효과적입니다.
-   **단점**: 적절한 학습률 $\alpha$를 선택하는 것이 중요하며, 반복 계산이 필요합니다.

## 4. 장점과 단점

### 장점

-   **해석의 용이성**: 모델이 단순한 직선이므로, 독립 변수가 결과에 미치는 영향을 직관적으로 이해하기 쉽습니다.
-   **계산 효율성**: 특히 데이터가 작을 때 매우 빠르게 모델을 만들 수 있습니다.

### 단점

-   **단순성**: 현실 세계의 복잡한 비선형 관계를 모델링하는 데 한계가 있습니다.
-   **이상치에 민감**: 데이터에 이상치(outlier)가 존재할 경우, 회귀선이 크게 왜곡될 수 있습니다.

#ai #regression #linear-regression #gradient-descent
