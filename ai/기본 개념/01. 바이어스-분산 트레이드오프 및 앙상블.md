# 01. 바이어스-분산 트레이드오프 및 앙상블 (Bias-Variance Trade-off & Ensemble)

## 1. 바이어스와 분산의 개념

머신러닝 모델의 예측 오류는 크게 **바이어스(Bias)**, **분산(Variance)**, 그리고 **제거 불가능한 오차(Irreducible Error)**의 세 가지 요소로 구성됩니다. 이 개념들을 이해하는 것은 모델의 성능을 분석하고 개선하는 데 매우 중요합니다.

### 1.1. 다트보드 비유

-   **바이어스(Bias)**: 모델의 **정확도**를 나타냅니다. 다트가 타겟의 중앙(실제값)에 얼마나 가깝게 맞는지에 비유할 수 있습니다.
    -   **낮은 바이어스**: 예측값이 실제값에 가깝습니다. (다트가 중앙 근처에 모임)
    -   **높은 바이어스**: 예측값이 실제값에서 멀리 떨어져 있습니다. (다트가 중앙에서 벗어남)
-   **분산(Variance)**: 모델의 **정밀도**를 나타냅니다. 다트들이 서로 얼마나 가깝게 그룹화되어 있는지, 즉 예측값들이 얼마나 일관성 있게 분포하는지에 비유할 수 있습니다.
    -   **낮은 분산**: 예측값들이 서로 가깝게 모여 있습니다. (다트가 좁게 그룹화)
    -   **높은 분산**: 예측값들이 넓게 퍼져 있습니다. (다트가 넓게 퍼짐)

**이상적인 모델**: 낮은 바이어스 + 낮은 분산 (정확하고 정밀하게 예측)

## 2. 예측 바이어스 (Prediction Bias)

### 정의

모델의 예측값과 실제 목표값 간의 **평균적인 차이**입니다. 모델이 훈련 데이터의 패턴을 얼마나 잘 포착하는지, 즉 모델이 얼마나 단순한지를 나타냅니다.

-   **높은 바이어스**: 모델이 너무 단순하여 데이터의 복잡한 패턴을 학습하지 못할 때 발생합니다. 이는 **[[과소적합(Underfitting)]]**으로 이어집니다.
-   **낮은 바이어스**: 모델이 데이터의 패턴을 잘 포착합니다.

## 3. 예측 분산 (Prediction Variance)

### 정의

동일한 데이터의 여러 **하위 집합(subsets)**에 대해 학습할 때, 모델의 예측값이 얼마나 변동하는지를 나타내는 정도입니다. 모델이 훈련 데이터의 노이즈에 얼마나 민감한지를 나타냅니다.

-   **높은 분산**: 모델이 훈련 데이터의 작은 변화나 노이즈에 극도로 민감하게 반응하여, 훈련 데이터셋마다 예측 결과가 크게 달라집니다. 이는 **[[과적합(Overfitting)]]**으로 이어집니다.
-   **낮은 분산**: 모델이 노이즈에 덜 민감하며, 새로운 데이터에 대해 예측이 안정적입니다.

## 4. 바이어스-분산 트레이드오프 (Bias-Variance Trade-off)

바이어스와 분산은 서로 상충 관계에 있습니다. 모델의 복잡도를 조절할 때, 바이어스를 줄이면 분산이 늘어나고, 분산을 줄이면 바이어스가 늘어나는 경향이 있습니다.

### 모델 복잡성에 따른 변화

-   **모델 복잡성 증가 시**:
    -   **바이어스 감소**: 모델이 더 많은 패턴을 학습할 수 있게 되어 실제값과의 평균적인 차이가 줄어듭니다.
    -   **분산 증가**: 모델이 훈련 데이터의 노이즈까지 학습하게 되어, 데이터셋의 작은 변화에도 예측이 크게 흔들립니다.
-   **모델 복잡성 감소 시**:
    -   **바이어스 증가**: 모델이 너무 단순하여 데이터의 중요한 패턴을 놓치게 됩니다.
    -   **분산 감소**: 모델이 노이즈에 덜 민감해져 예측이 안정적입니다.

### 최적점

목표는 바이어스와 분산의 합이 최소가 되는 **최적의 모델 복잡성**을 찾는 것입니다. 이 지점에서 모델은 훈련 데이터의 패턴을 잘 학습하면서도 새로운 데이터에 대해 잘 일반화됩니다.

**제거 불가능한 오차 (Irreducible Error)**: 데이터 자체에 내재된 노이즈나 측정 오류 등으로 인해 어떤 모델로도 줄일 수 없는 최소한의 오차입니다.

## 5. 학습자 유형

-   **약한 학습자 (Weak Learner)**:
    -   **높은 바이어스 + 낮은 분산** 경향을 가집니다.
    -   데이터의 복잡한 패턴을 잘 포착하지 못하고, [[과소적합(Underfitting)]]될 가능성이 높습니다.
    -   예시: 깊이가 얕은 [[01. 의사 결정 트리|의사 결정 트리]]
-   **강한 학습자 (Strong Learner)**:
    -   **낮은 바이어스 + 높은 분산** 경향을 가집니다.
    -   훈련 데이터의 패턴을 매우 잘 포착하지만, 노이즈에도 민감하여 [[과적합(Overfitting)]]될 가능성이 높습니다.
    -   예시: 깊이가 깊은 [[01. 의사 결정 트리|의사 결정 트리]]

## 6. 앙상블 방법 (Ensemble Methods)

앙상블 방법은 여러 개의 약한 학습자(Weak Learner)를 결합하여 하나의 강력한 모델을 만드는 기법입니다. 바이어스나 분산 중 하나를 줄이는 데 특화되어 있습니다.

### 6.1. 배깅 (Bagging: Bootstrap Aggregating)

#### 목적

모델의 **분산을 크게 감소**시켜 [[과적합(Overfitting)]]을 완화하는 데 중점을 둡니다.

#### 작동 방식

1.  **부트스트랩 샘플링**: 원본 훈련 데이터셋에서 **중복을 허용하여** 무작위로 여러 개의 하위 데이터셋을 생성합니다.
2.  **병렬 훈련**: 각 하위 데이터셋에 대해 독립적으로 기본 학습자(Base Learner, 주로 [[01. 의사 결정 트리|의사 결정 트리]])를 훈련시킵니다.
3.  **결과 집계**: 모든 기본 학습자의 예측 결과를 종합합니다.
    -   **분류**: 다수결 투표 (Majority Voting)
    -   **회귀**: 예측값들의 평균
4.  **효과**: 개별 모델의 예측은 분산이 높을 수 있지만, 여러 모델의 예측을 평균화함으로써 전체 모델의 분산을 크게 줄이고 안정적인 예측을 얻습니다.

#### 대표적인 알고리즘

-   **[[02. 랜덤 포레스트]] (Random Forest)**: 배깅의 대표적인 예시로, 부트스트랩 샘플링과 무작위 특성 선택을 통해 여러 의사 결정 트리를 훈련시켜 분산을 효과적으로 줄입니다.

### 6.2. 부스팅 (Boosting)

#### 목적

모델의 **바이어스를 감소**시켜 [[과소적합(Underfitting)]]을 완화하는 데 중점을 둡니다.

#### 작동 방식

1.  **순차적 훈련**: 기본 학습자들을 **순차적으로** 훈련시킵니다.
2.  **오류 수정**: 각 학습자는 이전 학습자가 잘못 예측한 데이터(오류)에 더 많은 가중치를 부여하거나, 이전 학습자의 잔차(오차)를 예측하도록 학습합니다. 즉, 이전 모델의 약점을 보완하는 데 집중합니다.
3.  **가중치 합산**: 최종 모델은 모든 기본 학습자의 예측 결과를 가중치 합산하여 만듭니다.

#### 대표적인 알고리즘

-   **[[03. 그래디언트 부스팅 머신|그래디언트 부스팅 (Gradient Boosting)]]**: 이전 모델의 잔차(오차)를 학습하는 방식으로 바이어스를 점진적으로 줄여나갑니다.
-   **XGBoost, LightGBM, CatBoost**: 그래디언트 부스팅을 더욱 발전시킨 고성능 알고리즘들입니다.
-   **AdaBoost (Adaptive Boosting)**: 잘못 분류된 샘플에 더 높은 가중치를 부여하여 다음 학습자가 집중하도록 합니다.

## 7. 배깅 vs 부스팅 비교

| 구분             | 배깅 (Bagging)                               | 부스팅 (Boosting)                               |
| :--------------- | :------------------------------------------- | :---------------------------------------------- |
| **목표**         | [[과적합(Overfitting)]] 완화 (분산 감소)     | [[과소적합(Underfitting)]] 완화 (바이어스 감소) |
| **기본 학습자 특성** | 분산이 높고 바이어스가 낮은 모델 (예: 깊은 트리) | 분산이 낮고 바이어스가 높은 모델 (예: 얕은 트리) |
| **훈련 방식**    | 병렬 훈련 (독립적)                           | 순차적 훈련 (의존적)                            |
| **오류 처리**    | 개별 모델의 분산을 평균하여 감소             | 이전 모델의 오류를 다음 모델이 수정             |
| **대표 알고리즘** | [[02. 랜덤 포레스트|랜덤 포레스트]]          | [[03. 그래디언트 부스팅 머신|그래디언트 부스팅]], XGBoost, AdaBoost |

## 8. 핵심 요약

-   **바이어스**: 모델의 정확도 (실제값과의 평균 차이)
-   **분산**: 모델의 정밀도 (예측값의 변동성)
-   **바이어스-분산 트레이드오프**: 모델 복잡성 증가 시 바이어스 감소, 분산 증가
-   **배깅**: 여러 모델의 평균으로 **분산 감소** → [[과적합(Overfitting)]] 방지 (예: [[02. 랜덤 포레스트|랜덤 포레스트]])
-   **부스팅**: 순차적 오류 수정으로 **바이어스 감소** → [[과소적합(Underfitting)]] 방지 (예: [[03. 그래디언트 부스팅 머신|그래디언트 부스팅]])
-   [[01. 의사 결정 트리|의사 결정 트리]]는 깊이 조정을 통해 바이어스/분산을 쉽게 조절할 수 있어 앙상블의 기본 학습자로 선호됩니다.

#ai #machine-learning #bias-variance-tradeoff #ensemble #bagging #boosting #random-forest #gradient-boosting #overfitting #underfitting
